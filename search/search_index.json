{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-the-bam-masterdata-documentation-page","title":"Welcome to the <code>bam-masterdata</code> documentation page.","text":"<p>The <code>bam-masterdata</code> is a Python package designed to define and handle the Masterdata used in the BAM Data Store project. The BAM Data Store is the central system for Research Data Management at the Bundesanstalt f\u00fcr Materialforschung und -pr\u00fcfung (BAM). It is a customized instance of openBIS.</p>"},{"location":"#tutorials","title":"Tutorials","text":"<p>The Tutorials are designed to guide you through the general and basic functionalities of the package. They are ideal for learning the software during your first interaction.</p> <ul> <li>Getting Started</li> <li>Automating Metadata with Parsers</li> </ul>"},{"location":"#how-to-guides","title":"How-to guides","text":"<p>The How-to guides provide step-by-step instructions for a variety of tasks. These serve as a quick reference for practical applications when you need specific information.</p> <ul> <li>Parsing<ul> <li>Use the Parser app</li> <li>Create new parsers</li> </ul> </li> </ul>"},{"location":"#explanations","title":"Explanations","text":"<p>The Explanations offer theoretical insights into the core concepts.</p> <ul> <li>Parsing Structure</li> <li>Schema Definitions</li> </ul>"},{"location":"#references","title":"References","text":"<p>The References include a glossary of terms and the automatically generated API documentation.</p> <ul> <li>Glossary</li> <li>API Reference</li> <li>Extra</li> </ul>"},{"location":"#contact","title":"Contact","text":"<p>This documentation is a work in progress. If you have questions that are not yet addressed, please contact datastore@bam.de.</p>"},{"location":"explanations/parsing_structure/","title":"Explanation: Parsing and ETL Structure in the Parser App","text":""},{"location":"explanations/parsing_structure/#concept","title":"Concept","text":"<p>The parser app implements a simplified ETL pipeline \u2013 Extract, Transform, and Load \u2013 with a strong focus on the parsing step. At its core, the app allows users to upload files from laboratory experiments or field measurements, apply custom parsers to interpret the content, and then transform the parsed results into structured database entities that can be stored reliably for later analysis.</p> <p>The idea behind this architecture is the separation of concerns. Instead of treating file upload, parsing, transformation, and database loading as one monolithic process, the app breaks them into clear stages. Each stage has its own responsibility:</p> <ul> <li>Extract: Handling files as raw input.</li> <li>Transform<ul> <li>Parsing: Turning raw input into structured domain objects.</li> <li>Mapping: Converting those domain objects into database-ready models.</li> </ul> </li> <li>Load: Persisting the final models into the database.</li> </ul> <p>This design provides clarity, maintainability, and flexibility. It allows the app to support many different test formats and research workflows without constantly rewriting the underlying logic.</p>"},{"location":"explanations/parsing_structure/#how-it-works","title":"How it works","text":"<p>The workflow of the app can be understood in four main steps, which follow the ETL principle.</p>"},{"location":"explanations/parsing_structure/#1-file-upload-extract","title":"1. File upload (Extract)","text":"<p>Researchers begin by uploading files generated by instruments or test rigs. These files might be in different formats \u2013 CSVs with tensile test data, JSON from sensor systems, XML from automated machines, or even proprietary log files. The app does not assume any specific structure at this point. Its only job is to receive and store the raw data, leaving interpretation for the next stage.</p>"},{"location":"explanations/parsing_structure/#2-parsing-step-transform-structure-discovery","title":"2. Parsing step (Transform \u2013 structure discovery)","text":"<p>Parsing is the stage where meaning is applied to raw data. The app allows users to provide custom parsers. A parser is a piece of logic that knows how to interpret a particular file format and produce structured domain objects.</p> <p>For example, consider a CSV file containing test results from material samples. A <code>MaterialSampleParser</code> would read each row of the file and create <code>Material</code> domain objects with attributes such as <code>sampleId</code>, <code>composition</code>, and <code>testDate</code>. Similarly, a <code>TestResultParser</code> could take a laboratory log file and output <code>TestResult</code> objects with fields like <code>testId</code>, <code>strengthValue</code>, and <code>temperature</code>.</p> <p>By allowing user-defined parsers, the app avoids being tied to one specific format. Instead, it becomes a flexible framework where parsing is pluggable.</p>"},{"location":"explanations/parsing_structure/#3-mapping-step-transform-semantic-mapping","title":"3. Mapping step (Transform \u2013 semantic mapping)","text":"<p>Once the parser has produced domain objects, the app transforms these into database entities. This involves mapping the structure of the domain objects to the database schema. Data types are normalized, units are converted, relations between samples and tests are established, and constraints (like uniqueness or non-null fields) are applied.</p> <p>For instance, the <code>Material</code> domain object created by the parser might be transformed into a <code>Material</code> database object that matches the schema of the database table <code>Material</code>. The transformation ensures consistency between user-defined logic (parsing) and the technical requirements of the storage system.</p>"},{"location":"explanations/parsing_structure/#4-loading-step-load","title":"4. Loading step (Load)","text":"<p>Finally, the database entities are persisted into the target system. At this stage, the app handles all the technical details of saving objects: inserting records, maintaining referential integrity, and reporting any errors that occur during the load process.</p> <p>The loading step is intelligent about whether to create new objects or update existing ones. If a parser sets the <code>code</code> attribute to be the one of an object instance, the system recognizes this as a reference to an existing object and updates its properties rather than creating a duplicate. This allows parsers to handle both initial data import and subsequent updates with the same workflow.</p> <p>By the end of this step, the raw test file uploaded by the researcher has been fully integrated into the database in a safe and consistent way.</p>"},{"location":"explanations/parsing_structure/#why-this-structure","title":"Why this structure?","text":"<p>The app could, in theory, skip some of these steps. One might ask: why not simply load raw files directly into the database? The answer lies in the advantages of separating parsing, transformation, and loading.</p> <ol> <li> <p>Flexibility    By introducing custom parsers, the app can handle a wide variety of test formats without modifying its core. A new instrument or file format only requires writing a new parser, while the rest of the system remains unchanged.</p> </li> <li> <p>Reusability    Parsers are reusable components. A parser written for one type of test file can be used repeatedly, regardless of how the transformation or loading steps evolve. This modularity prevents duplication of logic.</p> </li> <li> <p>Robustness    Parsing errors are detected early, before they affect the database. If a file cannot be interpreted, the process fails gracefully at the parsing stage instead of creating corrupt or inconsistent database entries.</p> </li> <li> <p>Maintainability    Each stage is easier to reason about and maintain when responsibilities are clearly separated. Developers working on parsers do not need to worry about database internals, and vice versa.</p> </li> <li> <p>Performance    By isolating parsing and transformation, the system can optimize specific parts of the pipeline. For example, parsing can be parallelized across large sets of test data, while loading can be tuned for batch inserts.</p> </li> </ol>"},{"location":"explanations/parsing_structure/#design-considerations","title":"Design considerations","text":"<p>Designing the parsing and ETL pipeline involves several important decisions.</p> <ul> <li> <p>Parser interface   All parsers follow a consistent contract: they accept files as input and return a collection of domain objects such as material samples or test results. This consistency allows the app to integrate any parser seamlessly.</p> </li> <li> <p>Error handling   Validation is critical at the parsing stage. Files may contain missing values, invalid formats, or corrupted rows. Detecting these issues early prevents them from propagating into the database and ensures data integrity for downstream analysis.</p> </li> <li> <p>Extensibility   The modular design makes it easy to support new instruments and data sources. Adding support for a new file format requires only a new parser, not a rewrite of the transformation or loading steps.</p> </li> <li> <p>Trade-offs   Modularity adds complexity. Developers must define and manage parsers carefully. There is also a performance cost when data passes through multiple transformations. However, the long-term benefits of reliability and flexibility outweigh these costs.</p> </li> </ul>"},{"location":"explanations/schema_defs/","title":"Schema Definitions","text":"<p>Warning</p> <p>This page is still under construction.</p>"},{"location":"howtos/object_references/","title":"How-to: Work with Object References","text":"<p>This how-to guide shows you how to work with properties whose data type is OBJECT in order to reference other objects in openBIS.</p>"},{"location":"howtos/object_references/#what-are-object-references","title":"What are object references?","text":"<p>Some object types have properties with <code>data_type=\"OBJECT\"</code> that create references to other objects. For example:</p> <ul> <li>An <code>Instrument</code> might have a <code>responsible_person</code> property that references a <code>Person</code> object</li> <li>A <code>Calibration</code> might have an <code>instrument</code> property that references an <code>Instrument</code> object</li> <li>A <code>Sample</code> might have a <code>parent_sample</code> property that references another <code>Sample</code> object</li> </ul> <p>These are properties defined to reference other existing objects in openBIS. Their purpose is to link between objects, similar to what a parent-child relationship do. However, these links have a semantic meaning, while parent-child relationships are only connecting inputs with outputs.</p> Semantic meaning of object referencing <p>The semantic meaning of object referencing is given by the name of the property (e.g., a person responsible for operating an instrument). Nevertheless, openBIS will soon allow for adding more metainformation to these properties to create a more complete description of objects and their relationships.</p>"},{"location":"howtos/object_references/#option-1-reference-by-object-instance","title":"Option 1: Reference by Object Instance","text":"<p>When creating multiple related objects in the same operation, you can reference them directly:</p> <pre><code>from bam_masterdata.datamodel.object_types import Person, Instrument\nfrom bam_masterdata.metadata.entities import CollectionType\n\n# Create a collection\ncollection = CollectionType()\n\n# Create a person\nperson = Person(name=\"Dr. Jane Smith\")\nperson.code = \"PERSON_001\"  # \u26a0\ufe0f Must set a code!\nperson_id = collection.add(person)\n\n# Create an instrument and reference the person\ninstrument = Instrument(name=\"High-Resolution Microscope\")\ninstrument.responsible_person = person  # Direct reference\ninstrument_id = collection.add(instrument)\n</code></pre> <p>Object must have a code</p> <p>When referencing an object instance, it must have a <code>code</code> attribute set. If not, you'll get a <code>ValueError</code>:</p> <pre><code>ValueError: Object instance for 'responsible_person' must have a 'code' attribute set\n</code></pre>"},{"location":"howtos/object_references/#option-2-reference-by-path-string","title":"Option 2: Reference by Path String","text":"<p>If the object already exists in openBIS, you can reference it using its identifier path:</p> <pre><code>from bam_masterdata.datamodel.object_types import Instrument\n\n# Create an instrument\ninstrument = Instrument(name=\"Spectrometer X500\")\n\n# Reference an existing person using the path format\n# Format: /{space}/{project}/{collection}/{object}\ninstrument.responsible_person = \"/LAB_SPACE/INSTRUMENTS/STAFF/PERSON_001\"\n\n# Or without collection: /{space}/{project}/{object}\ninstrument.responsible_person = \"/LAB_SPACE/INSTRUMENTS/PERSON_001\"\n</code></pre> <p>Path validation</p> <p>The path must:</p> <ul> <li>Start with <code>/</code></li> <li>Have either 3 parts (space/project/object) or 4 parts (space/project/collection/object)</li> <li>Point to an existing object in openBIS</li> </ul>"},{"location":"howtos/object_references/#combining-both-approaches","title":"Combining Both Approaches","text":"<p>You can mix both approaches in the same parser:</p> <pre><code>from bam_masterdata.parsing import AbstractParser\nfrom bam_masterdata.datamodel.object_types import Person, Instrument\n\nclass InstrumentParser(AbstractParser):\n    def parse(self, files, collection, logger):\n        # Create a new person\n        new_person = Person(name=\"Dr. Alice Johnson\")\n        new_person.code = \"PERSON_NEW_001\"\n        collection.add(new_person)\n\n        # Instrument 1: References the newly created person\n        instrument1 = Instrument(name=\"Microscope A\")\n        instrument1.responsible_person = new_person  # Object instance\n        collection.add(instrument1)\n\n        # Instrument 2: References an existing person in openBIS\n        instrument2 = Instrument(name=\"Microscope B\")\n        instrument2.responsible_person = \"/LAB_SPACE/PROJECT/PERSON_EXISTING\"  # Path\n        collection.add(instrument2)\n</code></pre>"},{"location":"howtos/object_references/#troubleshooting","title":"Troubleshooting","text":""},{"location":"howtos/object_references/#error-object-instance-must-have-a-code-attribute-set","title":"Error: \"Object instance must have a 'code' attribute set\"","text":"<p>Cause: You're trying to reference an object instance that doesn't have a code.</p> <p>Solution: Set the <code>code</code> attribute before using the object as a reference:</p> <pre><code>person = Person(name=\"Dr. Smith\")\nperson.code = \"PERSON_001\"  # \u2713 Set the code\ninstrument.responsible_person = person\n</code></pre>"},{"location":"howtos/object_references/#error-invalid-object-path-format","title":"Error: \"Invalid OBJECT path format\"","text":"<p>Cause: The path string doesn't follow the required format.</p> <p>Solution: Ensure your path:</p> <ul> <li>Starts with <code>/</code></li> <li>Has 3 or 4 parts separated by <code>/</code></li> </ul> <pre><code># \u2717 Wrong\ninstrument.responsible_person = \"PERSON_001\"\ninstrument.responsible_person = \"SPACE/PROJECT/PERSON_001\"\n\n# \u2713 Correct\ninstrument.responsible_person = \"/SPACE/PROJECT/PERSON_001\"\ninstrument.responsible_person = \"/SPACE/PROJECT/COLLECTION/PERSON_001\"\n</code></pre>"},{"location":"howtos/object_references/#error-failed-to-resolve-object-reference","title":"Error: \"Failed to resolve OBJECT reference\"","text":"<p>Cause: The path references an object that doesn't exist in openBIS.</p> <p>Solution: Verify the object exists in openBIS at the specified path, or create it first:</p> <pre><code># Check if the object exists in openBIS before referencing\nopenbis = ologin(...)\ntry:\n    obj = openbis.get_object(\"/SPACE/PROJECT/PERSON_001\")\n    print(f\"Object exists: {obj.identifier}\")\nexcept:\n    print(\"Object not found - create it first or use a different path\")\n</code></pre>"},{"location":"howtos/parsing/create_new_parsers/","title":"How-to: Create New Parsers","text":"<p>This how-to guide explains how to create a custom parser that reads raw files (CSV, Excel, JSON, XML, etc) and transforms them into the <code>bam-masterdata</code> format. By following these steps, your parser can be integrated into the Data Store workflow and used in the Parser app. This allows you to bring custom or third-party data sources into the existing masterdata workflows without manual conversion.</p> <p>Prerequisites</p> <ul> <li>Python \u2265 3.10 installed</li> <li>Knowledge of the bam-masterdata schema definitions in <code>bam_masterdata/datamodel/</code>. Learn more in Schema Definitions.</li> </ul>"},{"location":"howtos/parsing/create_new_parsers/#use-the-github-parser-example","title":"Use the GitHub parser example","text":"<ol> <li>Go to masterdata-parser-example.</li> <li>Either fork it (keep your own version) or use it as a template to start a new repository.</li> <li>Clone your fork/template locally:     <pre><code>git clone [your repository link]\n</code></pre></li> <li> <p>Verify the folder structure includes <code>src/</code>, <code>tests/</code>, <code>pyproject.toml</code>, and <code>README.md</code>:     <pre><code>[your repo name]\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 src\n\u2502   \u2514\u2500\u2500 masterdata_parser_example\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 parser.py\n\u2502       \u2514\u2500\u2500 _version.py\n\u2514\u2500\u2500 tests\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 conftest.py\n    \u2514\u2500\u2500 test_parser.py\n</code></pre></p> <ul> <li><code>src/</code> \u2192 contains the parser package code</li> <li><code>tests/</code> \u2192 contains test files to check your parser works correctly</li> <li><code>pyproject.toml</code> \u2192 defines dependencies and project configuration</li> <li><code>README.md</code> \u2192 instructions and documentation</li> </ul> </li> </ol> Forking or using the template <p>You can read more details in the GitHub docs on forking a repository and on creating a repository from a template.</p> <p>Either way, you should end up with your own repository in which you can work on the definition and logic behind the parser.</p>"},{"location":"howtos/parsing/create_new_parsers/#set-up-a-virtual-environment","title":"Set up a Virtual Environment","text":"<p>It is recommended to create a virtual environment named <code>.venv</code> (already included in <code>.gitignore</code>) to manage dependencies. In the terminal, do: <pre><code>cd [your repo name]\n</code></pre></p> <p>You have two options to create a virtual environment:</p> <ol> <li>Using venv     <pre><code>python -m venv .venv\nsource .venv/bin/activate  # on Linux/macOS\n.\\.venv\\Scripts\\activate  # on Windows\n</code></pre></li> <li>Using conda     <pre><code>conda create --prefix .venv python=3.10\nconda activate .venv\n</code></pre></li> </ol> <p>Verify that everything is set up correctly by running inside the repo:</p> <pre><code>pip install --upgrade pip\npip install -e .\npytest tests\n</code></pre> <p>You should see all tests passing before you start customizing.</p> Faster pip installation <p>We recommend installing <code>uv</code> before installing the package by doing: <pre><code>pip install --upgrade pip\npip install uv\nuv pip install -e .\n</code></pre></p>"},{"location":"howtos/parsing/create_new_parsers/#modify-the-project-structure-and-files","title":"Modify the project structure and files","text":"<p>Since everything in the template project is named <code>masterdata_parser_example</code> and derivates, you will need to replace that with your own parser name. This ensures that your parser has a unique and consistent package name.</p> <p>For the purpose of this guide, we will rename everything using a ficticious code name SupercodeX.</p> Python naming conventions <ul> <li>Packages / modules: lowercase, underscores allowed (e.g., <code>my_parser</code>)</li> <li>Classes: CapWords / PascalCase (e.g., <code>MyParser</code>)</li> <li>Variables / functions: lowercase with underscores (e.g., <code>file_name</code>, <code>parse_file</code>)</li> </ul> <p>See the official Python style guide: PEP 8 \u2013 Naming Conventions</p>"},{"location":"howtos/parsing/create_new_parsers/#rename-project-folder-and-parser-class","title":"Rename project folder and parser class","text":"<ol> <li>Modify the <code>src</code> package name from <code>masterdata_parser_example</code> to your package name (e.g., <code>supercode_x</code>). This will affect on how your users install the package later on by doing <code>pip install</code> (e.g., <code>pip install supercode_x</code>).</li> <li>Update in the <code>pyproject.toml</code> all occurrences of <code>masterdata_parser_example</code> to the new package name (e.g., <code>supercode_x</code>).</li> <li>Update the <code>[project]</code> section in <code>pyproject.toml</code> with your specific information.</li> <li>Go to <code>src/supercode_x/parser.py</code> and change the class name from <code>MasterdataParserExample</code> to your case (<code>SupercodeXParser</code>).</li> <li>Update importing this class in <code>src/supercode_x/__init__.py</code> and <code>tests/conftest.py</code>.</li> <li>Update the entry point dictionary in <code>src/supercode_x/__init__.py</code>.</li> <li>Verify that the project is still working by running <code>pytest tests</code>. If everything is good, the testing should pass.</li> </ol>"},{"location":"howtos/parsing/create_new_parsers/#rename-entry-point","title":"Rename entry point","text":"<ol> <li>Go to <code>src/supercode_x/__init__.py</code>.</li> <li>Modify <code>masterdata_parser_example_entry_point</code> for your new entry point variable name (e.g., <code>supercode_x_entry_point</code>).</li> <li>Update in the <code>pyproject.toml</code> all occurrences of <code>masterdata_parser_example_entry_point</code> to the new entry point name  (e.g., <code>supercode_x_entry_point</code>).</li> </ol>"},{"location":"howtos/parsing/create_new_parsers/#add-parser-logic","title":"Add parser logic","text":"<p>Open the <code>src/.../parser.py</code> file. After renaming your parser class to <code>SupercodeXParser</code>, you should have: <pre><code>from bam_masterdata.datamodel.object_types import ExperimentalStep\nfrom bam_masterdata.parsing import AbstractParser\n\n\nclass SupercodeXParser(AbstractParser):\n    def parse(self, files, collection, logger):\n        synthesis = ExperimentalStep(name=\"Synthesis\")\n        synthesis_id = collection.add(synthesis)\n        measurement = ExperimentalStep(name=\"Measurement\")\n        measurement_id = collection.add(measurement)\n        _ = collection.add_relationship(synthesis_id, measurement_id)\n        logger.info(\n            \"Parsing finished: Added examples synthesis and measurement experimental steps.\"\n        )\n</code></pre></p> <p>Writing a parser logic is composed of a series of steps: 1. The object type classes imported from <code>bam_masterdata</code> (in the example above, <code>ExperimentalStep</code>). 2. Open the <code>files</code> with Python and read metainformation from them. 3. Instantiate object types and add the metainformation to the corresponding fields. 4. Add those object types and their relationships to <code>collection</code>.</p> <p>Optionally, you can add log messages (<code>info</code>, <code>warning</code>, <code>error</code>, or <code>critical</code>) to debug the logic of your parser.</p>"},{"location":"howtos/parsing/create_new_parsers/#example","title":"Example","text":"<p>As an example, imagine we are expecting to pass a <code>super.json</code> file to our <code>SupercodeXParser</code> to read certain metadata. The file contents are: <pre><code>{\n    \"program_name\": \"SupercodeX\",\n    \"program_version\": \"1.1.0\",\n}\n</code></pre></p> <p>We recommend moving files in which you are testing the parser to a <code>tests/data/</code> folder.</p>"},{"location":"howtos/parsing/create_new_parsers/#step-1-import-necessary-classes","title":"Step 1: Import necessary classes","text":"<p>At the top of <code>parser.py</code>, ensure you import: <pre><code># Step 1: import necessary classes\nimport json\nfrom bam_masterdata.datamodel.object_types import SoftwareCode\nfrom bam_masterdata.parsing import AbstractParser\n</code></pre></p>"},{"location":"howtos/parsing/create_new_parsers/#step-2-modify-the-parse-method","title":"Step 2: Modify the parse() method","text":"<ol> <li>Iterate over the <code>files</code> argument.</li> <li>Open each file and read the JSON content.</li> <li>Optionally, log progress using <code>logger.info()</code>.</li> </ol> <pre><code># Step 1: import necessary classes\nimport json\nfrom bam_masterdata.datamodel.object_types import SoftwareCode\nfrom bam_masterdata.parsing import AbstractParser\n\n\nclass SupercodeXParser(AbstractParser):\n    def parse(self, files, collection, logger):\n        for file_path in files:\n            # Step 2: read files metainformation\n            logger.info(f\"Parsing file: {file_path}\")\n            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                data = json.load(f)\n</code></pre>"},{"location":"howtos/parsing/create_new_parsers/#step-3-instantiate-objects-and-add-metadata","title":"Step 3: Instantiate objects and add metadata","text":"<ol> <li>Instantiate <code>SoftwareCode</code> objects and fill in the fields with the JSON data.</li> <li>Optionally, log progress using <code>logger.info()</code>.</li> </ol> <pre><code># Step 1: import necessary classes\nimport json\nfrom bam_masterdata.datamodel.object_types import SoftwareCode\nfrom bam_masterdata.parsing import AbstractParser\n\n\nclass SupercodeXParser(AbstractParser):\n    def parse(self, files, collection, logger):\n        for file_path in files:\n            # Step 2: read files metainformation\n            logger.info(f\"Parsing file: {file_path}\")\n            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                data = json.load(f)\n\n            # Step 3: Instantiate and populate classes metadata\n            software = SoftwareCode(\n                name=data.get(\"program_name\"),\n                version=data.get(\"program_version\")\n            )\n</code></pre>"},{"location":"howtos/parsing/create_new_parsers/#step-4-add-objects-and-relationships-to-the-collection","title":"Step 4: Add objects and relationships to the collection","text":"<ol> <li>Add the object to the collection using <code>collection.add(object)</code>.</li> <li>You can also add relationships between objects using <code>collection.relationships(parent_id, child_id)</code>.</li> </ol> <pre><code># Step 1: import necessary classes\nimport json\nfrom bam_masterdata.datamodel.object_types import SoftwareCode\nfrom bam_masterdata.parsing import AbstractParser\n\n\nclass SupercodeXParser(AbstractParser):\n    def parse(self, files, collection, logger):\n        for file_path in files:\n            # Step 2: read files metainformation\n            logger.info(f\"Parsing file: {file_path}\")\n            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                data = json.load(f)\n\n            # Step 3: Instantiate and populate classes metadata\n            software = SoftwareCode(\n                name=data.get(\"program_name\"),\n                version=data.get(\"program_version\")\n            )\n\n            # Step 4: Add to collection\n            software_id = collection.add(software)\n            logger.info(f\"Added SoftwareCode with ID {software_id}\")\n</code></pre>"},{"location":"howtos/parsing/create_new_parsers/#referencing-existing-objects-in-openbis","title":"Referencing Existing Objects in OpenBIS","text":"<p>When parsing data, you may want to update an existing object in OpenBIS rather than creating a new one. This is useful when you're importing updated metadata for objects that already exist in the system.</p> <p>To reference an existing object, set the <code>code</code> attribute on your object instance before adding it to the collection:</p> <pre><code># Step 1\nimport json\nfrom bam_masterdata.datamodel.object_types import SoftwareCode\nfrom bam_masterdata.parsing import AbstractParser\n\n\nclass SupercodeXParser(AbstractParser):\n    def parse(self, files, collection, logger):\n        for file_path in files:\n            # Step 2\n            logger.info(f\"Parsing file: {file_path}\")\n            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                data = json.load(f)\n\n            # Step 3\n            software = SoftwareCode(\n                name=data.get(\"program_name\"),\n                version=data.get(\"program_version\")\n            )\n\n            # Reference an existing object by setting its `code`\n            # This code should match the object's code in OpenBIS\n            if data.get(\"existing_identifier\"):\n                software.code = data.get(\"existing_identifier\")\n                logger.info(f\"Referencing existing object: {software.code}\")\n\n            # Step 4\n            software_id = collection.add(software)\n</code></pre> <ul> <li>Without <code>code</code> set (default): A new object is created in OpenBIS with an automatically generated code based on the object type's <code>generated_code_prefix</code>.</li> <li>With <code>code</code> set: The parser looks for an existing object with that code in OpenBIS. If found, it updates the object's properties with the new values from your parser. If not found, the behavior depends on the OpenBIS configuration.</li> </ul> <p>You can have multiple scenarios when deciding if setting up <code>code</code> or not:</p> <ul> <li>Creating new objects: Leave the <code>code</code> attribute unset. The system will generate unique codes automatically.</li> <li>Updating existing objects: Set the <code>code</code> attribute to match the code of an existing object in OpenBIS. For example, if you have a sample with code <code>SAMPLE_001</code> in OpenBIS, set <code>sample.code = \"SAMPLE_001\"</code> in your parser to update it. Note that this is a static assignment.</li> <li>Mixed workflow: You can create some new objects while updating others in the same parsing operation by setting <code>code</code> only where needed.</li> </ul> <p>Code Format</p> <p>The <code>code</code> must match the exact format used in OpenBIS, including any prefixes or separators. Codes are typically uppercase with underscores (e.g., <code>SAMPLE_001</code>, <code>EXP_2024_01</code>).</p> <p>Identifier Construction</p> <p>When referencing an existing object, the full identifier is constructed as:</p> <ul> <li>With collection: <code>/{space_name}/{project_name}/{collection_name}/{code}</code></li> <li>Without collection: <code>/{space_name}/{project_name}/{code}</code></li> </ul> <p>Make sure the object exists at the expected location in the OpenBIS hierarchy.</p>"},{"location":"howtos/parsing/create_new_parsers/#tips","title":"Tips","text":"<ul> <li>Use the logger to provide useful messages during parsing, but bear in mind this can clutter the app if you plan to parse hundreds or more files.   <pre><code>logger.info(\"Parsing file XYZ\")\n</code></pre></li> <li>Test your parser incrementally by adding one object at a time to the collection and verifying results. You can test this by modifying the <code>tests/test_parser.py</code> testing file.</li> <li>When updating existing objects, log which objects are being updated to help with debugging and traceability.</li> </ul>"},{"location":"howtos/parsing/create_new_parsers/#final-steps","title":"Final steps","text":"<p>You now have all the core components of your custom parser in place:</p> <ul> <li>Project structure set up.</li> <li>Package renamed to your parser name.</li> <li>Parser class created and logic accepting the metainformation of the specific files.</li> <li>Entry points updated.</li> </ul>"},{"location":"howtos/parsing/create_new_parsers/#whats-left","title":"What\u2019s left?","text":"<ol> <li>Update <code>pyproject.toml</code><ul> <li>Make sure the package name, version, and entry points match your parser.</li> <li>Adjust dependencies if your parser requires additional libraries (e.g., <code>pandas</code>).</li> </ul> </li> <li>Update the <code>README.md</code><ul> <li>Replace the <code>README.md</code> content with a description of your parser.</li> <li>Document how to install it and how to run it.</li> </ul> </li> <li>Create a new release in GitHub<ul> <li>Go to your repository on GitHub.</li> <li>Click on the Releases tab (or navigate to <code>https://github.com/[your-username]/[your-repo]/releases</code>).</li> <li>Click Create a new release.</li> <li>Choose a tag version (e.g., <code>v1.0.0</code>) and add a release title.</li> <li>Optionally, add release notes describing changes or new features.</li> <li>Click Publish release to make it available.</li> </ul> </li> </ol>"},{"location":"howtos/parsing/create_new_parsers/#updating-the-parser","title":"Updating the Parser","text":"<p>Once your parser is implemented and tested, future updates are usually minimal and follow a clear process.</p> <ol> <li> <p>Modify only <code>parser.py</code></p> <ul> <li>All changes should be contained within your parser class and helper functions.</li> <li>Avoid renaming packages or changing the project structure unless absolutely necessary.</li> </ul> </li> <li> <p>Notify the Admin for a new release</p> <ul> <li>After updates, inform the administrator or the person responsible for releases.</li> <li>Provide details of the changes and any new dependencies.</li> </ul> </li> </ol>"},{"location":"howtos/parsing/parser_app/","title":"How-to: Use the Parser App","text":"<p>This how-to guide explains how to use the Parser App in your browser to upload files, run the parser, and transfer parsed metadata to the Data Store. It is intended for users who want a step-by-step walkthrough of the main app functions.</p> <p>Prerequisites</p> <p> WIP</p>"},{"location":"howtos/parsing/parser_app/#open-and-login-in-the-app","title":"Open and login in the app","text":"<ol> <li>Open your web browser.</li> <li>Go to the provided URL of the Parser App (e.g., https://parser.example.com).</li> <li>The login page of the app will appear.</li> <li>Enter your Username.</li> <li>Enter your Password.</li> <li>Press Login.</li> </ol>"},{"location":"howtos/parsing/parser_app/#select-and-upload-files","title":"Select and Upload Files","text":"<ol> <li>Select the Space where data will be saved.</li> <li>Enter Project and Collection names.</li> <li>Drag and drop or click to select files.</li> <li>Press Upload Files to upload them.<ul> <li>Click the Reset button to start over if needed.</li> </ul> </li> </ol>"},{"location":"howtos/parsing/parser_app/#select-parsers","title":"Select Parsers","text":"<ol> <li>Choose a parser by clicking Select Parser for each uploaded file.<ul> <li>Available parsers are listed in the corner.</li> <li>See How-to: Create new parsers to create your own.</li> </ul> </li> <li>Press Parse to extract metadata from each file and upload it to the Data Store. The extraction is defined in the logic of each defined parser.</li> </ol>"},{"location":"howtos/parsing/parser_app/#review-logs","title":"Review Logs","text":"<ol> <li>Check logs to verify successful parsing. If parsing fails, check the logs and debug the parsing process.</li> <li>If parsing works, only INFO messages will appear in the logs card.</li> <li>The logs will show whether objects were created new or updated if they already existed in the Data Store.</li> </ol>"},{"location":"howtos/parsing/parser_app/#advanced-updating-existing-objects","title":"Advanced: Updating Existing Objects","text":"<p>The Parser App can update existing objects in the Data Store rather than always creating new ones. This is useful when you want to:</p> <ul> <li>Update metadata for samples or experiments that already exist.</li> <li>Correct or enrich existing data.</li> <li>Maintain consistent object codes across multiple parsing operations.</li> </ul> <p>To update an existing object, your parser must set the <code>code</code> attribute on the object instance. When the <code>code</code> is set:</p> <ol> <li>The Parser App looks for an existing object with that code in your Space/Project/Collection (note that Collection is optional if objects exist at the Project level).</li> <li>If found, the object's properties are updated with the new values.</li> <li>A log message confirms that the existing object was updated.</li> </ol> <p>Warning</p> <p>Note that the object must exist in the specified Space, Project, and, optionally, Collection names.</p> <p>See How-to: Create new parsers for details on implementing this in custom parsers.</p>"},{"location":"references/api/","title":"API Reference","text":"<p>This API reference provides comprehensive documentation for all public classes and functions in the BAM Masterdata package. For more detailed examples and usage patterns, see the How-to Guides and Tutorial sections.</p>"},{"location":"references/api/#bam_masterdata.metadata.entities","title":"<code>bam_masterdata.metadata.entities</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.entities.BaseEntity","title":"<code>BaseEntity</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class used to define <code>ObjectType</code> and <code>VocabularyType</code> classes. It extends the <code>BaseModel</code> adding new methods that are useful for interfacing with openBIS.</p> Source code in <code>bam_masterdata/metadata/entities.py</code> <pre><code>class BaseEntity(BaseModel):\n    \"\"\"\n    Base class used to define `ObjectType` and `VocabularyType` classes. It extends the `BaseModel`\n    adding new methods that are useful for interfacing with openBIS.\n    \"\"\"\n\n    code: str | None = Field(\n        default=None,\n        description=\"\"\"\n        Code of the entity to assign as permanent identifier in openBIS.\n        \"\"\",\n    )\n\n    def __init__(self, **kwargs):\n        super().__init__()\n\n        # We store the `_property_metadata` during instantiation of the class\n        self._property_metadata = self.get_property_metadata()\n\n        for key, value in kwargs.items():\n            setattr(self, key, value)\n\n    def __setattr__(self, key, value):\n        if key == \"_property_metadata\":\n            super().__setattr__(key, value)\n            return\n\n        if key in self._property_metadata:\n            # TODO add CONTROLLEDVOCABULARY and OBJECT cases\n            expected_type = self._property_metadata[key].data_type.pytype\n            if expected_type and not isinstance(value, expected_type):\n                raise TypeError(\n                    f\"Invalid type for '{key}': Expected {expected_type.__name__}, got {type(value).__name__}\"\n                )\n\n        # TODO add check if someone tries to set up a definition instead of an assigned property\n\n        object.__setattr__(self, key, value)\n\n    def __repr__(self):\n        # Filter for attributes that are `PropertyTypeAssignment` and set to a finite value\n        class_prop_name = None\n        fields = []\n        for key, metadata in self._property_metadata.items():\n            if isinstance(metadata, PropertyTypeAssignment):\n                value = getattr(self, key, None)\n                # Only include set attributes\n                if value is not None and not isinstance(value, PropertyTypeAssignment):\n                    if key == \"name\":\n                        class_prop_name = value\n                    fields.append(f\"{key}={repr(value)}\")\n\n        # Format the output\n        class_name = self.cls_name\n        if class_prop_name:  # adding `name` if available\n            class_name = f\"{class_prop_name}:{class_name}\"\n        return f\"{class_name}({', '.join(fields)})\"\n\n    # Overwriting the __str__ method to use the same representation as __repr__\n    __str__ = __repr__\n\n    @property\n    def cls_name(self) -&gt; str:\n        \"\"\"\n        Returns the entity name of the class as a string to speed up checks. This is a property\n        to be overwritten by each of the abstract entity types.\n        \"\"\"\n        return self.__class__.__name__\n\n    @property\n    def _base_attrs(self) -&gt; list:\n        \"\"\"\n        List of base properties or terms assigned to an entity type. This are the direct properties or terms\n        assigned when defining a new entity type.\n        \"\"\"\n        cls_attrs = self.__class__.__dict__\n        base_attrs = [\n            attr_name\n            for attr_name in cls_attrs\n            if not (\n                attr_name.startswith(\"_\")\n                or callable(cls_attrs[attr_name])\n                or attr_name\n                in [\"defs\", \"model_config\", \"model_fields\", \"model_computed_fields\"]\n            )\n        ]\n        return [getattr(self, attr_name) for attr_name in base_attrs]\n\n    def _to_openbis(\n        self,\n        logger: \"BoundLoggerLazyProxy\",\n        openbis: \"Openbis\",\n        type: str,\n        type_map: dict,\n        get_type: Callable[..., Any],\n        create_type: Callable[..., Any],\n    ) -&gt; None:\n        \"\"\"\n        Simplified function to add or update the entity type in openBIS.\n        \"\"\"\n        # Get all existing entities from openBIS\n        openbis_entities = getattr(\n            OpenbisEntities(url=openbis.url), f\"get_{type}_dict\"\n        )()\n        defs = getattr(self, \"defs\")\n\n        is_vocab = isinstance(self, VocabularyType)\n\n        # Check if the entity already exists\n        if defs.code in openbis_entities:\n            logger.info(f\"Entity '{defs.code}' already exists in openBIS.\")\n            # Retrieve the existing entity\n            entity = get_type(openbis, defs.code)\n            # entity = openbis_entities[defs.code]\n\n            # Get properties from self and openBIS\n            self_properties = getattr(self, \"terms\" if is_vocab else \"properties\", [])\n            obis_properties = (\n                entity.get_terms().df.code\n                if is_vocab\n                else entity.get_property_assignments()\n            )\n            obis_property_codes = [\n                prop.code if not is_vocab else prop for prop in obis_properties\n            ]\n\n            # Check for properties in self that are not in openBIS\n            new_properties_added = False\n            for prop in self_properties:\n                if prop.code not in obis_property_codes:\n                    logger.info(\n                        f\"Adding new '{'term' if is_vocab else 'property'}' {prop.code}' to entity '{defs.code}'.\"\n                    )\n                    new_properties_added = True\n\n                    # Handle special case for OBJECT or SAMPLE data types\n                    if not is_vocab and (\n                        prop.data_type == \"OBJECT\" or prop.data_type == \"SAMPLE\"\n                    ):\n                        prop.data_type = \"SAMPLE\"\n\n                    # Assign the term or property to the entity\n                    if is_vocab:\n                        term = openbis.new_term(\n                            code=prop.code,\n                            vocabularyCode=defs.code,\n                            label=prop.label,\n                            description=prop.description,\n                        )\n                        if prop.official:\n                            term.official = prop.official\n                        term.save()\n                    else:\n                        if prop.vocabulary_code:\n                            entity.assign_property(\n                                prop=prop.code,\n                                section=prop.section,\n                                mandatory=prop.mandatory,\n                                showInEditView=prop.show_in_edit_views,\n                                vocabulary=prop.vocabulary_code,\n                            )\n                        else:\n                            entity.assign_property(\n                                prop=prop.code,\n                                section=prop.section,\n                                mandatory=prop.mandatory,\n                                showInEditView=prop.show_in_edit_views,\n                            )\n\n            if not new_properties_added:\n                logger.info(\n                    f\"No new '{'terms' if is_vocab else 'properties'}' added to entity '{defs.code}'.\"\n                )\n\n            # Save the entity after adding new properties\n            if not is_vocab:\n                entity.save()\n            return entity\n\n        # If the entity is new, create it\n        logger.info(f\"Creating new entity '{defs.code}' in openBIS.\")\n        if not is_vocab:\n            entity = create_type(openbis, defs)\n            entity.save()\n\n            # Assign properties to the new entity\n            properties = getattr(self, \"properties\", [])\n            for prop in properties:\n                logger.info(f\"Adding new property {prop.code} to {defs.code}.\")\n                # Handle special case for OBJECT or SAMPLE data types\n                if prop.data_type == \"OBJECT\" or prop.data_type == \"SAMPLE\":\n                    prop.data_type = \"SAMPLE\"\n\n                # Assign the property to the entity\n                entity.assign_property(\n                    prop=prop.code,\n                    section=prop.section,\n                    mandatory=prop.mandatory,\n                    showInEditView=prop.show_in_edit_views,\n                )\n        else:\n            # Transform the list of VocabularyTerm objects into the desired format\n            terms = [\n                {\n                    \"code\": term.code,\n                    \"label\": term.label,\n                    \"description\": term.description,\n                }\n                for term in getattr(self, \"terms\", [])\n            ]\n            term_codes = \", \".join([term.code for term in getattr(self, \"terms\", [])])\n            logger.info(f\"Adding new terms {term_codes} to {defs.code}.\")\n            entity = create_type(openbis, defs, terms)\n            entity.save()\n\n        # Save the entity after assigning properties\n        if not is_vocab:\n            entity.save()\n        return entity\n\n    def get_property_metadata(self) -&gt; dict:\n        \"\"\"\n        Dictionary containing the metadata of the properties assigned to the entity type.\n\n        Returns:\n            dict: A dictionary containing the keys of the `PropertyTypeAssignment` attribute names and the\n            values of the definitions of `PropertyTypeAssignment`. Example:\n            {\n                \"name\": PropertyTypeAssignment(\n                    code=\"$NAME\",\n                    data_type=VARCHAR,\n                    mandatory=True,\n                    property_label=\"Name\"\n                ),\n                \"age\": PropertyTypeAssignment(\n                    code=\"AGE\",\n                    data_type=INTEGER,\n                    mandatory=False,\n                    property_label=\"Age\"\n                ),\n            }\n        \"\"\"\n        cls_attrs = self.__class__.__dict__\n\n        # Store property metadata at class level\n        prop_meta_dict: dict = {}\n        for base in type(self).__mro__:\n            cls_attrs = getattr(base, \"__dict__\", {})\n            for attr_name, attr_value in cls_attrs.items():\n                if isinstance(attr_value, PropertyTypeAssignment):\n                    prop_meta_dict[attr_name] = attr_value\n        return prop_meta_dict\n\n    def to_json(self, indent: int | None = None) -&gt; str:\n        \"\"\"\n        Returns the entity as a string in JSON format storing the value of the properties\n        assigned to the entity.\n\n        Args:\n            indent (Optional[int], optional): The indent to print in JSON. Defaults to None.\n\n        Returns:\n            str: The JSON representation of the entity.\n        \"\"\"\n        data: dict = {}\n        for key in self._property_metadata.keys():\n            try:\n                value = getattr(self, key)\n            except AttributeError:\n                continue\n            if isinstance(value, PropertyTypeAssignment):\n                continue\n            data[key] = value\n        return json.dumps(data, indent=indent)\n\n    def to_dict(self) -&gt; dict:\n        \"\"\"\n        Returns the entity as a dictionary storing the value of the properties assigned to the entity.\n\n        Returns:\n            dict: The dictionary representation of the entity.\n        \"\"\"\n        dump_json = self.to_json()\n        return json.loads(dump_json)\n\n    def to_hdf5(self, hdf_file: h5py.File, group_name: str = \"\") -&gt; h5py.File:\n        \"\"\"\n        Serialize the entity to a HDF5 file under the group specified in the input.\n\n        Args:\n            hdf_file (h5py.File): The HDF5 file to store the entity.\n            group_name (str, optional): The group name to serialize the data.\n        \"\"\"\n        if not group_name:\n            group_name = self.cls_name\n        group = hdf_file.create_group(group_name)\n\n        for key in self._property_metadata.keys():\n            try:\n                value = getattr(self, key)\n                if not value:\n                    continue\n                if isinstance(value, str | int | float | bool | list | tuple):\n                    group.create_dataset(key, data=value)\n                else:\n                    raise TypeError(\n                        f\"Unsupported type {type(value)} for key {key} for HDF5 serialization.\"\n                    )\n            except AttributeError:\n                continue\n\n    def model_to_dict(self) -&gt; dict:\n        \"\"\"\n        Returns the model as a dictionary storing the data `defs` and the property or vocabulary term\n        assignments.\n\n        Returns:\n            dict: The dictionary representation of the model.\n        \"\"\"\n        data = self.model_dump()\n\n        attr_value = getattr(self, \"defs\")\n        if isinstance(attr_value, BaseModel):\n            data[\"defs\"] = attr_value.model_dump()\n        else:\n            data[\"defs\"] = attr_value\n        return data\n\n    def model_to_json(self, indent: int | None = None) -&gt; str:\n        \"\"\"\n        Returns the model as a string in JSON format storing the data `defs` and the property or\n        vocabulary term assignments.\n\n        Args:\n            indent (Optional[int], optional): The indent to print in JSON. Defaults to None.\n\n        Returns:\n            str: The JSON representation of the model.\n        \"\"\"\n        # * `model_dump_json()` from pydantic does not store the `defs` section of each entity.\n        data = self.model_to_dict()\n        return json.dumps(data, indent=indent)\n\n    def _add_properties_rdf(\n        self,\n        namespace: \"Namespace\",\n        graph: \"Graph\",\n        prop: \"PropertyTypeAssignment\",\n        logger: \"BoundLoggerLazyProxy\",\n    ) -&gt; \"URIRef\":\n        \"\"\"\n        Add the properties assigned to the entity to the RDF graph extracting the information from\n        OpenBIS for the `object_code` or `vocabulary_code`.\n\n        Args:\n            namespace (Namespace): The namespace to use for the RDF graph.\n            graph (Graph): The RDF graph to which the properties are added.\n            prop (PropertyTypeAssignment): The property assigned to the entity.\n            logger (BoundLoggerLazyProxy): The logger to log messages.\n\n        Returns:\n            URIRef: The URI reference of the property added to the RDF graph.\n        \"\"\"\n        prop_uri = namespace[prop.id]\n\n        # Define the property as an OWL class inheriting from PropertyType\n        graph.add((prop_uri, RDF.type, OWL.Thing))\n        graph.add((prop_uri, RDFS.subClassOf, namespace.PropertyType))\n\n        # Add attributes like id, code, description in English and Deutsch, property_label, data_type\n        graph.add((prop_uri, RDFS.label, Literal(prop.id, lang=\"en\")))\n        graph.add((prop_uri, DC.identifier, Literal(prop.code)))\n        descriptions = prop.description.split(\"//\")\n        if len(descriptions) &gt; 1:\n            graph.add((prop_uri, RDFS.comment, Literal(descriptions[0], lang=\"en\")))\n            graph.add((prop_uri, RDFS.comment, Literal(descriptions[1], lang=\"de\")))\n        else:\n            graph.add((prop_uri, RDFS.comment, Literal(prop.description, lang=\"en\")))\n        graph.add(\n            (prop_uri, namespace.propertyLabel, Literal(prop.property_label, lang=\"en\"))\n        )\n        graph.add((prop_uri, namespace.dataType, Literal(prop.data_type.value)))\n        if prop.data_type.value == \"OBJECT\":\n            # entity_ref_uri = BAM[code_to_class_name(obj.object_code)]\n            # graph.add((prop_uri, BAM.referenceTo, entity_ref_uri))\n            object_code = code_to_class_name(prop.object_code, logger)\n            if not object_code:\n                logger.error(\n                    f\"Failed to identify the `object_code` for the property {prop.id}\"\n                )\n                return prop_uri\n            entity_ref_uri = namespace[object_code]\n\n            # Create a restriction with referenceTo\n            restriction = BNode()\n            graph.add((restriction, RDF.type, OWL.Restriction))\n            graph.add((restriction, OWL.onProperty, namespace[\"referenceTo\"]))\n            graph.add((restriction, OWL.someValuesFrom, entity_ref_uri))\n\n            # Add the restriction as a subclass of the property\n            graph.add((prop_uri, RDFS.subClassOf, restriction))\n        return prop_uri\n\n    # skos:prefLabel used for class names\n    # skos:definition used for `description` (en, de)\n    # dc:identifier used for `code`  # ! only defined for internal codes with $ symbol\n    # parents defined from `code`\n    # assigned properties can be Mandatory or Optional, can be PropertyType or ObjectType\n    # ? For OBJECT TYPES\n    # ? `generated_code_prefix`, `auto_generate_codes`?\n    @no_type_check\n    def model_to_rdf(\n        self, namespace: \"Namespace\", graph: \"Graph\", logger: \"BoundLoggerLazyProxy\"\n    ) -&gt; None:\n        \"\"\"\n        Convert the entity to RDF triples and add them to the graph. The function uses the\n        `_add_properties_rdf` method to convert the properties assigned to the entity to RDF triples.\n\n        Args:\n            namespace (Namespace): The namespace to use for the RDF graph.\n            graph (Graph): The RDF graph to which the entity is added.\n            logger (BoundLoggerLazyProxy): The logger to log messages.\n        \"\"\"\n        entity_uri = namespace[self.defs.id]\n\n        # Define the entity as an OWL class inheriting from the specific namespace type\n        graph.add((entity_uri, RDF.type, OWL.Thing))\n        parent_classes = self.__class__.__bases__\n        for parent_class in parent_classes:\n            if issubclass(parent_class, BaseEntity) and parent_class != BaseEntity:\n                # if parent_class.__name__ in [\n                #     \"ObjectType\",\n                #     \"CollectionType\",\n                #     \"DatasetType\",\n                # ]:\n                #     # ! add here logic of subClassOf connecting with PROV-O or BFO\n                #     # ! maybe via classes instead of ObjectType/CollectionType/DatasetType?\n                #     # ! Example:\n                #     # !     graph.add((entity_uri, RDFS.subClassOf, \"http://www.w3.org/ns/prov#Entity\"))\n                #     continue\n                parent_uri = namespace[parent_class.__name__]\n                graph.add((entity_uri, RDFS.subClassOf, parent_uri))\n\n        # Add attributes like id, code, description in English and Deutsch, property_label, data_type\n        graph.add((entity_uri, RDFS.label, Literal(self.defs.id, lang=\"en\")))\n        graph.add((entity_uri, DC.identifier, Literal(self.defs.code)))\n        descriptions = self.defs.description.split(\"//\")\n        if len(descriptions) &gt; 1:\n            graph.add((entity_uri, RDFS.comment, Literal(descriptions[0], lang=\"en\")))\n            graph.add((entity_uri, RDFS.comment, Literal(descriptions[1], lang=\"de\")))\n        else:\n            graph.add(\n                (entity_uri, RDFS.comment, Literal(self.defs.description, lang=\"en\"))\n            )\n        # Adding properties relationships to the entities\n        for assigned_prop in self._base_attrs:\n            prop_uri = self._add_properties_rdf(namespace, graph, assigned_prop, logger)\n            restriction = BNode()\n            graph.add((restriction, RDF.type, OWL.Restriction))\n            if assigned_prop.mandatory:\n                graph.add(\n                    (restriction, OWL.onProperty, namespace[\"hasMandatoryProperty\"])\n                )\n            else:\n                graph.add(\n                    (restriction, OWL.onProperty, namespace[\"hasOptionalProperty\"])\n                )\n            graph.add((restriction, OWL.someValuesFrom, prop_uri))\n\n            # Add the restriction as a subclass of the entity\n            graph.add((entity_uri, RDFS.subClassOf, restriction))\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.entities.BaseEntity.code","title":"<code>code = Field(default=None, description='\\n        Code of the entity to assign as permanent identifier in openBIS.\\n        ')</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.entities.BaseEntity.__str__","title":"<code>__str__ = __repr__</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.entities.BaseEntity.cls_name","title":"<code>cls_name</code>","text":"<p>Returns the entity name of the class as a string to speed up checks. This is a property to be overwritten by each of the abstract entity types.</p>"},{"location":"references/api/#bam_masterdata.metadata.entities.BaseEntity.__init__","title":"<code>__init__(**kwargs)</code>","text":"Source code in <code>bam_masterdata/metadata/entities.py</code> <pre><code>def __init__(self, **kwargs):\n    super().__init__()\n\n    # We store the `_property_metadata` during instantiation of the class\n    self._property_metadata = self.get_property_metadata()\n\n    for key, value in kwargs.items():\n        setattr(self, key, value)\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.entities.BaseEntity.__setattr__","title":"<code>__setattr__(key, value)</code>","text":"Source code in <code>bam_masterdata/metadata/entities.py</code> <pre><code>def __setattr__(self, key, value):\n    if key == \"_property_metadata\":\n        super().__setattr__(key, value)\n        return\n\n    if key in self._property_metadata:\n        # TODO add CONTROLLEDVOCABULARY and OBJECT cases\n        expected_type = self._property_metadata[key].data_type.pytype\n        if expected_type and not isinstance(value, expected_type):\n            raise TypeError(\n                f\"Invalid type for '{key}': Expected {expected_type.__name__}, got {type(value).__name__}\"\n            )\n\n    # TODO add check if someone tries to set up a definition instead of an assigned property\n\n    object.__setattr__(self, key, value)\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.entities.BaseEntity.__repr__","title":"<code>__repr__()</code>","text":"Source code in <code>bam_masterdata/metadata/entities.py</code> <pre><code>def __repr__(self):\n    # Filter for attributes that are `PropertyTypeAssignment` and set to a finite value\n    class_prop_name = None\n    fields = []\n    for key, metadata in self._property_metadata.items():\n        if isinstance(metadata, PropertyTypeAssignment):\n            value = getattr(self, key, None)\n            # Only include set attributes\n            if value is not None and not isinstance(value, PropertyTypeAssignment):\n                if key == \"name\":\n                    class_prop_name = value\n                fields.append(f\"{key}={repr(value)}\")\n\n    # Format the output\n    class_name = self.cls_name\n    if class_prop_name:  # adding `name` if available\n        class_name = f\"{class_prop_name}:{class_name}\"\n    return f\"{class_name}({', '.join(fields)})\"\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.entities.BaseEntity.get_property_metadata","title":"<code>get_property_metadata()</code>","text":"<p>Dictionary containing the metadata of the properties assigned to the entity type.</p> RETURNS DESCRIPTION <code>dict</code> <p>A dictionary containing the keys of the <code>PropertyTypeAssignment</code> attribute names and the</p> <p> TYPE: <code>dict</code> </p> <code>dict</code> <p>values of the definitions of <code>PropertyTypeAssignment</code>. Example:</p> <code>dict</code> <p>{ \"name\": PropertyTypeAssignment(     code=\"$NAME\",     data_type=VARCHAR,     mandatory=True,     property_label=\"Name\" ), \"age\": PropertyTypeAssignment(     code=\"AGE\",     data_type=INTEGER,     mandatory=False,     property_label=\"Age\" ),</p> <code>dict</code> <p>}</p> Source code in <code>bam_masterdata/metadata/entities.py</code> <pre><code>def get_property_metadata(self) -&gt; dict:\n    \"\"\"\n    Dictionary containing the metadata of the properties assigned to the entity type.\n\n    Returns:\n        dict: A dictionary containing the keys of the `PropertyTypeAssignment` attribute names and the\n        values of the definitions of `PropertyTypeAssignment`. Example:\n        {\n            \"name\": PropertyTypeAssignment(\n                code=\"$NAME\",\n                data_type=VARCHAR,\n                mandatory=True,\n                property_label=\"Name\"\n            ),\n            \"age\": PropertyTypeAssignment(\n                code=\"AGE\",\n                data_type=INTEGER,\n                mandatory=False,\n                property_label=\"Age\"\n            ),\n        }\n    \"\"\"\n    cls_attrs = self.__class__.__dict__\n\n    # Store property metadata at class level\n    prop_meta_dict: dict = {}\n    for base in type(self).__mro__:\n        cls_attrs = getattr(base, \"__dict__\", {})\n        for attr_name, attr_value in cls_attrs.items():\n            if isinstance(attr_value, PropertyTypeAssignment):\n                prop_meta_dict[attr_name] = attr_value\n    return prop_meta_dict\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.entities.BaseEntity.to_json","title":"<code>to_json(indent=None)</code>","text":"<p>Returns the entity as a string in JSON format storing the value of the properties assigned to the entity.</p> PARAMETER DESCRIPTION <code>indent</code> <p>The indent to print in JSON. Defaults to None.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The JSON representation of the entity.</p> <p> TYPE: <code>str</code> </p> Source code in <code>bam_masterdata/metadata/entities.py</code> <pre><code>def to_json(self, indent: int | None = None) -&gt; str:\n    \"\"\"\n    Returns the entity as a string in JSON format storing the value of the properties\n    assigned to the entity.\n\n    Args:\n        indent (Optional[int], optional): The indent to print in JSON. Defaults to None.\n\n    Returns:\n        str: The JSON representation of the entity.\n    \"\"\"\n    data: dict = {}\n    for key in self._property_metadata.keys():\n        try:\n            value = getattr(self, key)\n        except AttributeError:\n            continue\n        if isinstance(value, PropertyTypeAssignment):\n            continue\n        data[key] = value\n    return json.dumps(data, indent=indent)\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.entities.BaseEntity.to_dict","title":"<code>to_dict()</code>","text":"<p>Returns the entity as a dictionary storing the value of the properties assigned to the entity.</p> RETURNS DESCRIPTION <code>dict</code> <p>The dictionary representation of the entity.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>bam_masterdata/metadata/entities.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"\n    Returns the entity as a dictionary storing the value of the properties assigned to the entity.\n\n    Returns:\n        dict: The dictionary representation of the entity.\n    \"\"\"\n    dump_json = self.to_json()\n    return json.loads(dump_json)\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.entities.BaseEntity.to_hdf5","title":"<code>to_hdf5(hdf_file, group_name='')</code>","text":"<p>Serialize the entity to a HDF5 file under the group specified in the input.</p> PARAMETER DESCRIPTION <code>hdf_file</code> <p>The HDF5 file to store the entity.</p> <p> TYPE: <code>File</code> </p> <code>group_name</code> <p>The group name to serialize the data.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> Source code in <code>bam_masterdata/metadata/entities.py</code> <pre><code>def to_hdf5(self, hdf_file: h5py.File, group_name: str = \"\") -&gt; h5py.File:\n    \"\"\"\n    Serialize the entity to a HDF5 file under the group specified in the input.\n\n    Args:\n        hdf_file (h5py.File): The HDF5 file to store the entity.\n        group_name (str, optional): The group name to serialize the data.\n    \"\"\"\n    if not group_name:\n        group_name = self.cls_name\n    group = hdf_file.create_group(group_name)\n\n    for key in self._property_metadata.keys():\n        try:\n            value = getattr(self, key)\n            if not value:\n                continue\n            if isinstance(value, str | int | float | bool | list | tuple):\n                group.create_dataset(key, data=value)\n            else:\n                raise TypeError(\n                    f\"Unsupported type {type(value)} for key {key} for HDF5 serialization.\"\n                )\n        except AttributeError:\n            continue\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.entities.BaseEntity.model_to_dict","title":"<code>model_to_dict()</code>","text":"<p>Returns the model as a dictionary storing the data <code>defs</code> and the property or vocabulary term assignments.</p> RETURNS DESCRIPTION <code>dict</code> <p>The dictionary representation of the model.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>bam_masterdata/metadata/entities.py</code> <pre><code>def model_to_dict(self) -&gt; dict:\n    \"\"\"\n    Returns the model as a dictionary storing the data `defs` and the property or vocabulary term\n    assignments.\n\n    Returns:\n        dict: The dictionary representation of the model.\n    \"\"\"\n    data = self.model_dump()\n\n    attr_value = getattr(self, \"defs\")\n    if isinstance(attr_value, BaseModel):\n        data[\"defs\"] = attr_value.model_dump()\n    else:\n        data[\"defs\"] = attr_value\n    return data\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.entities.BaseEntity.model_to_json","title":"<code>model_to_json(indent=None)</code>","text":"<p>Returns the model as a string in JSON format storing the data <code>defs</code> and the property or vocabulary term assignments.</p> PARAMETER DESCRIPTION <code>indent</code> <p>The indent to print in JSON. Defaults to None.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The JSON representation of the model.</p> <p> TYPE: <code>str</code> </p> Source code in <code>bam_masterdata/metadata/entities.py</code> <pre><code>def model_to_json(self, indent: int | None = None) -&gt; str:\n    \"\"\"\n    Returns the model as a string in JSON format storing the data `defs` and the property or\n    vocabulary term assignments.\n\n    Args:\n        indent (Optional[int], optional): The indent to print in JSON. Defaults to None.\n\n    Returns:\n        str: The JSON representation of the model.\n    \"\"\"\n    # * `model_dump_json()` from pydantic does not store the `defs` section of each entity.\n    data = self.model_to_dict()\n    return json.dumps(data, indent=indent)\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.entities.BaseEntity.model_to_rdf","title":"<code>model_to_rdf(namespace, graph, logger)</code>","text":"<p>Convert the entity to RDF triples and add them to the graph. The function uses the <code>_add_properties_rdf</code> method to convert the properties assigned to the entity to RDF triples.</p> PARAMETER DESCRIPTION <code>namespace</code> <p>The namespace to use for the RDF graph.</p> <p> TYPE: <code>Namespace</code> </p> <code>graph</code> <p>The RDF graph to which the entity is added.</p> <p> TYPE: <code>Graph</code> </p> <code>logger</code> <p>The logger to log messages.</p> <p> TYPE: <code>BoundLoggerLazyProxy</code> </p> Source code in <code>bam_masterdata/metadata/entities.py</code> <pre><code>@no_type_check\ndef model_to_rdf(\n    self, namespace: \"Namespace\", graph: \"Graph\", logger: \"BoundLoggerLazyProxy\"\n) -&gt; None:\n    \"\"\"\n    Convert the entity to RDF triples and add them to the graph. The function uses the\n    `_add_properties_rdf` method to convert the properties assigned to the entity to RDF triples.\n\n    Args:\n        namespace (Namespace): The namespace to use for the RDF graph.\n        graph (Graph): The RDF graph to which the entity is added.\n        logger (BoundLoggerLazyProxy): The logger to log messages.\n    \"\"\"\n    entity_uri = namespace[self.defs.id]\n\n    # Define the entity as an OWL class inheriting from the specific namespace type\n    graph.add((entity_uri, RDF.type, OWL.Thing))\n    parent_classes = self.__class__.__bases__\n    for parent_class in parent_classes:\n        if issubclass(parent_class, BaseEntity) and parent_class != BaseEntity:\n            # if parent_class.__name__ in [\n            #     \"ObjectType\",\n            #     \"CollectionType\",\n            #     \"DatasetType\",\n            # ]:\n            #     # ! add here logic of subClassOf connecting with PROV-O or BFO\n            #     # ! maybe via classes instead of ObjectType/CollectionType/DatasetType?\n            #     # ! Example:\n            #     # !     graph.add((entity_uri, RDFS.subClassOf, \"http://www.w3.org/ns/prov#Entity\"))\n            #     continue\n            parent_uri = namespace[parent_class.__name__]\n            graph.add((entity_uri, RDFS.subClassOf, parent_uri))\n\n    # Add attributes like id, code, description in English and Deutsch, property_label, data_type\n    graph.add((entity_uri, RDFS.label, Literal(self.defs.id, lang=\"en\")))\n    graph.add((entity_uri, DC.identifier, Literal(self.defs.code)))\n    descriptions = self.defs.description.split(\"//\")\n    if len(descriptions) &gt; 1:\n        graph.add((entity_uri, RDFS.comment, Literal(descriptions[0], lang=\"en\")))\n        graph.add((entity_uri, RDFS.comment, Literal(descriptions[1], lang=\"de\")))\n    else:\n        graph.add(\n            (entity_uri, RDFS.comment, Literal(self.defs.description, lang=\"en\"))\n        )\n    # Adding properties relationships to the entities\n    for assigned_prop in self._base_attrs:\n        prop_uri = self._add_properties_rdf(namespace, graph, assigned_prop, logger)\n        restriction = BNode()\n        graph.add((restriction, RDF.type, OWL.Restriction))\n        if assigned_prop.mandatory:\n            graph.add(\n                (restriction, OWL.onProperty, namespace[\"hasMandatoryProperty\"])\n            )\n        else:\n            graph.add(\n                (restriction, OWL.onProperty, namespace[\"hasOptionalProperty\"])\n            )\n        graph.add((restriction, OWL.someValuesFrom, prop_uri))\n\n        # Add the restriction as a subclass of the entity\n        graph.add((entity_uri, RDFS.subClassOf, restriction))\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.entities.ObjectType","title":"<code>ObjectType</code>","text":"<p>               Bases: <code>BaseEntity</code></p> <p>Base class used to define object types. All object types must inherit from this class. The object types are defined in the module <code>bam_masterdata/object_types.py</code>.</p> <p>The <code>ObjectType</code> class contains a list of all <code>properties</code> defined for a <code>ObjectType</code>, for internally represent the model in other formats (e.g., JSON or Excel).</p> <p>Note this is also used for <code>CollectionType</code> and <code>DatasetType</code>, as they also contain a list of properties.</p> Source code in <code>bam_masterdata/metadata/entities.py</code> <pre><code>class ObjectType(BaseEntity):\n    \"\"\"\n    Base class used to define object types. All object types must inherit from this class. The\n    object types are defined in the module `bam_masterdata/object_types.py`.\n\n    The `ObjectType` class contains a list of all `properties` defined for a `ObjectType`, for\n    internally represent the model in other formats (e.g., JSON or Excel).\n\n    Note this is also used for `CollectionType` and `DatasetType`, as they also contain a list of\n    properties.\n    \"\"\"\n\n    model_config = ConfigDict(\n        ignored_types=(\n            ObjectTypeDef,\n            CollectionTypeDef,\n            DatasetTypeDef,\n            PropertyTypeAssignment,\n        )\n    )\n\n    properties: list[PropertyTypeAssignment] = Field(\n        default=[],\n        description=\"\"\"\n        List of properties assigned to an object type. This is useful for internal representation of the model.\n        \"\"\",\n    )\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        # Initialize the properties list to store PropertyTypeAssignment instances\n        self._properties = {}\n        for key, prop in self._property_metadata.items():\n            self._properties[key] = prop.data_type\n\n    def _set_object_value(self, key, value):\n        \"\"\"\n        Sets the value when the data type is OBJECT.\n        \"\"\"\n        if isinstance(value, str):\n            # Validate the path format: /{space}/{project}/{collection}/{object} or /{space}/{project}/{object}\n            # If path is valid, store it as-is\n            if not value.startswith(\"/\"):\n                raise ValueError(\n                    f\"Invalid OBJECT path format for '{key}': Path must start with '/', got '{value}'\"\n                )\n            path_parts = value.strip(\"/\").split(\"/\")\n            if len(path_parts) not in [3, 4]:\n                raise ValueError(\n                    f\"Invalid OBJECT path format for '{key}': Expected '/&lt;space&gt;/&lt;project&gt;/&lt;collection&gt;/&lt;object&gt;' \"\n                    f\"or '/&lt;space&gt;/&lt;project&gt;/&lt;object&gt;', got '{value}'\"\n                )\n            # * We don't validate if the object exists here as it requires pybis connection\n            # * That validation should be done when saving to openBIS\n        elif isinstance(value, ObjectType):\n            # Check if the object instance has a code\n            if not hasattr(value, \"code\") or value.code is None:\n                raise ValueError(\n                    f\"Object instance for '{key}' must have a 'code' attribute set to be used as a reference\"\n                )\n        else:\n            raise TypeError(\n                f\"Invalid type for OBJECT property '{key}': Expected str (path) or ObjectType instance, \"\n                f\"got {type(value).__name__}\"\n            )\n        return value\n\n    def _validate_controlled_vocabulary(self, meta, key, value) -&gt; None:\n        \"\"\"\n        Validates the value of a CONTROLLEDVOCABULARY.\n        \"\"\"\n        vocabulary_code = meta[key].vocabulary_code\n\n        # Patch to handle institutional vocabularies\n        if vocabulary_code in [\n            \"BAM_FLOOR\",\n            \"BAM_HOUSE\",\n            \"BAM_LOCATION\",\n            \"BAM_LOCATION_COMPLETE\",\n            \"BAM_OE\",\n            \"BAM_ROOM\",\n            \"PERSON_STATUS\",\n        ]:\n            warnings.warn(\n                f\"The attribute '{key}' uses the institutional vocabulary '{vocabulary_code}'. \"\n                \"This value will not be validated against internal vocabulary definitions.\",\n                UserWarning,\n                stacklevel=3,\n            )\n            return None\n\n        if not vocabulary_code:\n            raise ValueError(\n                f\"Property '{key}' of type CONTROLLEDVOCABULARY must have a vocabulary_code defined.\"\n            )\n        vocab_path = None\n        for file in listdir_py_modules(DATAMODEL_DIR):\n            if \"vocabulary_types.py\" in file:\n                vocab_path = file\n                break\n        if vocab_path is None:\n            raise FileNotFoundError(\n                f\"The file 'vocabulary_types.py' was not found in the directory specified by {DATAMODEL_DIR}.\"\n            )\n        vocabulary_class = self.get_vocabulary_class(vocabulary_code, vocab_path)\n        if vocabulary_class is None:\n            raise ValueError(\n                f\"No matching vocabulary class found for vocabulary_code '{vocabulary_code}'.\"\n            )\n        codes = [term.code for term in vocabulary_class.terms]\n        if value not in codes:\n            raise ValueError(\n                f\"{value} for {key} is not in the list of allowed terms for vocabulary.\"\n            )\n\n    def __setattr__(self, key, value):\n        if key in [\"_property_metadata\", \"_properties\", \"code\"]:\n            super().__setattr__(key, value)\n            return\n\n        # key search in every nested class\n        for base in type(self).__mro__:\n            prop_meta = getattr(base, \"get_property_metadata\", None)\n            if callable(prop_meta):\n                meta = (\n                    prop_meta(self)\n                    if base is not type(self)\n                    else self._property_metadata\n                )\n                if key in meta:\n                    # Type check\n                    expected_type = meta[key].data_type.pytype\n                    if expected_type is datetime.datetime:\n                        if isinstance(value, datetime.datetime):\n                            try:\n                                value = value.strftime(\n                                    \"%Y-%m-%d %H:%M:%S\"\n                                )  # create string\n                                expected_type = str\n                            except ValueError:\n                                raise ValueError(\n                                    f\"Invalid datetime format for '{key}': Expected ISO format string, got '{value}'\"\n                                )\n                        elif isinstance(value, str):\n                            try:\n                                datetime.datetime.fromisoformat(value)\n                                expected_type = str\n                            except ValueError:\n                                raise ValueError(\n                                    f\"Invalid datetime format for '{key}': Expected ISO format string, got '{value}'\"\n                                )\n                        else:\n                            raise TypeError(\n                                f\"Invalid type for '{key}': Expected datetime or ISO format string, got {type(value).__name__}\"\n                            )\n                    if expected_type and not isinstance(value, expected_type):\n                        raise TypeError(\n                            f\"Invalid type for '{key}': Expected {expected_type.__name__}, got {type(value).__name__}\"\n                        )\n\n                    # Get data type for additional checks\n                    data_type = meta[key].data_type\n                    # OBJECT check and attr assignment\n                    if data_type == \"OBJECT\":\n                        return object.__setattr__(\n                            self, key, self._set_object_value(key, value)\n                        )\n                    # CONTROLLEDVOCABULARY check\n                    if data_type == \"CONTROLLEDVOCABULARY\":\n                        self._validate_controlled_vocabulary(meta, key, value)\n\n                    # Setting attribute value after all checks\n                    return object.__setattr__(self, key, value)\n\n        raise KeyError(\n            f\"Key '{key}' not found in any property_metadata of {type(self).__name__} or its bases.\"\n        )\n\n    def get_vocabulary_class(\n        self, vocabulary_code: str, vocab_path: str\n    ) -&gt; VocabularyType | None:\n        \"\"\"\n        Get the class instance of the vocabulary type defined by `vocabulary_code` in the Python module\n        specified by `vocab_path`.\n\n        Args:\n            vocabulary_code (str): Code of the vocabulary type to get.\n            vocab_path (str): Path to the module containing the vocabulary type definitions.\n\n        Returns:\n            VocabularyType | None: The class of the vocabulary type if found, otherwise None.\n        \"\"\"\n        module = import_module(vocab_path)\n        vocabulary_class = None\n        for name, obj in inspect.getmembers(module, inspect.isclass):\n            if name == code_to_class_name(vocabulary_code):\n                vocabulary_class = obj()\n                break\n\n        return vocabulary_class\n\n    @property\n    def base_name(self) -&gt; str:\n        \"\"\"\n        Returns the entity name of the class as a string.\n        \"\"\"\n        return \"ObjectType\"\n\n    @model_validator(mode=\"after\")\n    @classmethod\n    def model_validator_after_init(cls, data: Any) -&gt; Any:\n        \"\"\"\n        Validate the model after instantiation of the class.\n\n        Args:\n            data (Any): The data containing the fields values to validate.\n\n        Returns:\n            Any: The data with the validated fields.\n        \"\"\"\n        # Add all the properties assigned to the object type to the `properties` list.\n        # TODO check if the order is properly assigned\n        for base in cls.__mro__:\n            for _, attr_val in base.__dict__.items():\n                if isinstance(attr_val, PropertyTypeAssignment):\n                    data.properties.append(attr_val)\n\n        return data\n\n    def to_openbis(\n        self,\n        logger: \"BoundLoggerLazyProxy\",\n        openbis: \"Openbis\",\n        type: str = \"object\",\n        type_map: dict = OBJECT_TYPE_MAP,\n    ) -&gt; None:\n        def get_type(openbis: \"Openbis\", code: str):\n            return openbis.get_object_type(code)\n\n        def create_type(openbis: \"Openbis\", defs: ObjectTypeDef):\n            return openbis.new_object_type(\n                code=defs.code,\n                description=defs.description,\n                validationPlugin=defs.validation_script,\n                generatedCodePrefix=defs.generated_code_prefix,\n                autoGeneratedCode=defs.auto_generate_codes,\n            )\n\n        super()._to_openbis(\n            logger=logger,\n            openbis=openbis,\n            type=type,\n            type_map=type_map,\n            get_type=get_type,\n            create_type=create_type,\n        )\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.entities.ObjectType.model_config","title":"<code>model_config = ConfigDict(ignored_types=(ObjectTypeDef, CollectionTypeDef, DatasetTypeDef, PropertyTypeAssignment))</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.entities.ObjectType.properties","title":"<code>properties = Field(default=[], description='\\n        List of properties assigned to an object type. This is useful for internal representation of the model.\\n        ')</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.entities.ObjectType.base_name","title":"<code>base_name</code>","text":"<p>Returns the entity name of the class as a string.</p>"},{"location":"references/api/#bam_masterdata.metadata.entities.ObjectType.__init__","title":"<code>__init__(**kwargs)</code>","text":"Source code in <code>bam_masterdata/metadata/entities.py</code> <pre><code>def __init__(self, **kwargs):\n    super().__init__(**kwargs)\n\n    # Initialize the properties list to store PropertyTypeAssignment instances\n    self._properties = {}\n    for key, prop in self._property_metadata.items():\n        self._properties[key] = prop.data_type\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.entities.ObjectType.__setattr__","title":"<code>__setattr__(key, value)</code>","text":"Source code in <code>bam_masterdata/metadata/entities.py</code> <pre><code>def __setattr__(self, key, value):\n    if key in [\"_property_metadata\", \"_properties\", \"code\"]:\n        super().__setattr__(key, value)\n        return\n\n    # key search in every nested class\n    for base in type(self).__mro__:\n        prop_meta = getattr(base, \"get_property_metadata\", None)\n        if callable(prop_meta):\n            meta = (\n                prop_meta(self)\n                if base is not type(self)\n                else self._property_metadata\n            )\n            if key in meta:\n                # Type check\n                expected_type = meta[key].data_type.pytype\n                if expected_type is datetime.datetime:\n                    if isinstance(value, datetime.datetime):\n                        try:\n                            value = value.strftime(\n                                \"%Y-%m-%d %H:%M:%S\"\n                            )  # create string\n                            expected_type = str\n                        except ValueError:\n                            raise ValueError(\n                                f\"Invalid datetime format for '{key}': Expected ISO format string, got '{value}'\"\n                            )\n                    elif isinstance(value, str):\n                        try:\n                            datetime.datetime.fromisoformat(value)\n                            expected_type = str\n                        except ValueError:\n                            raise ValueError(\n                                f\"Invalid datetime format for '{key}': Expected ISO format string, got '{value}'\"\n                            )\n                    else:\n                        raise TypeError(\n                            f\"Invalid type for '{key}': Expected datetime or ISO format string, got {type(value).__name__}\"\n                        )\n                if expected_type and not isinstance(value, expected_type):\n                    raise TypeError(\n                        f\"Invalid type for '{key}': Expected {expected_type.__name__}, got {type(value).__name__}\"\n                    )\n\n                # Get data type for additional checks\n                data_type = meta[key].data_type\n                # OBJECT check and attr assignment\n                if data_type == \"OBJECT\":\n                    return object.__setattr__(\n                        self, key, self._set_object_value(key, value)\n                    )\n                # CONTROLLEDVOCABULARY check\n                if data_type == \"CONTROLLEDVOCABULARY\":\n                    self._validate_controlled_vocabulary(meta, key, value)\n\n                # Setting attribute value after all checks\n                return object.__setattr__(self, key, value)\n\n    raise KeyError(\n        f\"Key '{key}' not found in any property_metadata of {type(self).__name__} or its bases.\"\n    )\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.entities.ObjectType.get_vocabulary_class","title":"<code>get_vocabulary_class(vocabulary_code, vocab_path)</code>","text":"<p>Get the class instance of the vocabulary type defined by <code>vocabulary_code</code> in the Python module specified by <code>vocab_path</code>.</p> PARAMETER DESCRIPTION <code>vocabulary_code</code> <p>Code of the vocabulary type to get.</p> <p> TYPE: <code>str</code> </p> <code>vocab_path</code> <p>Path to the module containing the vocabulary type definitions.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>VocabularyType | None</code> <p>VocabularyType | None: The class of the vocabulary type if found, otherwise None.</p> Source code in <code>bam_masterdata/metadata/entities.py</code> <pre><code>def get_vocabulary_class(\n    self, vocabulary_code: str, vocab_path: str\n) -&gt; VocabularyType | None:\n    \"\"\"\n    Get the class instance of the vocabulary type defined by `vocabulary_code` in the Python module\n    specified by `vocab_path`.\n\n    Args:\n        vocabulary_code (str): Code of the vocabulary type to get.\n        vocab_path (str): Path to the module containing the vocabulary type definitions.\n\n    Returns:\n        VocabularyType | None: The class of the vocabulary type if found, otherwise None.\n    \"\"\"\n    module = import_module(vocab_path)\n    vocabulary_class = None\n    for name, obj in inspect.getmembers(module, inspect.isclass):\n        if name == code_to_class_name(vocabulary_code):\n            vocabulary_class = obj()\n            break\n\n    return vocabulary_class\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.entities.ObjectType.model_validator_after_init","title":"<code>model_validator_after_init(data)</code>","text":"<p>Validate the model after instantiation of the class.</p> PARAMETER DESCRIPTION <code>data</code> <p>The data containing the fields values to validate.</p> <p> TYPE: <code>Any</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>The data with the validated fields.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>bam_masterdata/metadata/entities.py</code> <pre><code>@model_validator(mode=\"after\")\n@classmethod\ndef model_validator_after_init(cls, data: Any) -&gt; Any:\n    \"\"\"\n    Validate the model after instantiation of the class.\n\n    Args:\n        data (Any): The data containing the fields values to validate.\n\n    Returns:\n        Any: The data with the validated fields.\n    \"\"\"\n    # Add all the properties assigned to the object type to the `properties` list.\n    # TODO check if the order is properly assigned\n    for base in cls.__mro__:\n        for _, attr_val in base.__dict__.items():\n            if isinstance(attr_val, PropertyTypeAssignment):\n                data.properties.append(attr_val)\n\n    return data\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.entities.ObjectType.to_openbis","title":"<code>to_openbis(logger, openbis, type='object', type_map=OBJECT_TYPE_MAP)</code>","text":"Source code in <code>bam_masterdata/metadata/entities.py</code> <pre><code>def to_openbis(\n    self,\n    logger: \"BoundLoggerLazyProxy\",\n    openbis: \"Openbis\",\n    type: str = \"object\",\n    type_map: dict = OBJECT_TYPE_MAP,\n) -&gt; None:\n    def get_type(openbis: \"Openbis\", code: str):\n        return openbis.get_object_type(code)\n\n    def create_type(openbis: \"Openbis\", defs: ObjectTypeDef):\n        return openbis.new_object_type(\n            code=defs.code,\n            description=defs.description,\n            validationPlugin=defs.validation_script,\n            generatedCodePrefix=defs.generated_code_prefix,\n            autoGeneratedCode=defs.auto_generate_codes,\n        )\n\n    super()._to_openbis(\n        logger=logger,\n        openbis=openbis,\n        type=type,\n        type_map=type_map,\n        get_type=get_type,\n        create_type=create_type,\n    )\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.entities.CollectionType","title":"<code>CollectionType</code>","text":"<p>               Bases: <code>ObjectType</code></p> Source code in <code>bam_masterdata/metadata/entities.py</code> <pre><code>class CollectionType(ObjectType):\n    model_config = ConfigDict(\n        ignored_types=(\n            ObjectTypeDef,\n            ObjectType,\n            CollectionTypeDef,\n            PropertyTypeAssignment,\n        )\n    )\n\n    attached_objects: dict[str, ObjectType] = Field(\n        default={},\n        exclude=True,\n        description=\"\"\"\n        Dictionary containing the object types attached to the collection type.\n        The keys are object unique identifiers and the values are the ObjectType instances.\n        \"\"\",\n    )\n\n    relationships: dict[str, tuple[str, str]] = Field(\n        default={},\n        exclude=True,\n        description=\"\"\"\n        Dictionary containing the relationships between the objects attached to the collection type.\n        The keys are relationships unique identifiers, the values are the object unique identifiers as a\n        tuple, and the order is always (parent_id, child_id).\n        \"\"\",\n    )\n\n    def __repr__(self):\n        return f\"{self.base_name}(attached_objects={self.attached_objects}, relationships={self.relationships})\"\n\n    @property\n    def base_name(self) -&gt; str:\n        \"\"\"\n        Returns the entity name of the class as a string.\n        \"\"\"\n        return \"CollectionType\"\n\n    def to_openbis(\n        self,\n        logger: \"BoundLoggerLazyProxy\",\n        openbis: \"Openbis\",\n        type: str = \"collection\",\n        type_map: dict = COLLECTION_TYPE_MAP,\n    ) -&gt; None:\n        def get_type(openbis: \"Openbis\", code: str):\n            return openbis.get_collection_type(code)\n\n        def create_type(openbis: \"Openbis\", defs: CollectionTypeDef):\n            if defs.validation_script == \"None\":\n                defs.validation_script = None\n            if defs.validation_script:\n                return openbis.new_collection_type(\n                    code=defs.code,\n                    description=defs.description,\n                    validationPlugin=defs.validation_script,\n                )\n            else:\n                return openbis.new_collection_type(\n                    code=defs.code,\n                    description=defs.description,\n                    validationPlugin=\"\",\n                )\n\n        super()._to_openbis(\n            logger=logger,\n            openbis=openbis,\n            type=type,\n            type_map=type_map,\n            get_type=get_type,\n            create_type=create_type,\n        )\n\n    def add(self, object_type: ObjectType) -&gt; str:\n        \"\"\"\n        Add an object type to the collection type.\n\n        Args:\n            object_type (ObjectType): The object type to add to the collection type.\n\n        Returns:\n            str: The unique identifier of the object type assigned in openBIS.\n        \"\"\"\n        if not isinstance(object_type, ObjectType):\n            raise TypeError(\n                f\"Expected an ObjectType instance, got `{type(object_type).__name__}`\"\n            )\n\n        # Check mandatory properties are filled\n        missing_fields = []\n        for attr_name, prop in object_type._property_metadata.items():\n            assigned_prop = getattr(object_type, attr_name, None)\n            if prop.mandatory and isinstance(assigned_prop, PropertyTypeAssignment):\n                missing_fields.append(attr_name)\n\n        if missing_fields:\n            raise ValueError(\n                f\"The following mandatory fields are missing for ObjectType '{object_type.cls_name}': {', '.join(missing_fields)}\"\n            )\n\n        object_id = generate_object_id(object_type)\n        self.attached_objects[object_id] = object_type\n        return object_id\n\n    def remove(self, object_id: str = \"\") -&gt; None:\n        \"\"\"\n        Remove an object type from the collection type by its unique identifier.\n\n        Args:\n            object_id (str, optional): The ID of the object type to be removed from the collection.\n        \"\"\"\n        if not object_id:\n            raise ValueError(\n                \"You must provide an `object_id` to remove the object type from the collection.\"\n            )\n        if object_id not in self.attached_objects.keys():\n            raise ValueError(\n                f\"Object with ID '{object_id}' does not exist in the collection.\"\n            )\n        del self.attached_objects[object_id]\n\n    def add_relationship(self, parent_id: str, child_id: str) -&gt; str:\n        \"\"\"\n        Add a relationship between two object types in the collection type.\n\n        Args:\n            parent_id (str): The unique identifier of the parent object type.\n            child_id (str): The unique identifier of the child object type.\n\n        Returns:\n            str: The unique identifier of the relationship created, which is a concatenation of the parent\n            and child IDs.\n        \"\"\"\n        if not parent_id or not child_id:\n            raise ValueError(\n                \"Both `parent_id` and `child_id` must be provided to add a relationship.\"\n            )\n        if (\n            parent_id not in self.attached_objects.keys()\n            or child_id not in self.attached_objects.keys()\n        ):\n            raise ValueError(\n                \"Both `parent_id` and `child_id` must be assigned to objects attached to the collection.\"\n            )\n        relationship_id = generate_object_relationship_id(parent_id, child_id)\n        self.relationships[relationship_id] = (parent_id, child_id)\n        return relationship_id\n\n    def remove_relationship(self, relationship_id: str) -&gt; None:\n        \"\"\"\n        Remove a relationship from the collection type.\n\n        Args:\n            relationship_id (str): The unique identifier of the relationship to remove.\n        \"\"\"\n        if not relationship_id:\n            raise ValueError(\n                \"You must provide a `relationship_id` to remove the relationship from the collection type.\"\n            )\n        if relationship_id not in self.relationships.keys():\n            raise ValueError(\n                f\"Relationship with ID '{relationship_id}' does not exist in the collection type.\"\n            )\n        del self.relationships[relationship_id]\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.entities.CollectionType.model_config","title":"<code>model_config = ConfigDict(ignored_types=(ObjectTypeDef, ObjectType, CollectionTypeDef, PropertyTypeAssignment))</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.entities.CollectionType.attached_objects","title":"<code>attached_objects = Field(default={}, exclude=True, description='\\n        Dictionary containing the object types attached to the collection type.\\n        The keys are object unique identifiers and the values are the ObjectType instances.\\n        ')</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.entities.CollectionType.relationships","title":"<code>relationships = Field(default={}, exclude=True, description='\\n        Dictionary containing the relationships between the objects attached to the collection type.\\n        The keys are relationships unique identifiers, the values are the object unique identifiers as a\\n        tuple, and the order is always (parent_id, child_id).\\n        ')</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.entities.CollectionType.base_name","title":"<code>base_name</code>","text":"<p>Returns the entity name of the class as a string.</p>"},{"location":"references/api/#bam_masterdata.metadata.entities.CollectionType.__repr__","title":"<code>__repr__()</code>","text":"Source code in <code>bam_masterdata/metadata/entities.py</code> <pre><code>def __repr__(self):\n    return f\"{self.base_name}(attached_objects={self.attached_objects}, relationships={self.relationships})\"\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.entities.CollectionType.to_openbis","title":"<code>to_openbis(logger, openbis, type='collection', type_map=COLLECTION_TYPE_MAP)</code>","text":"Source code in <code>bam_masterdata/metadata/entities.py</code> <pre><code>def to_openbis(\n    self,\n    logger: \"BoundLoggerLazyProxy\",\n    openbis: \"Openbis\",\n    type: str = \"collection\",\n    type_map: dict = COLLECTION_TYPE_MAP,\n) -&gt; None:\n    def get_type(openbis: \"Openbis\", code: str):\n        return openbis.get_collection_type(code)\n\n    def create_type(openbis: \"Openbis\", defs: CollectionTypeDef):\n        if defs.validation_script == \"None\":\n            defs.validation_script = None\n        if defs.validation_script:\n            return openbis.new_collection_type(\n                code=defs.code,\n                description=defs.description,\n                validationPlugin=defs.validation_script,\n            )\n        else:\n            return openbis.new_collection_type(\n                code=defs.code,\n                description=defs.description,\n                validationPlugin=\"\",\n            )\n\n    super()._to_openbis(\n        logger=logger,\n        openbis=openbis,\n        type=type,\n        type_map=type_map,\n        get_type=get_type,\n        create_type=create_type,\n    )\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.entities.CollectionType.add","title":"<code>add(object_type)</code>","text":"<p>Add an object type to the collection type.</p> PARAMETER DESCRIPTION <code>object_type</code> <p>The object type to add to the collection type.</p> <p> TYPE: <code>ObjectType</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The unique identifier of the object type assigned in openBIS.</p> <p> TYPE: <code>str</code> </p> Source code in <code>bam_masterdata/metadata/entities.py</code> <pre><code>def add(self, object_type: ObjectType) -&gt; str:\n    \"\"\"\n    Add an object type to the collection type.\n\n    Args:\n        object_type (ObjectType): The object type to add to the collection type.\n\n    Returns:\n        str: The unique identifier of the object type assigned in openBIS.\n    \"\"\"\n    if not isinstance(object_type, ObjectType):\n        raise TypeError(\n            f\"Expected an ObjectType instance, got `{type(object_type).__name__}`\"\n        )\n\n    # Check mandatory properties are filled\n    missing_fields = []\n    for attr_name, prop in object_type._property_metadata.items():\n        assigned_prop = getattr(object_type, attr_name, None)\n        if prop.mandatory and isinstance(assigned_prop, PropertyTypeAssignment):\n            missing_fields.append(attr_name)\n\n    if missing_fields:\n        raise ValueError(\n            f\"The following mandatory fields are missing for ObjectType '{object_type.cls_name}': {', '.join(missing_fields)}\"\n        )\n\n    object_id = generate_object_id(object_type)\n    self.attached_objects[object_id] = object_type\n    return object_id\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.entities.CollectionType.remove","title":"<code>remove(object_id='')</code>","text":"<p>Remove an object type from the collection type by its unique identifier.</p> PARAMETER DESCRIPTION <code>object_id</code> <p>The ID of the object type to be removed from the collection.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> Source code in <code>bam_masterdata/metadata/entities.py</code> <pre><code>def remove(self, object_id: str = \"\") -&gt; None:\n    \"\"\"\n    Remove an object type from the collection type by its unique identifier.\n\n    Args:\n        object_id (str, optional): The ID of the object type to be removed from the collection.\n    \"\"\"\n    if not object_id:\n        raise ValueError(\n            \"You must provide an `object_id` to remove the object type from the collection.\"\n        )\n    if object_id not in self.attached_objects.keys():\n        raise ValueError(\n            f\"Object with ID '{object_id}' does not exist in the collection.\"\n        )\n    del self.attached_objects[object_id]\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.entities.CollectionType.add_relationship","title":"<code>add_relationship(parent_id, child_id)</code>","text":"<p>Add a relationship between two object types in the collection type.</p> PARAMETER DESCRIPTION <code>parent_id</code> <p>The unique identifier of the parent object type.</p> <p> TYPE: <code>str</code> </p> <code>child_id</code> <p>The unique identifier of the child object type.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The unique identifier of the relationship created, which is a concatenation of the parent</p> <p> TYPE: <code>str</code> </p> <code>str</code> <p>and child IDs.</p> Source code in <code>bam_masterdata/metadata/entities.py</code> <pre><code>def add_relationship(self, parent_id: str, child_id: str) -&gt; str:\n    \"\"\"\n    Add a relationship between two object types in the collection type.\n\n    Args:\n        parent_id (str): The unique identifier of the parent object type.\n        child_id (str): The unique identifier of the child object type.\n\n    Returns:\n        str: The unique identifier of the relationship created, which is a concatenation of the parent\n        and child IDs.\n    \"\"\"\n    if not parent_id or not child_id:\n        raise ValueError(\n            \"Both `parent_id` and `child_id` must be provided to add a relationship.\"\n        )\n    if (\n        parent_id not in self.attached_objects.keys()\n        or child_id not in self.attached_objects.keys()\n    ):\n        raise ValueError(\n            \"Both `parent_id` and `child_id` must be assigned to objects attached to the collection.\"\n        )\n    relationship_id = generate_object_relationship_id(parent_id, child_id)\n    self.relationships[relationship_id] = (parent_id, child_id)\n    return relationship_id\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.entities.CollectionType.remove_relationship","title":"<code>remove_relationship(relationship_id)</code>","text":"<p>Remove a relationship from the collection type.</p> PARAMETER DESCRIPTION <code>relationship_id</code> <p>The unique identifier of the relationship to remove.</p> <p> TYPE: <code>str</code> </p> Source code in <code>bam_masterdata/metadata/entities.py</code> <pre><code>def remove_relationship(self, relationship_id: str) -&gt; None:\n    \"\"\"\n    Remove a relationship from the collection type.\n\n    Args:\n        relationship_id (str): The unique identifier of the relationship to remove.\n    \"\"\"\n    if not relationship_id:\n        raise ValueError(\n            \"You must provide a `relationship_id` to remove the relationship from the collection type.\"\n        )\n    if relationship_id not in self.relationships.keys():\n        raise ValueError(\n            f\"Relationship with ID '{relationship_id}' does not exist in the collection type.\"\n        )\n    del self.relationships[relationship_id]\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.entities.DatasetType","title":"<code>DatasetType</code>","text":"<p>               Bases: <code>ObjectType</code></p> Source code in <code>bam_masterdata/metadata/entities.py</code> <pre><code>class DatasetType(ObjectType):\n    @property\n    def base_name(self) -&gt; str:\n        \"\"\"\n        Returns the entity name of the class as a string.\n        \"\"\"\n        return \"DatasetType\"\n\n    def to_openbis(\n        self,\n        logger: \"BoundLoggerLazyProxy\",\n        openbis: \"Openbis\",\n        type: str = \"dataset\",\n        type_map: dict = DATASET_TYPE_MAP,\n    ) -&gt; None:\n        def get_type(openbis: \"Openbis\", code: str):\n            return openbis.get_dataset_type(code)\n\n        def create_type(openbis: \"Openbis\", defs: DatasetTypeDef):\n            return openbis.new_dataset_type(\n                code=defs.code,\n                description=defs.description,\n                validationPlugin=defs.validation_script,\n                # This is not accepted by openBIS when creating dataset types\n                # mainDatasetPattern=defs.main_dataset_pattern,\n                # mainDatasetPath=defs.main_dataset_path,\n            )\n\n        super()._to_openbis(\n            logger=logger,\n            openbis=openbis,\n            type=type,\n            type_map=type_map,\n            get_type=get_type,\n            create_type=create_type,\n        )\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.entities.DatasetType.base_name","title":"<code>base_name</code>","text":"<p>Returns the entity name of the class as a string.</p>"},{"location":"references/api/#bam_masterdata.metadata.entities.DatasetType.to_openbis","title":"<code>to_openbis(logger, openbis, type='dataset', type_map=DATASET_TYPE_MAP)</code>","text":"Source code in <code>bam_masterdata/metadata/entities.py</code> <pre><code>def to_openbis(\n    self,\n    logger: \"BoundLoggerLazyProxy\",\n    openbis: \"Openbis\",\n    type: str = \"dataset\",\n    type_map: dict = DATASET_TYPE_MAP,\n) -&gt; None:\n    def get_type(openbis: \"Openbis\", code: str):\n        return openbis.get_dataset_type(code)\n\n    def create_type(openbis: \"Openbis\", defs: DatasetTypeDef):\n        return openbis.new_dataset_type(\n            code=defs.code,\n            description=defs.description,\n            validationPlugin=defs.validation_script,\n            # This is not accepted by openBIS when creating dataset types\n            # mainDatasetPattern=defs.main_dataset_pattern,\n            # mainDatasetPath=defs.main_dataset_path,\n        )\n\n    super()._to_openbis(\n        logger=logger,\n        openbis=openbis,\n        type=type,\n        type_map=type_map,\n        get_type=get_type,\n        create_type=create_type,\n    )\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.entities.VocabularyType","title":"<code>VocabularyType</code>","text":"<p>               Bases: <code>BaseEntity</code></p> <p>Base class used to define vocabulary types. All vocabulary types must inherit from this class. The vocabulary types are defined in the module <code>bam_masterdata/vocabulary_types.py</code>.</p> <p>The <code>VocabularyType</code> class contains a list of all <code>terms</code> defined for a <code>VocabularyType</code>, for internally represent the model in other formats (e.g., JSON or Excel).</p> Source code in <code>bam_masterdata/metadata/entities.py</code> <pre><code>class VocabularyType(BaseEntity):\n    \"\"\"\n    Base class used to define vocabulary types. All vocabulary types must inherit from this class. The\n    vocabulary types are defined in the module `bam_masterdata/vocabulary_types.py`.\n\n    The `VocabularyType` class contains a list of all `terms` defined for a `VocabularyType`, for\n    internally represent the model in other formats (e.g., JSON or Excel).\n    \"\"\"\n\n    model_config = ConfigDict(ignored_types=(VocabularyTypeDef, VocabularyTerm))\n\n    terms: list[VocabularyTerm] = Field(\n        default=[],\n        description=\"\"\"\n        List of vocabulary terms. This is useful for internal representation of the model.\n        \"\"\",\n    )\n\n    @property\n    def base_name(self) -&gt; str:\n        \"\"\"\n        Returns the entity name of the class as a string.\n        \"\"\"\n        return \"VocabularyType\"\n\n    @model_validator(mode=\"after\")\n    @classmethod\n    def model_validator_after_init(cls, data: Any) -&gt; Any:\n        \"\"\"\n        Validate the model after instantiation of the class.\n\n        Args:\n            data (Any): The data containing the fields values to validate.\n\n        Returns:\n            Any: The data with the validated fields.\n        \"\"\"\n        # Add all the vocabulary terms defined in the vocabulary type to the `terms` list.\n        # TODO check if the order is properly assigned\n        for base in cls.__mro__:\n            for attr_name, attr_val in base.__dict__.items():\n                if isinstance(attr_val, VocabularyTerm):\n                    data.terms.append(attr_val)\n\n        return data\n\n    def to_openbis(\n        self,\n        logger: \"BoundLoggerLazyProxy\",\n        openbis: \"Openbis\",\n        type: str = \"vocabulary\",\n        type_map: dict = VOCABULARY_TYPE_MAP,\n    ) -&gt; None:\n        def get_type(openbis: \"Openbis\", code: str):\n            return openbis.get_vocabulary(code)\n\n        def create_type(openbis: \"Openbis\", defs: VocabularyTypeDef, terms: list):\n            return openbis.new_vocabulary(\n                code=defs.code, description=defs.description, terms=terms\n            )\n\n        super()._to_openbis(\n            logger=logger,\n            openbis=openbis,\n            type=type,\n            type_map=type_map,\n            get_type=get_type,\n            create_type=create_type,\n        )\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.entities.VocabularyType.model_config","title":"<code>model_config = ConfigDict(ignored_types=(VocabularyTypeDef, VocabularyTerm))</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.entities.VocabularyType.terms","title":"<code>terms = Field(default=[], description='\\n        List of vocabulary terms. This is useful for internal representation of the model.\\n        ')</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.entities.VocabularyType.base_name","title":"<code>base_name</code>","text":"<p>Returns the entity name of the class as a string.</p>"},{"location":"references/api/#bam_masterdata.metadata.entities.VocabularyType.model_validator_after_init","title":"<code>model_validator_after_init(data)</code>","text":"<p>Validate the model after instantiation of the class.</p> PARAMETER DESCRIPTION <code>data</code> <p>The data containing the fields values to validate.</p> <p> TYPE: <code>Any</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>The data with the validated fields.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>bam_masterdata/metadata/entities.py</code> <pre><code>@model_validator(mode=\"after\")\n@classmethod\ndef model_validator_after_init(cls, data: Any) -&gt; Any:\n    \"\"\"\n    Validate the model after instantiation of the class.\n\n    Args:\n        data (Any): The data containing the fields values to validate.\n\n    Returns:\n        Any: The data with the validated fields.\n    \"\"\"\n    # Add all the vocabulary terms defined in the vocabulary type to the `terms` list.\n    # TODO check if the order is properly assigned\n    for base in cls.__mro__:\n        for attr_name, attr_val in base.__dict__.items():\n            if isinstance(attr_val, VocabularyTerm):\n                data.terms.append(attr_val)\n\n    return data\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.entities.VocabularyType.to_openbis","title":"<code>to_openbis(logger, openbis, type='vocabulary', type_map=VOCABULARY_TYPE_MAP)</code>","text":"Source code in <code>bam_masterdata/metadata/entities.py</code> <pre><code>def to_openbis(\n    self,\n    logger: \"BoundLoggerLazyProxy\",\n    openbis: \"Openbis\",\n    type: str = \"vocabulary\",\n    type_map: dict = VOCABULARY_TYPE_MAP,\n) -&gt; None:\n    def get_type(openbis: \"Openbis\", code: str):\n        return openbis.get_vocabulary(code)\n\n    def create_type(openbis: \"Openbis\", defs: VocabularyTypeDef, terms: list):\n        return openbis.new_vocabulary(\n            code=defs.code, description=defs.description, terms=terms\n        )\n\n    super()._to_openbis(\n        logger=logger,\n        openbis=openbis,\n        type=type,\n        type_map=type_map,\n        get_type=get_type,\n        create_type=create_type,\n    )\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.definitions","title":"<code>bam_masterdata.metadata.definitions</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.definitions.EntityDef","title":"<code>EntityDef</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Abstract base class for all masterdata entity definitions. The entity definitions are immutable properties. This class provides a common interface (with common attributes like <code>code</code> and <code>description</code>.) for all entity definitions.</p> Source code in <code>bam_masterdata/metadata/definitions.py</code> <pre><code>class EntityDef(BaseModel):\n    \"\"\"\n    Abstract base class for all masterdata entity definitions. The entity definitions are immutable properties.\n    This class provides a common interface (with common attributes like `code` and\n    `description`.) for all entity definitions.\n    \"\"\"\n\n    code: str = Field(\n        ...,\n        description=\"\"\"\n        Code string identifying the entity with an openBIS inventory definition. Note that:\n\n        - Must be uppercase and separated by underscores, e.g. `'EXPERIMENTAL_STEP'`.\n        - If the entity is native to openBIS, the code must start with a dollar sign, e.g. `'$NAME'`.\n        - In the case of inheritance, it needs to be separated by dots, e.g. `'WELDING_EQUIPMENT.INSTRUMENT'`.\n        \"\"\",\n    )\n\n    description: str = Field(\n        ...,\n        description=\"\"\"\n        Description of the entity. This is the human-readable text for the object and must be\n        as complete and concise as possible. The German description can be added after the English\n        description separated by a double slash (//), e.g. `'Chemical Substance//Chemische Substanz'`.\n        \"\"\",\n    )\n\n    # TODO: check if it is necessary to add something like `ontology_annotation_id` in the future\n    iri: str | None = Field(\n        default=None,\n        description=\"\"\"\n        IRI (Internationalized Resource Identifier) of the entity. This is a unique identifier for the entity\n        and is used to link the entity to an ontology. It is a string with the format `\"&lt;ontology_id&gt;:&lt;ontology_version&gt;\"`.\n        Example: \"http://purl.obolibrary.org/bam-masterdata/Instrument:1.0.0\".\n        \"\"\",\n    )\n\n    id: str | None = Field(\n        default=None,\n        description=\"\"\"\n        Identifier of the entity defined as the class name and used to serialize the entity definitions\n        in other formats.\n        \"\"\",\n    )\n\n    row_location: str | None = Field(\n        default=None,\n        description=\"\"\"\n        Row in the Excel at which the entity type field is defined. It is a string with the format `\"&lt;row-letter&gt;&lt;row_number&gt;\"`.\n        Example: \"A1\" ot \"A107\". This field is useful when checking the consistency of Excel files with multiple entity\n        types defined to quickly locate the specific Excel cell which logs a message when applying the `checker` CLI.\n        \"\"\",\n    )\n\n    # TODO check ontology_id, ontology_version, ontology_annotation_id, internal (found in the openBIS docu)\n\n    @field_validator(\"code\")\n    @classmethod\n    def validate_code(cls, value: str) -&gt; str:\n        if not value or not re.match(r\"^[\\w_\\$\\.\\-\\+]+$\", value):\n            raise ValueError(\n                \"`code` must follow the rules specified in the description: 1) Must be uppercase, \"\n                \"2) separated by underscores, 3) start with a dollar sign if native to openBIS, \"\n                \"4) separated by dots if there is inheritance.\"\n            )\n        return value\n\n    @field_validator(\"iri\")\n    @classmethod\n    def validate_iri(cls, value: str | None) -&gt; str | None:\n        if not value:\n            return value\n        if not re.match(\n            r\"^http://purl.obolibrary.org/bam-masterdata/[\\w_]+:[\\d.]+$\", value\n        ):\n            raise ValueError(\n                \"`iri` must follow the rules specified in the description: 1) Must start with 'http://purl.obolibrary.org/bam-masterdata/', \"\n                \"2) followed by the entity name, 3) separated by a colon, 4) followed by the semantic versioning number. \"\n                \"Example: 'http://purl.obolibrary.org/bam-masterdata/Instrument:1.0.0'.\"\n            )\n        return value\n\n    @field_validator(\"description\")\n    @classmethod\n    def strip_description(cls, value: str) -&gt; str:\n        return value.strip()\n\n    @property\n    def name(self) -&gt; str:\n        return self.__class__.__name__\n\n    @property\n    def excel_name(self) -&gt; str:\n        \"\"\"\n        Returns the name of the entity in a format suitable for the openBIS Excel file.\n        \"\"\"\n        name_map = {\n            \"CollectionTypeDef\": \"EXPERIMENT_TYPE\",\n            \"DatasetTypeDef\": \"DATASET_TYPE\",\n            \"ObjectTypeDef\": \"SAMPLE_TYPE\",\n            \"VocabularyTypeDef\": \"VOCABULARY_TYPE\",\n        }\n        return name_map.get(self.name)\n\n    @property\n    def excel_headers_map(self) -&gt; dict:\n        \"\"\"\n        Maps the field keys of the Pydantic model into the openBIS Excel style headers.\n        \"\"\"\n        fields = [\n            k\n            for k in self.model_fields.keys()\n            if k not in [\"iri\", \"id\", \"row_location\"]\n        ]\n        headers: dict = {}\n        for f in fields:\n            headers[f] = f.replace(\"_\", \" \").capitalize()\n        return headers\n\n    @model_validator(mode=\"after\")\n    @classmethod\n    def model_id(cls, data: Any) -&gt; Any:\n        \"\"\"\n        Stores the model `id` as the class name from the `code` field.\n\n        Args:\n            data (Any): The data containing the fields values to validate.\n\n        Returns:\n            Any: The data with the validated fields.\n        \"\"\"\n        if \"PropertyType\" in data.name:\n            data.id = code_to_class_name(code=data.code, entity_type=\"property\")\n        else:\n            data.id = code_to_class_name(code=data.code, entity_type=\"object\")\n        return data\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.definitions.EntityDef.code","title":"<code>code = Field(..., description=\"\\n        Code string identifying the entity with an openBIS inventory definition. Note that:\\n\\n        - Must be uppercase and separated by underscores, e.g. `'EXPERIMENTAL_STEP'`.\\n        - If the entity is native to openBIS, the code must start with a dollar sign, e.g. `'$NAME'`.\\n        - In the case of inheritance, it needs to be separated by dots, e.g. `'WELDING_EQUIPMENT.INSTRUMENT'`.\\n        \")</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.definitions.EntityDef.description","title":"<code>description = Field(..., description=\"\\n        Description of the entity. This is the human-readable text for the object and must be\\n        as complete and concise as possible. The German description can be added after the English\\n        description separated by a double slash (//), e.g. `'Chemical Substance//Chemische Substanz'`.\\n        \")</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.definitions.EntityDef.iri","title":"<code>iri = Field(default=None, description='\\n        IRI (Internationalized Resource Identifier) of the entity. This is a unique identifier for the entity\\n        and is used to link the entity to an ontology. It is a string with the format `\"&lt;ontology_id&gt;:&lt;ontology_version&gt;\"`.\\n        Example: \"http://purl.obolibrary.org/bam-masterdata/Instrument:1.0.0\".\\n        ')</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.definitions.EntityDef.id","title":"<code>id = Field(default=None, description='\\n        Identifier of the entity defined as the class name and used to serialize the entity definitions\\n        in other formats.\\n        ')</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.definitions.EntityDef.row_location","title":"<code>row_location = Field(default=None, description='\\n        Row in the Excel at which the entity type field is defined. It is a string with the format `\"&lt;row-letter&gt;&lt;row_number&gt;\"`.\\n        Example: \"A1\" ot \"A107\". This field is useful when checking the consistency of Excel files with multiple entity\\n        types defined to quickly locate the specific Excel cell which logs a message when applying the `checker` CLI.\\n        ')</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.definitions.EntityDef.name","title":"<code>name</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.definitions.EntityDef.excel_name","title":"<code>excel_name</code>","text":"<p>Returns the name of the entity in a format suitable for the openBIS Excel file.</p>"},{"location":"references/api/#bam_masterdata.metadata.definitions.EntityDef.excel_headers_map","title":"<code>excel_headers_map</code>","text":"<p>Maps the field keys of the Pydantic model into the openBIS Excel style headers.</p>"},{"location":"references/api/#bam_masterdata.metadata.definitions.EntityDef.validate_code","title":"<code>validate_code(value)</code>","text":"Source code in <code>bam_masterdata/metadata/definitions.py</code> <pre><code>@field_validator(\"code\")\n@classmethod\ndef validate_code(cls, value: str) -&gt; str:\n    if not value or not re.match(r\"^[\\w_\\$\\.\\-\\+]+$\", value):\n        raise ValueError(\n            \"`code` must follow the rules specified in the description: 1) Must be uppercase, \"\n            \"2) separated by underscores, 3) start with a dollar sign if native to openBIS, \"\n            \"4) separated by dots if there is inheritance.\"\n        )\n    return value\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.definitions.EntityDef.validate_iri","title":"<code>validate_iri(value)</code>","text":"Source code in <code>bam_masterdata/metadata/definitions.py</code> <pre><code>@field_validator(\"iri\")\n@classmethod\ndef validate_iri(cls, value: str | None) -&gt; str | None:\n    if not value:\n        return value\n    if not re.match(\n        r\"^http://purl.obolibrary.org/bam-masterdata/[\\w_]+:[\\d.]+$\", value\n    ):\n        raise ValueError(\n            \"`iri` must follow the rules specified in the description: 1) Must start with 'http://purl.obolibrary.org/bam-masterdata/', \"\n            \"2) followed by the entity name, 3) separated by a colon, 4) followed by the semantic versioning number. \"\n            \"Example: 'http://purl.obolibrary.org/bam-masterdata/Instrument:1.0.0'.\"\n        )\n    return value\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.definitions.EntityDef.strip_description","title":"<code>strip_description(value)</code>","text":"Source code in <code>bam_masterdata/metadata/definitions.py</code> <pre><code>@field_validator(\"description\")\n@classmethod\ndef strip_description(cls, value: str) -&gt; str:\n    return value.strip()\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.definitions.EntityDef.model_id","title":"<code>model_id(data)</code>","text":"<p>Stores the model <code>id</code> as the class name from the <code>code</code> field.</p> PARAMETER DESCRIPTION <code>data</code> <p>The data containing the fields values to validate.</p> <p> TYPE: <code>Any</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>The data with the validated fields.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>bam_masterdata/metadata/definitions.py</code> <pre><code>@model_validator(mode=\"after\")\n@classmethod\ndef model_id(cls, data: Any) -&gt; Any:\n    \"\"\"\n    Stores the model `id` as the class name from the `code` field.\n\n    Args:\n        data (Any): The data containing the fields values to validate.\n\n    Returns:\n        Any: The data with the validated fields.\n    \"\"\"\n    if \"PropertyType\" in data.name:\n        data.id = code_to_class_name(code=data.code, entity_type=\"property\")\n    else:\n        data.id = code_to_class_name(code=data.code, entity_type=\"object\")\n    return data\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.definitions.ObjectTypeDef","title":"<code>ObjectTypeDef</code>","text":"<p>               Bases: <code>BaseObjectTypeDef</code></p> <p>Definition class for an object type. It adds the fields of <code>generated_code_prefix</code>, <code>auto_generate_codes</code>, and <code>validation_script</code> to the common attributes of a base object type definition. E.g.:</p> <pre><code>class Instrument(BaseModel):\n    defs = ObjectTypeDef(\n        code='INSTRUMENT',\n        description='\n        Measuring Instrument//Messger\u00e4t\n        ',\n        generated_code_prefix='INS',\n    )\n</code></pre> Source code in <code>bam_masterdata/metadata/definitions.py</code> <pre><code>class ObjectTypeDef(BaseObjectTypeDef):\n    \"\"\"\n    Definition class for an object type. It adds the fields of `generated_code_prefix`, `auto_generate_codes`,\n    and `validation_script` to the common attributes of a base object type definition. E.g.:\n\n    ```python\n    class Instrument(BaseModel):\n        defs = ObjectTypeDef(\n            code='INSTRUMENT',\n            description='\n            Measuring Instrument//Messger\\u00e4t\n            ',\n            generated_code_prefix='INS',\n        )\n    ```\n    \"\"\"\n\n    generated_code_prefix: str | None = Field(\n        default=None,\n        description=\"\"\"\n        A short prefix for the defined object type, e.g. 'CHEM'. If not specified, it is defined\n        using the first 3 characters of `code`.\n        \"\"\",\n    )\n\n    auto_generate_codes: bool = Field(\n        True,\n        description=\"\"\"\n        Boolean used to generate codes using `generated_code_prefix` plus a unique number. Set to\n        True by default.\n        \"\"\",\n    )\n\n    @model_validator(mode=\"after\")\n    @classmethod\n    def model_validator_after_init(cls, data: Any) -&gt; Any:\n        \"\"\"\n        Validate the model after instantiation of the class.\n\n        Args:\n            data (Any): The data containing the fields values to validate.\n\n        Returns:\n            Any: The data with the validated fields.\n        \"\"\"\n        # If `generated_code_prefix` is not set, use the first 3 characters of `code`\n        if not data.generated_code_prefix:\n            data.generated_code_prefix = data.code[:3]\n\n        return data\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.definitions.ObjectTypeDef.generated_code_prefix","title":"<code>generated_code_prefix = Field(default=None, description=\"\\n        A short prefix for the defined object type, e.g. 'CHEM'. If not specified, it is defined\\n        using the first 3 characters of `code`.\\n        \")</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.definitions.ObjectTypeDef.auto_generate_codes","title":"<code>auto_generate_codes = Field(True, description='\\n        Boolean used to generate codes using `generated_code_prefix` plus a unique number. Set to\\n        True by default.\\n        ')</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.definitions.ObjectTypeDef.model_validator_after_init","title":"<code>model_validator_after_init(data)</code>","text":"<p>Validate the model after instantiation of the class.</p> PARAMETER DESCRIPTION <code>data</code> <p>The data containing the fields values to validate.</p> <p> TYPE: <code>Any</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>The data with the validated fields.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>bam_masterdata/metadata/definitions.py</code> <pre><code>@model_validator(mode=\"after\")\n@classmethod\ndef model_validator_after_init(cls, data: Any) -&gt; Any:\n    \"\"\"\n    Validate the model after instantiation of the class.\n\n    Args:\n        data (Any): The data containing the fields values to validate.\n\n    Returns:\n        Any: The data with the validated fields.\n    \"\"\"\n    # If `generated_code_prefix` is not set, use the first 3 characters of `code`\n    if not data.generated_code_prefix:\n        data.generated_code_prefix = data.code[:3]\n\n    return data\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.definitions.CollectionTypeDef","title":"<code>CollectionTypeDef</code>","text":"<p>               Bases: <code>BaseObjectTypeDef</code></p> <p>Definition class for a collection type. E.g.:</p> <pre><code>class DefaultExperiment(BaseModel):\n    defs = CollectionTypeDef(\n        code='DEFAULT_EXPERIMENT',\n        description='...',\n        validation_script='DEFAULT_EXPERIMENT.date_range_validation',\n    )\n</code></pre> Source code in <code>bam_masterdata/metadata/definitions.py</code> <pre><code>class CollectionTypeDef(BaseObjectTypeDef):\n    \"\"\"\n    Definition class for a collection type. E.g.:\n\n    ```python\n    class DefaultExperiment(BaseModel):\n        defs = CollectionTypeDef(\n            code='DEFAULT_EXPERIMENT',\n            description='...',\n            validation_script='DEFAULT_EXPERIMENT.date_range_validation',\n        )\n    ```\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.definitions.DatasetTypeDef","title":"<code>DatasetTypeDef</code>","text":"<p>               Bases: <code>BaseObjectTypeDef</code></p> <p>Definition class for a data set type. E.g.:</p> <p>```python class RawData(BaseModel):     defs = DatasetTypeDef(         code='RAW_DATA',         description='...',     )</p> Source code in <code>bam_masterdata/metadata/definitions.py</code> <pre><code>class DatasetTypeDef(BaseObjectTypeDef):\n    \"\"\"\n    Definition class for a data set type. E.g.:\n\n    ```python\n    class RawData(BaseModel):\n        defs = DatasetTypeDef(\n            code='RAW_DATA',\n            description='...',\n        )\n    \"\"\"\n\n    # TODO add descriptions for `main_dataset_pattern` and `main_dataset_path`\n\n    main_dataset_pattern: str | None = Field(\n        default=None,\n        description=\"\"\"\"\"\",\n    )\n\n    main_dataset_path: str | None = Field(\n        default=None,\n        description=\"\"\"\"\"\",\n    )\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.definitions.DatasetTypeDef.main_dataset_pattern","title":"<code>main_dataset_pattern = Field(default=None, description='')</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.definitions.DatasetTypeDef.main_dataset_path","title":"<code>main_dataset_path = Field(default=None, description='')</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.definitions.VocabularyTypeDef","title":"<code>VocabularyTypeDef</code>","text":"<p>               Bases: <code>EntityDef</code></p> <p>Definition class for a vocabulary type. It adds the fields of <code>url_template</code> to the common attributes of an entity definition. E.g.:</p> <pre><code>class DocumentType(VocabularyType):\n    defs = VocabularyTypeDef(\n        code='DOCUMENT_TYPE',\n        description='Document type//Dokumententypen',\n    )\n</code></pre> Source code in <code>bam_masterdata/metadata/definitions.py</code> <pre><code>class VocabularyTypeDef(EntityDef):\n    \"\"\"\n    Definition class for a vocabulary type. It adds the fields of `url_template` to the common attributes of\n    an entity definition. E.g.:\n\n    ```python\n    class DocumentType(VocabularyType):\n        defs = VocabularyTypeDef(\n            code='DOCUMENT_TYPE',\n            description='Document type//Dokumententypen',\n        )\n    ```\n    \"\"\"\n\n    # TODO add descriptions for `url_template`\n\n    url_template: str | None = Field(\n        default=None,\n        description=\"\"\"\"\"\",\n    )\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.definitions.VocabularyTypeDef.url_template","title":"<code>url_template = Field(default=None, description='')</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.definitions.PropertyTypeDef","title":"<code>PropertyTypeDef</code>","text":"<p>               Bases: <code>EntityDef</code></p> <p>Definition class for a property type. It adds the fields of <code>property_label</code>, <code>data_type</code>, <code>vocabulary_code</code>, <code>metadata</code>, <code>dynamic_script</code>, and <code>multivalued</code> to the common attributes of an entity definition.</p> <p>This class is used as an abstract layer for <code>PropertyTypeAssignment</code>, as in openBIS a PropertyType definition has less fields than when it is actually assigned to an entity type.</p> Source code in <code>bam_masterdata/metadata/definitions.py</code> <pre><code>class PropertyTypeDef(EntityDef):\n    \"\"\"\n    Definition class for a property type. It adds the fields of `property_label`, `data_type`,\n    `vocabulary_code`, `metadata`, `dynamic_script`, and `multivalued` to the common attributes of\n    an entity definition.\n\n    This class is used as an abstract layer for `PropertyTypeAssignment`, as in openBIS a PropertyType\n    definition has less fields than when it is actually assigned to an entity type.\n    \"\"\"\n\n    property_label: str = Field(\n        ...,\n        description=\"\"\"\n        Label that appears in the inventory view. This is the human-readable text for the property\n        type definition, and it typically coincides with the `code`, e.g., `'Monitoring date'` for the\n        `MONITORING_DATE` property type.\n        \"\"\",\n    )\n\n    data_type: DataType = Field(\n        ...,\n        description=\"\"\"\n        The data type of the property, i.e., if it is an integer, float, string, etc. The allowed\n        data types in openBIS are:\n            - `BOOLEAN`\n            - `CONTROLLEDVOCABULARY`\n            - `DATE`\n            - `HYPERLINK`\n            - `INTEGER`\n            - `MATERIAL`\n            - `MULTILINE_VARCHAR`\n            - `OBJECT`\n            - `SAMPLE`\n            - `REAL`\n            - `TIMESTAMP`\n            - `VARCHAR`\n            - `XML`\n\n        These are defined as an enumeration in the `DataType` class.\n\n        Read more in https://openbis.readthedocs.io/en/latest/uncategorized/register-master-data-via-the-admin-interface.html#data-types-available-in-openbis.\n        \"\"\",\n    )\n\n    vocabulary_code: str | None = Field(\n        default=None,\n        description=\"\"\"\n        String identifying the controlled vocabulary used for the data type of the property. This is\n        thus only relevant if `data_type == 'CONTROLLEDVOCABULARY'`.\n        \"\"\",\n    )\n\n    object_code: str | None = Field(\n        default=None,\n        description=\"\"\"\n        String identifying the object type used for the data type of the property. This is only\n        relevant if `data_type == 'OBJECT'`.\n        \"\"\",\n    )\n\n    # TODO add descriptions for `dynamic_script`\n\n    metadata: dict | None = Field(\n        default=None,\n        description=\"\"\"\n        General metadata written in a dictionary format. This is used to store additional information\n        about the property type, e.g., `{'unit': 'm', 'precision': 2}`.\n        \"\"\",\n    )\n\n    dynamic_script: str | None = Field(\n        default=None,\n        description=\"\"\"\"\"\",\n    )\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.definitions.PropertyTypeDef.property_label","title":"<code>property_label = Field(..., description=\"\\n        Label that appears in the inventory view. This is the human-readable text for the property\\n        type definition, and it typically coincides with the `code`, e.g., `'Monitoring date'` for the\\n        `MONITORING_DATE` property type.\\n        \")</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.definitions.PropertyTypeDef.data_type","title":"<code>data_type = Field(..., description='\\n        The data type of the property, i.e., if it is an integer, float, string, etc. The allowed\\n        data types in openBIS are:\\n            - `BOOLEAN`\\n            - `CONTROLLEDVOCABULARY`\\n            - `DATE`\\n            - `HYPERLINK`\\n            - `INTEGER`\\n            - `MATERIAL`\\n            - `MULTILINE_VARCHAR`\\n            - `OBJECT`\\n            - `SAMPLE`\\n            - `REAL`\\n            - `TIMESTAMP`\\n            - `VARCHAR`\\n            - `XML`\\n\\n        These are defined as an enumeration in the `DataType` class.\\n\\n        Read more in https://openbis.readthedocs.io/en/latest/uncategorized/register-master-data-via-the-admin-interface.html#data-types-available-in-openbis.\\n        ')</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.definitions.PropertyTypeDef.vocabulary_code","title":"<code>vocabulary_code = Field(default=None, description=\"\\n        String identifying the controlled vocabulary used for the data type of the property. This is\\n        thus only relevant if `data_type == 'CONTROLLEDVOCABULARY'`.\\n        \")</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.definitions.PropertyTypeDef.object_code","title":"<code>object_code = Field(default=None, description=\"\\n        String identifying the object type used for the data type of the property. This is only\\n        relevant if `data_type == 'OBJECT'`.\\n        \")</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.definitions.PropertyTypeDef.metadata","title":"<code>metadata = Field(default=None, description=\"\\n        General metadata written in a dictionary format. This is used to store additional information\\n        about the property type, e.g., `{'unit': 'm', 'precision': 2}`.\\n        \")</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.definitions.PropertyTypeDef.dynamic_script","title":"<code>dynamic_script = Field(default=None, description='')</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.definitions.PropertyTypeAssignment","title":"<code>PropertyTypeAssignment</code>","text":"<p>               Bases: <code>PropertyTypeDef</code></p> <p>Base class used to define properties inside <code>ObjectType</code>, <code>CollectionType</code>, or <code>DatasetType</code>. This is used to construct these types by assigning property types to them. It adds the fields of <code>mandatory</code>, <code>show_in_edit_views</code>, <code>section</code>, <code>unique</code>, and <code>internal_assignment</code> to the common attributes of a property type definition. E.g.:</p> <pre><code>class Instrument(ObjectType):\n    defs = ObjectTypeDef(\n        code='INSTRUMENT',\n        description='\n        Measuring Instrument//Messger\u00e4t\n        ',\n        generated_code_prefix='INS',\n    )\n\n    alias = PropertyTypeAssignment(\n        code='ALIAS',\n        data_type='VARCHAR',\n        property_label='Alternative name',\n        description='\n        e.g. abbreviation or nickname//z.B. Abk\u00fcrzung oder Spitzname//z.B. Abk\u00fcrzung oder Spitzname\n        ',\n        mandatory=False,\n        show_in_edit_views=True,\n        section='General information',\n    )\n\n    # ... other property type assignments here ...\n</code></pre> Source code in <code>bam_masterdata/metadata/definitions.py</code> <pre><code>class PropertyTypeAssignment(PropertyTypeDef):\n    \"\"\"\n    Base class used to define properties inside `ObjectType`, `CollectionType`, or `DatasetType`.\n    This is used to construct these types by assigning property types to them. It adds the fields\n    of `mandatory`, `show_in_edit_views`, `section`, `unique`, and `internal_assignment` to the common\n    attributes of a property type definition. E.g.:\n\n    ```python\n    class Instrument(ObjectType):\n        defs = ObjectTypeDef(\n            code='INSTRUMENT',\n            description='\n            Measuring Instrument//Messger\\u00e4t\n            ',\n            generated_code_prefix='INS',\n        )\n\n        alias = PropertyTypeAssignment(\n            code='ALIAS',\n            data_type='VARCHAR',\n            property_label='Alternative name',\n            description='\n            e.g. abbreviation or nickname//z.B. Abk\u00fcrzung oder Spitzname//z.B. Abk\u00fcrzung oder Spitzname\n            ',\n            mandatory=False,\n            show_in_edit_views=True,\n            section='General information',\n        )\n\n        # ... other property type assignments here ...\n    ```\n    \"\"\"\n\n    mandatory: bool = Field(\n        ...,\n        description=\"\"\"\n        If `True`, the property is mandatory and has to be set during instantiation of the object type.\n        If `False`, the property is optional.\n        \"\"\",\n    )\n\n    show_in_edit_views: bool = Field(\n        ...,\n        description=\"\"\"\n        If `True`, the property is shown in the edit views of the ELN in the object type instantiation.\n        If `False`, the property is hidden.\n        \"\"\",\n    )\n\n    section: str = Field(\n        ...,\n        description=\"\"\"\n        Section to which the property type belongs to. E.g., `'General Information'`.\n        \"\"\",\n    )\n\n    # TODO add descriptions for `unique` and `internal_assignment`\n\n    unique: str | None = Field(\n        default=None,\n        description=\"\"\"\"\"\",\n    )\n\n    internal_assignment: str | None = Field(\n        default=None,\n        description=\"\"\"\"\"\",\n    )\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.definitions.PropertyTypeAssignment.mandatory","title":"<code>mandatory = Field(..., description='\\n        If `True`, the property is mandatory and has to be set during instantiation of the object type.\\n        If `False`, the property is optional.\\n        ')</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.definitions.PropertyTypeAssignment.show_in_edit_views","title":"<code>show_in_edit_views = Field(..., description='\\n        If `True`, the property is shown in the edit views of the ELN in the object type instantiation.\\n        If `False`, the property is hidden.\\n        ')</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.definitions.PropertyTypeAssignment.section","title":"<code>section = Field(..., description=\"\\n        Section to which the property type belongs to. E.g., `'General Information'`.\\n        \")</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.definitions.PropertyTypeAssignment.unique","title":"<code>unique = Field(default=None, description='')</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.definitions.PropertyTypeAssignment.internal_assignment","title":"<code>internal_assignment = Field(default=None, description='')</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.definitions.VocabularyTerm","title":"<code>VocabularyTerm</code>","text":"<p>               Bases: <code>VocabularyTypeDef</code></p> <p>Base class used to define terms inside a <code>VocabularyType</code>. This is used to construct the vocabulary types by assigning vocabulary terms to them. It adds the fields of <code>label</code> and <code>official</code> to the common attributes of a vocabulary type definition. E.g.:</p> <p>```python class DocumentType(VocabularyType):     defs = VocabularyTypeDef(         code='DOCUMENT_TYPE',         description='Document type//Dokumententypen',     )</p> <pre><code>acceptance_certificate = VocabularyTerm(\n    code='ACCEPTANCE_CERTIFICATE',\n    label='Acceptance Certificate',\n    description='Acceptance Certificate//Abnahmezeugnis',\n)\n\ncalibration_certificate = VocabularyTerm(\n    code='CALIBRATION_CERTIFICATE',\n    label='Calibration Certificate',\n    description='Calibration Certificate//Kalibrierschein',\n)\n\n# ... other vocabulary term definitions here ...\n</code></pre> Source code in <code>bam_masterdata/metadata/definitions.py</code> <pre><code>class VocabularyTerm(VocabularyTypeDef):\n    \"\"\"\n    Base class used to define terms inside a `VocabularyType`. This is used to construct the vocabulary types\n    by assigning vocabulary terms to them. It adds the fields of `label` and `official` to the common attributes\n    of a vocabulary type definition. E.g.:\n\n    ```python\n    class DocumentType(VocabularyType):\n        defs = VocabularyTypeDef(\n            code='DOCUMENT_TYPE',\n            description='Document type//Dokumententypen',\n        )\n\n        acceptance_certificate = VocabularyTerm(\n            code='ACCEPTANCE_CERTIFICATE',\n            label='Acceptance Certificate',\n            description='Acceptance Certificate//Abnahmezeugnis',\n        )\n\n        calibration_certificate = VocabularyTerm(\n            code='CALIBRATION_CERTIFICATE',\n            label='Calibration Certificate',\n            description='Calibration Certificate//Kalibrierschein',\n        )\n\n        # ... other vocabulary term definitions here ...\n    \"\"\"\n\n    # TODO add descriptions for `label` and `official`\n\n    label: str = Field(\n        ...,\n        description=\"\"\"\"\"\",\n    )\n\n    official: bool = Field(\n        True,\n        description=\"\"\"\"\"\",\n    )\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.definitions.VocabularyTerm.label","title":"<code>label = Field(..., description='')</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.definitions.VocabularyTerm.official","title":"<code>official = Field(True, description='')</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.entities_dict","title":"<code>bam_masterdata.metadata.entities_dict</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.entities_dict.EntitiesDict","title":"<code>EntitiesDict</code>","text":"<p>Class to convert the entities in the datamodel defined in Python to a dictionary. The entities are read from the Python files defined in <code>python_path</code>.</p> Source code in <code>bam_masterdata/metadata/entities_dict.py</code> <pre><code>class EntitiesDict:\n    \"\"\"\n    Class to convert the entities in the datamodel defined in Python to a dictionary. The entities are read from the Python\n    files defined in `python_path`.\n    \"\"\"\n\n    def __init__(self, python_path: str = \"\", **kwargs):\n        self.python_path = python_path\n        self.logger = kwargs.get(\"logger\", logger)\n        self.data: dict = {}\n\n    def to_dict(self, module_path: str) -&gt; dict:\n        \"\"\"\n        Returns a dictionary containing entities read from the `module_path` Python file. The Python modules\n        are imported using the function `import_module` and their contents are inspected (using `inspect`) to\n        find the classes in the datamodel containing `defs` and with a `model_to_dict` method defined.\n\n        Args:\n            module_path (str): Path to the Python module file.\n\n        Returns:\n            dict: A dictionary containing the entities in the datamodel defined in one Python module file.\n        \"\"\"\n        module = import_module(module_path=module_path)\n\n        # initializing the dictionary with keys as the `code` of the entity and values the json dumped data\n        data: dict = {}\n\n        # Read the module source code and store line numbers\n        with open(module_path, encoding=\"utf-8\") as f:\n            module_source = f.readlines()\n\n        # Detect class definitions (entity types)\n        class_locations = {\n            match.group(1): i + 1  # Store line number (1-based index)\n            for i, line in enumerate(module_source)\n            if (match := re.match(r\"^\\s*class\\s+(\\w+)\\s*\\(.*\\):\", line))\n        }\n\n        # Detect property assignments (`PropertyTypeAssignment(...)`) with class context\n        property_locations: dict = {}\n        current_class = None\n\n        for i, line in enumerate(module_source):\n            class_match = re.match(r\"^\\s*class\\s+(\\w+)\\s*\\(.*\\):\", line)\n            if class_match:\n                current_class = class_match.group(1)\n\n            prop_match = re.search(r\"^\\s*(\\w+)\\s*=\\s*PropertyTypeAssignment\\(\", line)\n            if prop_match and current_class:\n                property_name = prop_match.group(1)\n                if current_class not in property_locations:\n                    property_locations[current_class] = {}\n                property_locations[current_class][property_name] = i + 1\n\n        # Detect vocabulary terms (`VocabularyTerm(...)`) with class context\n        vocabulary_term_locations: dict = {}\n        current_vocab_class = None\n\n        for i, line in enumerate(module_source):\n            class_match = re.match(r\"^\\s*class\\s+(\\w+)\\s*\\(.*\\):\", line)\n            if class_match:\n                current_vocab_class = class_match.group(1)\n\n            term_match = re.search(r\"^\\s*(\\w+)\\s*=\\s*VocabularyTerm\\(\", line)\n            if term_match and current_vocab_class:\n                term_name = term_match.group(1)\n                if current_vocab_class not in vocabulary_term_locations:\n                    vocabulary_term_locations[current_vocab_class] = {}\n                vocabulary_term_locations[current_vocab_class][term_name] = i + 1\n\n        # Process all classes in the module\n        for name, obj in inspect.getmembers(module, inspect.isclass):\n            if not hasattr(obj, \"defs\") or not callable(getattr(obj, \"model_to_dict\")):\n                continue\n            try:\n                obj_data = obj().model_to_dict()\n                obj_data[\"defs\"][\"row_location\"] = class_locations.get(name, None)\n\n                if \"properties\" in obj_data:\n                    # Processing standard properties (PropertyTypeAssignment)\n                    for prop in obj_data[\"properties\"]:\n                        prop_id = (\n                            prop[\"code\"].lower().replace(\".\", \"_\").replace(\"$\", \"\")\n                        )\n                        matched_key = next(\n                            (\n                                key\n                                for key in property_locations.get(name, {})\n                                if key == prop_id\n                            ),\n                            None,\n                        )\n                        prop[\"row_location\"] = property_locations.get(name, {}).get(\n                            matched_key, None\n                        )\n\n                elif \"terms\" in obj_data:\n                    # Processing vocabulary terms (VocabularyTerm)\n                    for term in obj_data[\"terms\"]:\n                        term_id = term[\"code\"].lower().replace(\".\", \"_\")\n                        matched_key = next(\n                            (\n                                key\n                                for key in vocabulary_term_locations.get(name, {})\n                                if key == term_id\n                            ),\n                            None,\n                        )\n                        term[\"row_location\"] = vocabulary_term_locations.get(\n                            name, {}\n                        ).get(matched_key, None)\n\n                data[obj.defs.code] = obj_data\n            except Exception as err:\n                click.echo(f\"Failed to process class {name} in {module_path}: {err}\")\n\n        return data\n\n    def single_json(self) -&gt; dict:\n        \"\"\"\n        Returns a single dictionary containing all the entities in the datamodel defined in the Python files\n        in `python_path`. The format of this dictionary is:\n            {\n                \"collection_type\": {\n                    \"COLLECTION\": {\n                        \"defs\": {\n                            \"code\": \"COLLECTION\",\n                            \"description\": \"\",\n                            ...\n                        },\n                        \"properties\": [\n                            {\n                                \"code\": \"$DEFAULT_COLLECTION_VIEW\",\n                                \"description\": \"Default view for experiments of the type collection\",\n                                ...\n                            },\n                            {...},\n                            ...\n                        ]\n                    }\n                },\n                \"object_type\": {...},\n                ...\n            }\n\n        Returns:\n            dict: A dictionary containing all the entities in the datamodel.\n        \"\"\"\n        # Get the Python modules to process the datamodel\n        py_modules = listdir_py_modules(\n            directory_path=self.python_path, logger=self.logger\n        )\n\n        # Process each module using the `model_to_dict` method of each entity and store them in a single dictionary\n        full_data: dict = {}\n        for module_path in py_modules:\n            data = self.to_dict(module_path=module_path)\n            # name can be collection_type, object_type, dataset_type, vocabulary_type, or property_type\n            name = os.path.basename(module_path).replace(\".py\", \"\")\n            full_data[name] = data\n        return full_data\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.entities_dict.EntitiesDict.python_path","title":"<code>python_path = python_path</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.entities_dict.EntitiesDict.logger","title":"<code>logger = kwargs.get('logger', logger)</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.entities_dict.EntitiesDict.data","title":"<code>data = {}</code>","text":""},{"location":"references/api/#bam_masterdata.metadata.entities_dict.EntitiesDict.__init__","title":"<code>__init__(python_path='', **kwargs)</code>","text":"Source code in <code>bam_masterdata/metadata/entities_dict.py</code> <pre><code>def __init__(self, python_path: str = \"\", **kwargs):\n    self.python_path = python_path\n    self.logger = kwargs.get(\"logger\", logger)\n    self.data: dict = {}\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.entities_dict.EntitiesDict.to_dict","title":"<code>to_dict(module_path)</code>","text":"<p>Returns a dictionary containing entities read from the <code>module_path</code> Python file. The Python modules are imported using the function <code>import_module</code> and their contents are inspected (using <code>inspect</code>) to find the classes in the datamodel containing <code>defs</code> and with a <code>model_to_dict</code> method defined.</p> PARAMETER DESCRIPTION <code>module_path</code> <p>Path to the Python module file.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>A dictionary containing the entities in the datamodel defined in one Python module file.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>bam_masterdata/metadata/entities_dict.py</code> <pre><code>def to_dict(self, module_path: str) -&gt; dict:\n    \"\"\"\n    Returns a dictionary containing entities read from the `module_path` Python file. The Python modules\n    are imported using the function `import_module` and their contents are inspected (using `inspect`) to\n    find the classes in the datamodel containing `defs` and with a `model_to_dict` method defined.\n\n    Args:\n        module_path (str): Path to the Python module file.\n\n    Returns:\n        dict: A dictionary containing the entities in the datamodel defined in one Python module file.\n    \"\"\"\n    module = import_module(module_path=module_path)\n\n    # initializing the dictionary with keys as the `code` of the entity and values the json dumped data\n    data: dict = {}\n\n    # Read the module source code and store line numbers\n    with open(module_path, encoding=\"utf-8\") as f:\n        module_source = f.readlines()\n\n    # Detect class definitions (entity types)\n    class_locations = {\n        match.group(1): i + 1  # Store line number (1-based index)\n        for i, line in enumerate(module_source)\n        if (match := re.match(r\"^\\s*class\\s+(\\w+)\\s*\\(.*\\):\", line))\n    }\n\n    # Detect property assignments (`PropertyTypeAssignment(...)`) with class context\n    property_locations: dict = {}\n    current_class = None\n\n    for i, line in enumerate(module_source):\n        class_match = re.match(r\"^\\s*class\\s+(\\w+)\\s*\\(.*\\):\", line)\n        if class_match:\n            current_class = class_match.group(1)\n\n        prop_match = re.search(r\"^\\s*(\\w+)\\s*=\\s*PropertyTypeAssignment\\(\", line)\n        if prop_match and current_class:\n            property_name = prop_match.group(1)\n            if current_class not in property_locations:\n                property_locations[current_class] = {}\n            property_locations[current_class][property_name] = i + 1\n\n    # Detect vocabulary terms (`VocabularyTerm(...)`) with class context\n    vocabulary_term_locations: dict = {}\n    current_vocab_class = None\n\n    for i, line in enumerate(module_source):\n        class_match = re.match(r\"^\\s*class\\s+(\\w+)\\s*\\(.*\\):\", line)\n        if class_match:\n            current_vocab_class = class_match.group(1)\n\n        term_match = re.search(r\"^\\s*(\\w+)\\s*=\\s*VocabularyTerm\\(\", line)\n        if term_match and current_vocab_class:\n            term_name = term_match.group(1)\n            if current_vocab_class not in vocabulary_term_locations:\n                vocabulary_term_locations[current_vocab_class] = {}\n            vocabulary_term_locations[current_vocab_class][term_name] = i + 1\n\n    # Process all classes in the module\n    for name, obj in inspect.getmembers(module, inspect.isclass):\n        if not hasattr(obj, \"defs\") or not callable(getattr(obj, \"model_to_dict\")):\n            continue\n        try:\n            obj_data = obj().model_to_dict()\n            obj_data[\"defs\"][\"row_location\"] = class_locations.get(name, None)\n\n            if \"properties\" in obj_data:\n                # Processing standard properties (PropertyTypeAssignment)\n                for prop in obj_data[\"properties\"]:\n                    prop_id = (\n                        prop[\"code\"].lower().replace(\".\", \"_\").replace(\"$\", \"\")\n                    )\n                    matched_key = next(\n                        (\n                            key\n                            for key in property_locations.get(name, {})\n                            if key == prop_id\n                        ),\n                        None,\n                    )\n                    prop[\"row_location\"] = property_locations.get(name, {}).get(\n                        matched_key, None\n                    )\n\n            elif \"terms\" in obj_data:\n                # Processing vocabulary terms (VocabularyTerm)\n                for term in obj_data[\"terms\"]:\n                    term_id = term[\"code\"].lower().replace(\".\", \"_\")\n                    matched_key = next(\n                        (\n                            key\n                            for key in vocabulary_term_locations.get(name, {})\n                            if key == term_id\n                        ),\n                        None,\n                    )\n                    term[\"row_location\"] = vocabulary_term_locations.get(\n                        name, {}\n                    ).get(matched_key, None)\n\n            data[obj.defs.code] = obj_data\n        except Exception as err:\n            click.echo(f\"Failed to process class {name} in {module_path}: {err}\")\n\n    return data\n</code></pre>"},{"location":"references/api/#bam_masterdata.metadata.entities_dict.EntitiesDict.single_json","title":"<code>single_json()</code>","text":"<p>Returns a single dictionary containing all the entities in the datamodel defined in the Python files in <code>python_path</code>. The format of this dictionary is:     {         \"collection_type\": {             \"COLLECTION\": {                 \"defs\": {                     \"code\": \"COLLECTION\",                     \"description\": \"\",                     ...                 },                 \"properties\": [                     {                         \"code\": \"$DEFAULT_COLLECTION_VIEW\",                         \"description\": \"Default view for experiments of the type collection\",                         ...                     },                     {...},                     ...                 ]             }         },         \"object_type\": {...},         ...     }</p> RETURNS DESCRIPTION <code>dict</code> <p>A dictionary containing all the entities in the datamodel.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>bam_masterdata/metadata/entities_dict.py</code> <pre><code>def single_json(self) -&gt; dict:\n    \"\"\"\n    Returns a single dictionary containing all the entities in the datamodel defined in the Python files\n    in `python_path`. The format of this dictionary is:\n        {\n            \"collection_type\": {\n                \"COLLECTION\": {\n                    \"defs\": {\n                        \"code\": \"COLLECTION\",\n                        \"description\": \"\",\n                        ...\n                    },\n                    \"properties\": [\n                        {\n                            \"code\": \"$DEFAULT_COLLECTION_VIEW\",\n                            \"description\": \"Default view for experiments of the type collection\",\n                            ...\n                        },\n                        {...},\n                        ...\n                    ]\n                }\n            },\n            \"object_type\": {...},\n            ...\n        }\n\n    Returns:\n        dict: A dictionary containing all the entities in the datamodel.\n    \"\"\"\n    # Get the Python modules to process the datamodel\n    py_modules = listdir_py_modules(\n        directory_path=self.python_path, logger=self.logger\n    )\n\n    # Process each module using the `model_to_dict` method of each entity and store them in a single dictionary\n    full_data: dict = {}\n    for module_path in py_modules:\n        data = self.to_dict(module_path=module_path)\n        # name can be collection_type, object_type, dataset_type, vocabulary_type, or property_type\n        name = os.path.basename(module_path).replace(\".py\", \"\")\n        full_data[name] = data\n    return full_data\n</code></pre>"},{"location":"references/api/#bam_masterdata.cli.fill_masterdata","title":"<code>bam_masterdata.cli.fill_masterdata</code>","text":""},{"location":"references/api/#bam_masterdata.cli.fill_masterdata.MasterdataCodeGenerator","title":"<code>MasterdataCodeGenerator</code>","text":"<p>Class to generate Python code for the masterdata datamodel based on the entities existing in an openBIS instance.</p> Source code in <code>bam_masterdata/cli/fill_masterdata.py</code> <pre><code>class MasterdataCodeGenerator:\n    \"\"\"\n    Class to generate Python code for the masterdata datamodel based on the entities existing in an\n    openBIS instance.\n    \"\"\"\n\n    def __init__(self, url: str = \"\", path: str = \"\", **kwargs):\n        start_time = time.time()\n        self.row_cell_info = kwargs.get(\"row_cell_info\", False)\n        # * This part takes some time due to the loading of all entities from Openbis\n        if url:\n            self.generator_type = \"openbis\"\n            self.properties = OpenbisEntities(url=url).get_property_dict()\n            self.collections = OpenbisEntities(url=url).get_collection_dict()\n            self.datasets = OpenbisEntities(url=url).get_dataset_dict()\n            self.objects = OpenbisEntities(url=url).get_object_dict()\n            self.vocabularies = OpenbisEntities(url=url).get_vocabulary_dict()\n            elapsed_time = time.time() - start_time\n            click.echo(\n                f\"Loaded OpenBIS entities in `MasterdataCodeGenerator` initialization {elapsed_time:.2f} seconds\\n\"\n            )\n        else:\n            self.generator_type = \"excel\"\n            entities_dict = MasterdataExcelExtractor(\n                excel_path=path, row_cell_info=self.row_cell_info\n            ).excel_to_entities()\n            self.properties = entities_dict.get(\"property_types\", {})\n            self.collections = entities_dict.get(\"collection_types\", {})\n            self.datasets = entities_dict.get(\"dataset_types\", {})\n            self.objects = entities_dict.get(\"object_types\", {})\n            self.vocabularies = entities_dict.get(\"vocabulary_types\", {})\n            elapsed_time = time.time() - start_time\n            click.echo(\n                f\"Loaded Masterdata excel entities in `MasterdataCodeGenerator` initialization {elapsed_time:.2f} seconds\\n\"\n            )\n\n    def determine_parent_class(\n        self, code: str, class_names: dict, default: str, lines: list\n    ) -&gt; tuple:\n        \"\"\"\n        Determine the parent class information of the entity based on its `code`. It returns\n        the `parent_code` and `parent class`, as well as the `class_name` of the entity. The\n        class will inherit from `parent_class`.\n\n        If the parent class does not exist, a note is added to the `lines` list for debugging purposes.\n\n        Args:\n            code (str): The code of the entity.\n            class_names (dict): A dictionary with the class names of the entities.\n            default (str): The default parent class if the parent class does not exist.\n            lines (list): A list of strings to be printed to the Python module.\n        Returns:\n            tuple: The parent code, parent class, and class name of the entity.\n        \"\"\"\n        parent_code = \"\"\n        if \".\" in code:\n            parent_code = code.rsplit(\".\", 1)[0]\n        parent_class = class_names.get(parent_code, default)\n\n        # Format class name\n        class_name = code_to_class_name(code)\n        class_names[code] = class_name\n\n        # If the parent class does not exist but the `code` shows some inheritance, we add a note for debugging\n        if parent_code and parent_class == default:\n            lines.append(\n                f\"# ! The parent class of {class_name} is not defined (missing {parent_class})\"\n            )\n\n        return parent_code, parent_class, class_name\n\n    def get_property_object_code(self, prop_data: dict) -&gt; str:\n        \"\"\"\n        Get the object code (or vocabulary code) used for reference for the assigned property with `prop_code`.\n\n        Args:\n            prop_data (dict): The data information for the property as obtained from openBIS.\n\n        Returns:\n            str: The object/vocabulary code used for reference for the assigned property.\n        \"\"\"\n        if not prop_data:\n            return \"\"\n\n        # TODO check excel extractor to add sampleType column\n        object_code = prop_data.get(\"sampleType\", \"\")\n        if object_code:\n            return object_code\n\n        # TODO fix this patch and avoid using generator type\n        vocabulary_code = \"\"\n        if self.generator_type == \"openbis\":\n            vocabulary_code = prop_data.get(\"vocabulary\", \"\")\n        elif self.generator_type == \"excel\":\n            vocabulary_code = prop_data.get(\"vocabularyType\", \"\")\n        return vocabulary_code\n\n    def add_properties(\n        self, entities: dict, parent_code: str, data: dict, lines: list\n    ) -&gt; None:\n        \"\"\"\n        Add the properties of the entity to the `lines` list. The properties are added as\n        `PropertyTypeAssignment` objects.\n\n        Note: the assigned properties do not have the information of `code` for the entity when\n        data_type is OBJECT or CONTROLLEDVOCABULARY. These are instead defined in `property_types.py`.\n\n        Args:\n            entities (dict): The dictionary of entities (objects, collections, datasets, vocabularies).\n            parent_code (code): The code of the parent class.\n            data (dict): The data information for the entity as obtained from openBIS.\n            lines (list): A list of strings to be printed to the Python module.\n        \"\"\"\n        parent_properties_code = (\n            entities.get(parent_code, {}).get(\"properties\", {}).keys()\n        )\n        for prop_code, prop_data in data.get(\"properties\", {}).items():\n            # Skip \"UNKNOWN\" properties\n            # We check if the property is inherited from the parent class\n            if prop_code == \"UNKNOWN\" or prop_code in parent_properties_code:\n                continue\n\n            prop_name = prop_code.lstrip(\"$\").replace(\".\", \"_\").lower()\n            lines.append(f\"    {prop_name} = PropertyTypeAssignment(\")\n            lines.append(f'        code=\"{prop_code}\",')\n            # ! patching dataType=SAMPLE instead of OBJECT\n            data_type = prop_data.get(\"dataType\", \"\")\n            if data_type == \"SAMPLE\":\n                data_type = \"OBJECT\"\n            lines.append(f'        data_type=\"{data_type}\",')\n            if data_type == \"OBJECT\":\n                object_code = self.get_property_object_code(prop_data=prop_data)\n                if object_code:\n                    lines.append(f'        object_code=\"{object_code}\",')\n            elif data_type == \"CONTROLLEDVOCABULARY\":\n                vocabulary_code = self.get_property_object_code(prop_data=prop_data)\n                if vocabulary_code:\n                    lines.append(f'        vocabulary_code=\"{vocabulary_code}\",')\n\n            property_label = (prop_data.get(\"label\") or \"\").replace(\"\\n\", \"\\\\n\")\n            lines.append(f'        property_label=\"{property_label}\",')\n            description = (\n                (prop_data.get(\"description\") or \"\")\n                .replace('\"', \"`\")\n                .replace(\"\\n\", \"\\\\n\")\n                .replace(\"'\", \"`\")\n            )\n            lines.append(f'        description=\"\"\"{description}\"\"\",')\n            lines.append(f\"        mandatory={prop_data.get('mandatory', False)},\")\n            lines.append(\n                f\"        show_in_edit_views={prop_data.get('show_in_edit_views', False)},\"\n            )\n            section = (\n                (prop_data.get(\"section\") or \"\")\n                .replace('\"', '\\\\\"')\n                .replace(\"\\n\", \"\\\\n\")\n                .replace(\"'\", \"\\\\'\")\n            )\n            lines.append(f'        section=\"{section}\",')\n            lines.append(\"    )\")\n            lines.append(\"\")\n\n    def generate_collection_types(self) -&gt; str:\n        \"\"\"\n        Generate Python code for the collection types in the Openbis datamodel. The code is generated\n        as a string which is then printed out to the specific Python module in `bam_masterdata/datamodel/collection_types.py`.\n\n        Returns:\n            str: Python code for the collection types.\n        \"\"\"\n        lines = []\n        class_names: dict = {}\n        # from bam_masterdata.metadata.definitions import (\n        #     CollectionTypeDef,\n        #     PropertyTypeAssignment,\n        # )\n        if self.collections != {}:\n            # Add imports at the top\n            lines.append(\"from bam_masterdata.metadata.definitions import (\")\n            lines.append(\"    CollectionTypeDef,\")\n            lines.append(\"    PropertyTypeAssignment,\")\n            lines.append(\")\")\n            lines.append(\"from bam_masterdata.metadata.entities import CollectionType\")\n            lines.append(\"\")\n            lines.append(\"\")\n\n        # Process each collection type\n        for code, data in self.collections.items():\n            # Skip the \"UNKNOWN\" object type\n            if code == \"UNKNOWN\":\n                continue\n\n            # Determine parent class\n            parent_code, parent_class, class_name = self.determine_parent_class(\n                code=code,\n                class_names=class_names,\n                default=\"CollectionType\",\n                lines=lines,\n            )\n\n            # Add class definition\n            lines.append(f\"class {class_name}({parent_class}):\")\n            lines.append(\"    defs = CollectionTypeDef(\")\n            lines.append(f'        code=\"{code}\",')\n            description = (\n                (data.get(\"description\") or \"\")\n                .replace('\"', \"`\")\n                .replace(\"\\n\", \"\\\\n\")\n                .replace(\"'\", \"`\")\n            )\n            lines.append(f'        description=\"\"\"{description}\"\"\",')\n            if data.get(\"validationPlugin\") != \"\":\n                lines.append(\n                    f'        validation_script=\"{data.get(\"validationPlugin\")}\",'\n                )\n            lines.append(\"    )\")\n            lines.append(\"\")\n\n            # Add properties\n            self.add_properties(self.collections, parent_code, data, lines)\n            # Add newline between classes\n            lines.append(\"\")\n\n        return \"\\n\".join(lines)\n\n    def generate_dataset_types(self) -&gt; str:\n        \"\"\"\n        Generate Python code for the dataset types in the Openbis datamodel. The code is generated\n        as a string which is then printed out to the specific Python module in `bam_masterdata/datamodel/dataset_types.py`.\n\n        Returns:\n            str: Python code for the dataset types.\n        \"\"\"\n        lines = []\n        class_names: dict = {}\n\n        if self.datasets != {}:\n            # Add imports at the top\n            lines.append(\n                \"from bam_masterdata.metadata.definitions import DatasetTypeDef, PropertyTypeAssignment\"\n            )\n            lines.append(\"from bam_masterdata.metadata.entities import DatasetType\")\n            lines.append(\"\")\n            lines.append(\"\")\n\n        # Process each dataset type\n        for code, data in self.datasets.items():\n            # Skip the \"UNKNOWN\" object type\n            if code == \"UNKNOWN\":\n                continue\n\n            # Determine parent class\n            parent_code, parent_class, class_name = self.determine_parent_class(\n                code=code, class_names=class_names, default=\"DatasetType\", lines=lines\n            )\n\n            # Add class definition\n            lines.append(f\"class {class_name}({parent_class}):\")\n            lines.append(\"    defs = DatasetTypeDef(\")\n            lines.append(f'        code=\"{code}\",')\n            description = (\n                (data.get(\"description\") or \"\")\n                .replace('\"', \"`\")\n                .replace(\"\\n\", \"\\\\n\")\n                .replace(\"'\", \"`\")\n            )\n            lines.append(f'        description=\"\"\"{description}\"\"\",')\n            lines.append(\"    )\")\n            lines.append(\"\")\n\n            # Add properties\n            self.add_properties(self.datasets, parent_code, data, lines)\n            # Add newline between classes\n            lines.append(\"\")\n\n        return \"\\n\".join(lines)\n\n    def generate_object_types(self) -&gt; str:\n        \"\"\"\n        Generate Python code for the object types in the Openbis datamodel. The code is generated\n        as a string which is then printed out to the specific Python module in `bam_masterdata/datamodel/object_types.py`.\n\n        Returns:\n            str: Python code for the object types.\n        \"\"\"\n        lines = []\n        class_names: dict = {}\n\n        if self.objects != {}:\n            # Add imports at the top\n            lines.append(\n                \"from bam_masterdata.metadata.definitions import ObjectTypeDef, PropertyTypeAssignment\"\n            )\n            lines.append(\"from bam_masterdata.metadata.entities import ObjectType\")\n            lines.append(\"\")\n            lines.append(\"\")\n\n        # Process each object type\n        for code, data in self.objects.items():\n            # Skip the \"UNKNOWN\" object type\n            if code == \"UNKNOWN\":\n                continue\n\n            # Determine parent class\n            parent_code, parent_class, class_name = self.determine_parent_class(\n                code=code, class_names=class_names, default=\"ObjectType\", lines=lines\n            )\n\n            # Add class definition\n            lines.append(f\"class {class_name}({parent_class}):\")\n            lines.append(\"    defs = ObjectTypeDef(\")\n            lines.append(f'        code=\"{code}\",')\n            description = (\n                (data.get(\"description\") or \"\")\n                .replace('\"', \"`\")\n                .replace(\"\\n\", \"\\\\n\")\n                .replace(\"'\", \"`\")\n            )\n            lines.append(f'        description=\"\"\"{description}\"\"\",')\n            lines.append(\n                f'        generated_code_prefix=\"{data.get(\"generatedCodePrefix\", \"\")}\",'\n            )\n            lines.append(\"    )\")\n            lines.append(\"\")\n\n            # Add properties\n            self.add_properties(self.objects, parent_code, data, lines)\n            # Add newline between classes\n            lines.append(\"\")\n\n        return \"\\n\".join(lines)\n\n    def generate_vocabulary_types(self) -&gt; str:\n        \"\"\"\n        Generate Python code for the vocabulary types in the Openbis datamodel. The code is generated\n        as a string which is then printed out to the specific Python module in `bam_masterdata/datamodel/vocabulary_types.py`.\n\n        Returns:\n            str: Python code for the vocabulary types.\n        \"\"\"\n        lines = []\n        class_names: dict = {}\n\n        if self.vocabularies != {}:\n            # Add imports at the top\n            lines.append(\n                \"from bam_masterdata.metadata.definitions import VocabularyTerm, VocabularyTypeDef\"\n            )\n            lines.append(\"from bam_masterdata.metadata.entities import VocabularyType\")\n            lines.append(\"\")\n            lines.append(\"\")\n\n        # Process each object type\n        for code, data in self.vocabularies.items():\n            # Skip the \"UNKNOWN\" object type\n            if code == \"UNKNOWN\":\n                continue\n\n            # Determine parent class\n            parent_code, parent_class, class_name = self.determine_parent_class(\n                code=code,\n                class_names=class_names,\n                default=\"VocabularyType\",\n                lines=lines,\n            )\n\n            # Add class definition\n            lines.append(f\"class {class_name}({parent_class}):\")\n            lines.append(\"    defs = VocabularyTypeDef(\")\n            lines.append(f'        code=\"{code}\",')\n            description = (\n                (data.get(\"description\") or \"\")\n                .replace('\"', \"`\")\n                .replace(\"\\n\", \"\\\\n\")\n                .replace(\"'\", \"`\")\n            )\n            lines.append(f'        description=\"\"\"{description}\"\"\",')\n            lines.append(\"    )\")\n            lines.append(\"\")\n\n            # Add terms\n            parent_terms = self.objects.get(parent_code, {}).get(\"terms\", {}).keys()\n            for term_code, term_data in data.get(\"terms\", {}).items():\n                # Skip \"UNKNOWN\" properties\n                if term_code == \"UNKNOWN\":\n                    continue\n\n                # We check if the term is inherited from the parent class\n                if term_code in parent_terms:\n                    continue\n\n                term_name = (\n                    term_code.lstrip(\"$\").replace(\".\", \"_\").replace(\"-\", \"_\").lower()\n                )\n                if term_name[0].isdigit():\n                    term_name = f\"_{term_name}\"\n                if term_name == \"l\":\n                    term_name = \"L\"\n                if term_name == \"O\":\n                    term_name = \"o\"\n                if term_name == \"I\":\n                    term_name = \"i\"\n                lines.append(f\"    {term_name} = VocabularyTerm(\")\n                lines.append(f'        code=\"{term_code}\",')\n                label = (term_data.get(\"label\") or \"\").replace('\"', \"\")\n                lines.append(f'        label=\"{label}\",')\n                description = (\n                    (term_data.get(\"description\") or \"\")\n                    .replace('\"', \"`\")\n                    .replace(\"\\n\", \"\\\\n\")\n                    .replace(\"'\", \"`\")\n                )\n                lines.append(f'        description=\"\"\"{description}\"\"\",')\n                lines.append(\"    )\")\n                lines.append(\"\")\n\n            # Add newline between classes\n            lines.append(\"\")\n\n        return \"\\n\".join(lines)\n</code></pre>"},{"location":"references/api/#bam_masterdata.cli.fill_masterdata.MasterdataCodeGenerator.row_cell_info","title":"<code>row_cell_info = kwargs.get('row_cell_info', False)</code>","text":""},{"location":"references/api/#bam_masterdata.cli.fill_masterdata.MasterdataCodeGenerator.generator_type","title":"<code>generator_type = 'openbis'</code>","text":""},{"location":"references/api/#bam_masterdata.cli.fill_masterdata.MasterdataCodeGenerator.properties","title":"<code>properties = OpenbisEntities(url=url).get_property_dict()</code>","text":""},{"location":"references/api/#bam_masterdata.cli.fill_masterdata.MasterdataCodeGenerator.collections","title":"<code>collections = OpenbisEntities(url=url).get_collection_dict()</code>","text":""},{"location":"references/api/#bam_masterdata.cli.fill_masterdata.MasterdataCodeGenerator.datasets","title":"<code>datasets = OpenbisEntities(url=url).get_dataset_dict()</code>","text":""},{"location":"references/api/#bam_masterdata.cli.fill_masterdata.MasterdataCodeGenerator.objects","title":"<code>objects = OpenbisEntities(url=url).get_object_dict()</code>","text":""},{"location":"references/api/#bam_masterdata.cli.fill_masterdata.MasterdataCodeGenerator.vocabularies","title":"<code>vocabularies = OpenbisEntities(url=url).get_vocabulary_dict()</code>","text":""},{"location":"references/api/#bam_masterdata.cli.fill_masterdata.MasterdataCodeGenerator.__init__","title":"<code>__init__(url='', path='', **kwargs)</code>","text":"Source code in <code>bam_masterdata/cli/fill_masterdata.py</code> <pre><code>def __init__(self, url: str = \"\", path: str = \"\", **kwargs):\n    start_time = time.time()\n    self.row_cell_info = kwargs.get(\"row_cell_info\", False)\n    # * This part takes some time due to the loading of all entities from Openbis\n    if url:\n        self.generator_type = \"openbis\"\n        self.properties = OpenbisEntities(url=url).get_property_dict()\n        self.collections = OpenbisEntities(url=url).get_collection_dict()\n        self.datasets = OpenbisEntities(url=url).get_dataset_dict()\n        self.objects = OpenbisEntities(url=url).get_object_dict()\n        self.vocabularies = OpenbisEntities(url=url).get_vocabulary_dict()\n        elapsed_time = time.time() - start_time\n        click.echo(\n            f\"Loaded OpenBIS entities in `MasterdataCodeGenerator` initialization {elapsed_time:.2f} seconds\\n\"\n        )\n    else:\n        self.generator_type = \"excel\"\n        entities_dict = MasterdataExcelExtractor(\n            excel_path=path, row_cell_info=self.row_cell_info\n        ).excel_to_entities()\n        self.properties = entities_dict.get(\"property_types\", {})\n        self.collections = entities_dict.get(\"collection_types\", {})\n        self.datasets = entities_dict.get(\"dataset_types\", {})\n        self.objects = entities_dict.get(\"object_types\", {})\n        self.vocabularies = entities_dict.get(\"vocabulary_types\", {})\n        elapsed_time = time.time() - start_time\n        click.echo(\n            f\"Loaded Masterdata excel entities in `MasterdataCodeGenerator` initialization {elapsed_time:.2f} seconds\\n\"\n        )\n</code></pre>"},{"location":"references/api/#bam_masterdata.cli.fill_masterdata.MasterdataCodeGenerator.determine_parent_class","title":"<code>determine_parent_class(code, class_names, default, lines)</code>","text":"<p>Determine the parent class information of the entity based on its <code>code</code>. It returns the <code>parent_code</code> and <code>parent class</code>, as well as the <code>class_name</code> of the entity. The class will inherit from <code>parent_class</code>.</p> <p>If the parent class does not exist, a note is added to the <code>lines</code> list for debugging purposes.</p> PARAMETER DESCRIPTION <code>code</code> <p>The code of the entity.</p> <p> TYPE: <code>str</code> </p> <code>class_names</code> <p>A dictionary with the class names of the entities.</p> <p> TYPE: <code>dict</code> </p> <code>default</code> <p>The default parent class if the parent class does not exist.</p> <p> TYPE: <code>str</code> </p> <code>lines</code> <p>A list of strings to be printed to the Python module.</p> <p> TYPE: <code>list</code> </p> <p>Returns:     tuple: The parent code, parent class, and class name of the entity.</p> Source code in <code>bam_masterdata/cli/fill_masterdata.py</code> <pre><code>def determine_parent_class(\n    self, code: str, class_names: dict, default: str, lines: list\n) -&gt; tuple:\n    \"\"\"\n    Determine the parent class information of the entity based on its `code`. It returns\n    the `parent_code` and `parent class`, as well as the `class_name` of the entity. The\n    class will inherit from `parent_class`.\n\n    If the parent class does not exist, a note is added to the `lines` list for debugging purposes.\n\n    Args:\n        code (str): The code of the entity.\n        class_names (dict): A dictionary with the class names of the entities.\n        default (str): The default parent class if the parent class does not exist.\n        lines (list): A list of strings to be printed to the Python module.\n    Returns:\n        tuple: The parent code, parent class, and class name of the entity.\n    \"\"\"\n    parent_code = \"\"\n    if \".\" in code:\n        parent_code = code.rsplit(\".\", 1)[0]\n    parent_class = class_names.get(parent_code, default)\n\n    # Format class name\n    class_name = code_to_class_name(code)\n    class_names[code] = class_name\n\n    # If the parent class does not exist but the `code` shows some inheritance, we add a note for debugging\n    if parent_code and parent_class == default:\n        lines.append(\n            f\"# ! The parent class of {class_name} is not defined (missing {parent_class})\"\n        )\n\n    return parent_code, parent_class, class_name\n</code></pre>"},{"location":"references/api/#bam_masterdata.cli.fill_masterdata.MasterdataCodeGenerator.get_property_object_code","title":"<code>get_property_object_code(prop_data)</code>","text":"<p>Get the object code (or vocabulary code) used for reference for the assigned property with <code>prop_code</code>.</p> PARAMETER DESCRIPTION <code>prop_data</code> <p>The data information for the property as obtained from openBIS.</p> <p> TYPE: <code>dict</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The object/vocabulary code used for reference for the assigned property.</p> <p> TYPE: <code>str</code> </p> Source code in <code>bam_masterdata/cli/fill_masterdata.py</code> <pre><code>def get_property_object_code(self, prop_data: dict) -&gt; str:\n    \"\"\"\n    Get the object code (or vocabulary code) used for reference for the assigned property with `prop_code`.\n\n    Args:\n        prop_data (dict): The data information for the property as obtained from openBIS.\n\n    Returns:\n        str: The object/vocabulary code used for reference for the assigned property.\n    \"\"\"\n    if not prop_data:\n        return \"\"\n\n    # TODO check excel extractor to add sampleType column\n    object_code = prop_data.get(\"sampleType\", \"\")\n    if object_code:\n        return object_code\n\n    # TODO fix this patch and avoid using generator type\n    vocabulary_code = \"\"\n    if self.generator_type == \"openbis\":\n        vocabulary_code = prop_data.get(\"vocabulary\", \"\")\n    elif self.generator_type == \"excel\":\n        vocabulary_code = prop_data.get(\"vocabularyType\", \"\")\n    return vocabulary_code\n</code></pre>"},{"location":"references/api/#bam_masterdata.cli.fill_masterdata.MasterdataCodeGenerator.add_properties","title":"<code>add_properties(entities, parent_code, data, lines)</code>","text":"<p>Add the properties of the entity to the <code>lines</code> list. The properties are added as <code>PropertyTypeAssignment</code> objects.</p> <p>Note: the assigned properties do not have the information of <code>code</code> for the entity when data_type is OBJECT or CONTROLLEDVOCABULARY. These are instead defined in <code>property_types.py</code>.</p> PARAMETER DESCRIPTION <code>entities</code> <p>The dictionary of entities (objects, collections, datasets, vocabularies).</p> <p> TYPE: <code>dict</code> </p> <code>parent_code</code> <p>The code of the parent class.</p> <p> TYPE: <code>code</code> </p> <code>data</code> <p>The data information for the entity as obtained from openBIS.</p> <p> TYPE: <code>dict</code> </p> <code>lines</code> <p>A list of strings to be printed to the Python module.</p> <p> TYPE: <code>list</code> </p> Source code in <code>bam_masterdata/cli/fill_masterdata.py</code> <pre><code>def add_properties(\n    self, entities: dict, parent_code: str, data: dict, lines: list\n) -&gt; None:\n    \"\"\"\n    Add the properties of the entity to the `lines` list. The properties are added as\n    `PropertyTypeAssignment` objects.\n\n    Note: the assigned properties do not have the information of `code` for the entity when\n    data_type is OBJECT or CONTROLLEDVOCABULARY. These are instead defined in `property_types.py`.\n\n    Args:\n        entities (dict): The dictionary of entities (objects, collections, datasets, vocabularies).\n        parent_code (code): The code of the parent class.\n        data (dict): The data information for the entity as obtained from openBIS.\n        lines (list): A list of strings to be printed to the Python module.\n    \"\"\"\n    parent_properties_code = (\n        entities.get(parent_code, {}).get(\"properties\", {}).keys()\n    )\n    for prop_code, prop_data in data.get(\"properties\", {}).items():\n        # Skip \"UNKNOWN\" properties\n        # We check if the property is inherited from the parent class\n        if prop_code == \"UNKNOWN\" or prop_code in parent_properties_code:\n            continue\n\n        prop_name = prop_code.lstrip(\"$\").replace(\".\", \"_\").lower()\n        lines.append(f\"    {prop_name} = PropertyTypeAssignment(\")\n        lines.append(f'        code=\"{prop_code}\",')\n        # ! patching dataType=SAMPLE instead of OBJECT\n        data_type = prop_data.get(\"dataType\", \"\")\n        if data_type == \"SAMPLE\":\n            data_type = \"OBJECT\"\n        lines.append(f'        data_type=\"{data_type}\",')\n        if data_type == \"OBJECT\":\n            object_code = self.get_property_object_code(prop_data=prop_data)\n            if object_code:\n                lines.append(f'        object_code=\"{object_code}\",')\n        elif data_type == \"CONTROLLEDVOCABULARY\":\n            vocabulary_code = self.get_property_object_code(prop_data=prop_data)\n            if vocabulary_code:\n                lines.append(f'        vocabulary_code=\"{vocabulary_code}\",')\n\n        property_label = (prop_data.get(\"label\") or \"\").replace(\"\\n\", \"\\\\n\")\n        lines.append(f'        property_label=\"{property_label}\",')\n        description = (\n            (prop_data.get(\"description\") or \"\")\n            .replace('\"', \"`\")\n            .replace(\"\\n\", \"\\\\n\")\n            .replace(\"'\", \"`\")\n        )\n        lines.append(f'        description=\"\"\"{description}\"\"\",')\n        lines.append(f\"        mandatory={prop_data.get('mandatory', False)},\")\n        lines.append(\n            f\"        show_in_edit_views={prop_data.get('show_in_edit_views', False)},\"\n        )\n        section = (\n            (prop_data.get(\"section\") or \"\")\n            .replace('\"', '\\\\\"')\n            .replace(\"\\n\", \"\\\\n\")\n            .replace(\"'\", \"\\\\'\")\n        )\n        lines.append(f'        section=\"{section}\",')\n        lines.append(\"    )\")\n        lines.append(\"\")\n</code></pre>"},{"location":"references/api/#bam_masterdata.cli.fill_masterdata.MasterdataCodeGenerator.generate_collection_types","title":"<code>generate_collection_types()</code>","text":"<p>Generate Python code for the collection types in the Openbis datamodel. The code is generated as a string which is then printed out to the specific Python module in <code>bam_masterdata/datamodel/collection_types.py</code>.</p> RETURNS DESCRIPTION <code>str</code> <p>Python code for the collection types.</p> <p> TYPE: <code>str</code> </p> Source code in <code>bam_masterdata/cli/fill_masterdata.py</code> <pre><code>def generate_collection_types(self) -&gt; str:\n    \"\"\"\n    Generate Python code for the collection types in the Openbis datamodel. The code is generated\n    as a string which is then printed out to the specific Python module in `bam_masterdata/datamodel/collection_types.py`.\n\n    Returns:\n        str: Python code for the collection types.\n    \"\"\"\n    lines = []\n    class_names: dict = {}\n    # from bam_masterdata.metadata.definitions import (\n    #     CollectionTypeDef,\n    #     PropertyTypeAssignment,\n    # )\n    if self.collections != {}:\n        # Add imports at the top\n        lines.append(\"from bam_masterdata.metadata.definitions import (\")\n        lines.append(\"    CollectionTypeDef,\")\n        lines.append(\"    PropertyTypeAssignment,\")\n        lines.append(\")\")\n        lines.append(\"from bam_masterdata.metadata.entities import CollectionType\")\n        lines.append(\"\")\n        lines.append(\"\")\n\n    # Process each collection type\n    for code, data in self.collections.items():\n        # Skip the \"UNKNOWN\" object type\n        if code == \"UNKNOWN\":\n            continue\n\n        # Determine parent class\n        parent_code, parent_class, class_name = self.determine_parent_class(\n            code=code,\n            class_names=class_names,\n            default=\"CollectionType\",\n            lines=lines,\n        )\n\n        # Add class definition\n        lines.append(f\"class {class_name}({parent_class}):\")\n        lines.append(\"    defs = CollectionTypeDef(\")\n        lines.append(f'        code=\"{code}\",')\n        description = (\n            (data.get(\"description\") or \"\")\n            .replace('\"', \"`\")\n            .replace(\"\\n\", \"\\\\n\")\n            .replace(\"'\", \"`\")\n        )\n        lines.append(f'        description=\"\"\"{description}\"\"\",')\n        if data.get(\"validationPlugin\") != \"\":\n            lines.append(\n                f'        validation_script=\"{data.get(\"validationPlugin\")}\",'\n            )\n        lines.append(\"    )\")\n        lines.append(\"\")\n\n        # Add properties\n        self.add_properties(self.collections, parent_code, data, lines)\n        # Add newline between classes\n        lines.append(\"\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"references/api/#bam_masterdata.cli.fill_masterdata.MasterdataCodeGenerator.generate_dataset_types","title":"<code>generate_dataset_types()</code>","text":"<p>Generate Python code for the dataset types in the Openbis datamodel. The code is generated as a string which is then printed out to the specific Python module in <code>bam_masterdata/datamodel/dataset_types.py</code>.</p> RETURNS DESCRIPTION <code>str</code> <p>Python code for the dataset types.</p> <p> TYPE: <code>str</code> </p> Source code in <code>bam_masterdata/cli/fill_masterdata.py</code> <pre><code>def generate_dataset_types(self) -&gt; str:\n    \"\"\"\n    Generate Python code for the dataset types in the Openbis datamodel. The code is generated\n    as a string which is then printed out to the specific Python module in `bam_masterdata/datamodel/dataset_types.py`.\n\n    Returns:\n        str: Python code for the dataset types.\n    \"\"\"\n    lines = []\n    class_names: dict = {}\n\n    if self.datasets != {}:\n        # Add imports at the top\n        lines.append(\n            \"from bam_masterdata.metadata.definitions import DatasetTypeDef, PropertyTypeAssignment\"\n        )\n        lines.append(\"from bam_masterdata.metadata.entities import DatasetType\")\n        lines.append(\"\")\n        lines.append(\"\")\n\n    # Process each dataset type\n    for code, data in self.datasets.items():\n        # Skip the \"UNKNOWN\" object type\n        if code == \"UNKNOWN\":\n            continue\n\n        # Determine parent class\n        parent_code, parent_class, class_name = self.determine_parent_class(\n            code=code, class_names=class_names, default=\"DatasetType\", lines=lines\n        )\n\n        # Add class definition\n        lines.append(f\"class {class_name}({parent_class}):\")\n        lines.append(\"    defs = DatasetTypeDef(\")\n        lines.append(f'        code=\"{code}\",')\n        description = (\n            (data.get(\"description\") or \"\")\n            .replace('\"', \"`\")\n            .replace(\"\\n\", \"\\\\n\")\n            .replace(\"'\", \"`\")\n        )\n        lines.append(f'        description=\"\"\"{description}\"\"\",')\n        lines.append(\"    )\")\n        lines.append(\"\")\n\n        # Add properties\n        self.add_properties(self.datasets, parent_code, data, lines)\n        # Add newline between classes\n        lines.append(\"\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"references/api/#bam_masterdata.cli.fill_masterdata.MasterdataCodeGenerator.generate_object_types","title":"<code>generate_object_types()</code>","text":"<p>Generate Python code for the object types in the Openbis datamodel. The code is generated as a string which is then printed out to the specific Python module in <code>bam_masterdata/datamodel/object_types.py</code>.</p> RETURNS DESCRIPTION <code>str</code> <p>Python code for the object types.</p> <p> TYPE: <code>str</code> </p> Source code in <code>bam_masterdata/cli/fill_masterdata.py</code> <pre><code>def generate_object_types(self) -&gt; str:\n    \"\"\"\n    Generate Python code for the object types in the Openbis datamodel. The code is generated\n    as a string which is then printed out to the specific Python module in `bam_masterdata/datamodel/object_types.py`.\n\n    Returns:\n        str: Python code for the object types.\n    \"\"\"\n    lines = []\n    class_names: dict = {}\n\n    if self.objects != {}:\n        # Add imports at the top\n        lines.append(\n            \"from bam_masterdata.metadata.definitions import ObjectTypeDef, PropertyTypeAssignment\"\n        )\n        lines.append(\"from bam_masterdata.metadata.entities import ObjectType\")\n        lines.append(\"\")\n        lines.append(\"\")\n\n    # Process each object type\n    for code, data in self.objects.items():\n        # Skip the \"UNKNOWN\" object type\n        if code == \"UNKNOWN\":\n            continue\n\n        # Determine parent class\n        parent_code, parent_class, class_name = self.determine_parent_class(\n            code=code, class_names=class_names, default=\"ObjectType\", lines=lines\n        )\n\n        # Add class definition\n        lines.append(f\"class {class_name}({parent_class}):\")\n        lines.append(\"    defs = ObjectTypeDef(\")\n        lines.append(f'        code=\"{code}\",')\n        description = (\n            (data.get(\"description\") or \"\")\n            .replace('\"', \"`\")\n            .replace(\"\\n\", \"\\\\n\")\n            .replace(\"'\", \"`\")\n        )\n        lines.append(f'        description=\"\"\"{description}\"\"\",')\n        lines.append(\n            f'        generated_code_prefix=\"{data.get(\"generatedCodePrefix\", \"\")}\",'\n        )\n        lines.append(\"    )\")\n        lines.append(\"\")\n\n        # Add properties\n        self.add_properties(self.objects, parent_code, data, lines)\n        # Add newline between classes\n        lines.append(\"\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"references/api/#bam_masterdata.cli.fill_masterdata.MasterdataCodeGenerator.generate_vocabulary_types","title":"<code>generate_vocabulary_types()</code>","text":"<p>Generate Python code for the vocabulary types in the Openbis datamodel. The code is generated as a string which is then printed out to the specific Python module in <code>bam_masterdata/datamodel/vocabulary_types.py</code>.</p> RETURNS DESCRIPTION <code>str</code> <p>Python code for the vocabulary types.</p> <p> TYPE: <code>str</code> </p> Source code in <code>bam_masterdata/cli/fill_masterdata.py</code> <pre><code>def generate_vocabulary_types(self) -&gt; str:\n    \"\"\"\n    Generate Python code for the vocabulary types in the Openbis datamodel. The code is generated\n    as a string which is then printed out to the specific Python module in `bam_masterdata/datamodel/vocabulary_types.py`.\n\n    Returns:\n        str: Python code for the vocabulary types.\n    \"\"\"\n    lines = []\n    class_names: dict = {}\n\n    if self.vocabularies != {}:\n        # Add imports at the top\n        lines.append(\n            \"from bam_masterdata.metadata.definitions import VocabularyTerm, VocabularyTypeDef\"\n        )\n        lines.append(\"from bam_masterdata.metadata.entities import VocabularyType\")\n        lines.append(\"\")\n        lines.append(\"\")\n\n    # Process each object type\n    for code, data in self.vocabularies.items():\n        # Skip the \"UNKNOWN\" object type\n        if code == \"UNKNOWN\":\n            continue\n\n        # Determine parent class\n        parent_code, parent_class, class_name = self.determine_parent_class(\n            code=code,\n            class_names=class_names,\n            default=\"VocabularyType\",\n            lines=lines,\n        )\n\n        # Add class definition\n        lines.append(f\"class {class_name}({parent_class}):\")\n        lines.append(\"    defs = VocabularyTypeDef(\")\n        lines.append(f'        code=\"{code}\",')\n        description = (\n            (data.get(\"description\") or \"\")\n            .replace('\"', \"`\")\n            .replace(\"\\n\", \"\\\\n\")\n            .replace(\"'\", \"`\")\n        )\n        lines.append(f'        description=\"\"\"{description}\"\"\",')\n        lines.append(\"    )\")\n        lines.append(\"\")\n\n        # Add terms\n        parent_terms = self.objects.get(parent_code, {}).get(\"terms\", {}).keys()\n        for term_code, term_data in data.get(\"terms\", {}).items():\n            # Skip \"UNKNOWN\" properties\n            if term_code == \"UNKNOWN\":\n                continue\n\n            # We check if the term is inherited from the parent class\n            if term_code in parent_terms:\n                continue\n\n            term_name = (\n                term_code.lstrip(\"$\").replace(\".\", \"_\").replace(\"-\", \"_\").lower()\n            )\n            if term_name[0].isdigit():\n                term_name = f\"_{term_name}\"\n            if term_name == \"l\":\n                term_name = \"L\"\n            if term_name == \"O\":\n                term_name = \"o\"\n            if term_name == \"I\":\n                term_name = \"i\"\n            lines.append(f\"    {term_name} = VocabularyTerm(\")\n            lines.append(f'        code=\"{term_code}\",')\n            label = (term_data.get(\"label\") or \"\").replace('\"', \"\")\n            lines.append(f'        label=\"{label}\",')\n            description = (\n                (term_data.get(\"description\") or \"\")\n                .replace('\"', \"`\")\n                .replace(\"\\n\", \"\\\\n\")\n                .replace(\"'\", \"`\")\n            )\n            lines.append(f'        description=\"\"\"{description}\"\"\",')\n            lines.append(\"    )\")\n            lines.append(\"\")\n\n        # Add newline between classes\n        lines.append(\"\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"references/api/#bam_masterdata.excel.excel_to_entities","title":"<code>bam_masterdata.excel.excel_to_entities</code>","text":""},{"location":"references/api/#bam_masterdata.excel.excel_to_entities.MasterdataExcelExtractor","title":"<code>MasterdataExcelExtractor</code>","text":"Source code in <code>bam_masterdata/excel/excel_to_entities.py</code> <pre><code>class MasterdataExcelExtractor:\n    # TODO move these validation rules to a separate json\n    VALIDATION_RULES: dict[str, dict[str, dict[str, Any]]] = {}\n\n    def __init__(self, excel_path: str, **kwargs):\n        \"\"\"Initialize the MasterdataExtractor.\"\"\"\n        self.excel_path = excel_path\n        self.row_cell_info = kwargs.get(\"row_cell_info\", False)\n        self.workbook = openpyxl.load_workbook(excel_path)\n        self.logger = kwargs.get(\"logger\", logger)\n\n        # Load validation rules at initialization\n        if not MasterdataExcelExtractor.VALIDATION_RULES:\n            self.VALIDATION_RULES = load_validation_rules(\n                self.logger,\n                os.path.join(VALIDATION_RULES_DIR, \"excel_validation_rules.json\"),\n            )\n\n    def index_to_excel_column(self, index: int) -&gt; str:\n        \"\"\"\n        Converts a 1-based index to an Excel column name.\n\n        Args:\n            index: The 1-based index to convert.\n\n        Returns:\n            The corresponding Excel column name.\n        \"\"\"\n        if not index &gt;= 1:\n            raise ValueError(\"Index must be a positive integer starting from 1.\")\n\n        column = \"\"\n        while index &gt; 0:\n            index, remainder = divmod(index - 1, 26)\n            column = chr(65 + remainder) + column\n        return column\n\n    def get_last_non_empty_row(\n        self, sheet: \"Worksheet\", start_index: int\n    ) -&gt; int | None:\n        \"\"\"\n        Finds the last non-empty row before encountering a completely empty row.\n\n        Args:\n            sheet: The worksheet object.\n            start_index: The row number to start checking from (1-based index).\n\n        Returns:\n            The row number of the last non-empty row before an empty row is encountered,\n            or None if no non-empty rows are found starting from the given index.\n        \"\"\"\n        if start_index &lt; 1 or start_index &gt; sheet.max_row:\n            raise ValueError(\n                f\"Invalid start index: {start_index}. It must be between 1 and {sheet.max_row}.\"\n            )\n\n        last_non_empty_row = None\n        for row in range(start_index, sheet.max_row + 1):\n            if all(\n                sheet.cell(row=row, column=col).value in (None, \"\")\n                for col in range(1, sheet.max_column + 1)\n            ):\n                return last_non_empty_row  # Return the last non-empty row before the current empty row\n\n            last_non_empty_row = row  # Update the last non-empty row\n\n        return last_non_empty_row  # If no empty row is encountered, return the last non-empty row\n\n    def str_to_bool(\n        self,\n        value: str | bool | None,\n        term: str,\n        coordinate: str,\n        sheet_title: str,\n    ) -&gt; bool:\n        \"\"\"\n        Converts a string to a boolean value.\n\n        Args:\n            value: The string to convert.\n\n        Returns:\n            The boolean value.\n        \"\"\"\n        # No `value` provided\n        if not value:\n            return False\n\n        val = str(value).strip().lower()\n        if val not in [\"true\", \"false\"]:\n            self.logger.error(\n                f\"Invalid {term.lower()} value found in the {term} column at position {coordinate} in {sheet_title}. Accepted values: TRUE or FALSE.\",\n                term=term,\n                cell_value=val,\n                cell_coordinate=coordinate,\n                sheet_title=sheet_title,\n            )\n        return val == \"true\"\n\n    def get_and_check_property(\n        self,\n        value: str | bool | None,\n        term: str,\n        coordinate: str,\n        sheet_title: str,\n        is_description: bool = False,\n        is_code: bool = False,\n        is_data: bool = False,\n        is_url: bool = False,\n    ) -&gt; str:\n        \"\"\"\n        Gets a property and checks its format.\n\n        Args:\n            value: The string to convert.\n\n        Returns:\n            The property.\n        \"\"\"\n\n        # No `value` provided\n        if not value:\n            return \"\"\n\n        val = str(value)\n        error_message = f\"Invalid {term.lower()} value found in the {term} column at position {coordinate} in {sheet_title}.\"\n        if is_description:\n            if not re.match(r\".*//.*\", val):\n                self.logger.error(\n                    error_message\n                    + \"Description should follow the schema: English Description + '//' + German Description. \",\n                    term=term,\n                    cell_value=val,\n                    cell_coordinate=coordinate,\n                    sheet_title=sheet_title,\n                )\n        elif is_code:\n            if not re.match(r\"^\\$?[A-Z0-9_.]+$\", val):\n                self.logger.error(\n                    error_message,\n                    term=term,\n                    cell_value=val,\n                    cell_coordinate=coordinate,\n                    sheet_title=sheet_title,\n                )\n        elif is_data:\n            # Normalize data type to uppercase and allow dynamic SAMPLE/OBJECT codes\n            val_upper = val.upper()\n            allowed_types = [dt.value for dt in DataType]\n            is_valid_standard = val_upper in allowed_types\n            is_valid_dynamic = False\n\n            if not is_valid_standard and (\n                val_upper.startswith(\"SAMPLE:\") or val_upper.startswith(\"OBJECT:\")\n            ):\n                parts = val_upper.split(\":\", 1)\n                if len(parts) == 2 and parts[1].strip():\n                    is_valid_dynamic = True\n\n            if not is_valid_standard and not is_valid_dynamic:\n                self.logger.error(\n                    error_message\n                    + f\"The Data Type should be one of the following: {allowed_types} or follow the format 'SAMPLE:&lt;CODE&gt;' or 'OBJECT:&lt;CODE&gt;'\",\n                    term=term,\n                    cell_value=val_upper,\n                    cell_coordinate=coordinate,\n                    sheet_title=sheet_title,\n                )\n            val = val_upper\n        elif is_url:\n            if not re.match(\n                r\"https?://(?:www\\.)?[a-zA-Z0-9-._~:/?#@!$&amp;'()*+,;=%]+\", val\n            ):\n                self.logger.error(\n                    error_message,\n                    term=term,\n                    cell_value=val,\n                    cell_coordinate=coordinate,\n                    sheet_title=sheet_title,\n                )\n        else:\n            if not re.match(r\".*\", val):\n                self.logger.error(\n                    error_message,\n                    term=term,\n                    cell_value=val,\n                    cell_coordinate=coordinate,\n                    sheet_title=sheet_title,\n                )\n        return val\n\n    # Helper function to process each term\n    def process_term(\n        self, term: str, cell_value: Any, coordinate: str, sheet_title: str\n    ) -&gt; Any:\n        \"\"\"\n        Processes a term by converting it to a boolean if necessary or checking its validity.\n\n        Args:\n            term: The term being processed.\n            cell_value: The value of the cell.\n            coordinate: The coordinate of the cell in the sheet.\n            sheet_title: The title of the sheet.\n\n        Returns:\n            The processed value, either as a boolean or the original value after validation.\n        \"\"\"\n        # Check if the term is a boolean type\n        if term in (\"Mandatory\", \"Show in edit views\"):\n            return self.str_to_bool(\n                value=cell_value,\n                term=term,\n                coordinate=coordinate,\n                sheet_title=sheet_title,\n            )\n        # Check and validate the property\n        return self.get_and_check_property(\n            value=cell_value,\n            term=term,\n            coordinate=coordinate,\n            sheet_title=sheet_title,\n            is_code=(term in [\"Code\", \"Vocabulary code\"]),\n            is_data=(term == \"Data type\"),\n        )\n\n    def extract_value(\n        self,\n        sheet: \"Worksheet\",\n        row: int,\n        column: int,\n        validation_pattern: str = None,\n        is_description: bool = False,\n        is_data: bool = False,\n        is_url: bool = False,\n    ) -&gt; str:\n        \"\"\"\n        Extracts and validates a value from a specified cell in the Excel sheet.\n\n        Args:\n            sheet: The worksheet object.\n            row: The row number of the cell (1-based index).\n            column: The column number of the cell (1-based index).\n            validation_pattern: Optional regex pattern to validate the cell value.\n            is_description: Flag indicating if the value is a description.\n            is_data: Flag indicating if the value is a data type.\n            is_url: Flag indicating if the value is a URL.\n\n        Returns:\n            The extracted and validated cell value as a string. Returns an empty string if the value is invalid or not provided.\n        \"\"\"\n        value = sheet.cell(row=row, column=column).value\n\n        # No `value` provided\n        if not value:\n            return \"\"\n\n        validated = (\n            bool(re.match(validation_pattern, str(value)))\n            if validation_pattern\n            else True\n        )\n        error_message = f\"Invalid value '{value}' at row {row}, column {column} in sheet {sheet.title}\"\n\n        if is_description:\n            error_message += \" Description should follow the schema: English Description + '//' + German Description.\"\n        elif is_data:\n            val_upper = str(value).upper()\n            allowed_types = [dt.value for dt in DataType]\n            is_valid_standard = val_upper in allowed_types\n            is_valid_dynamic = False\n            if not is_valid_standard and (\n                val_upper.startswith(\"SAMPLE:\") or val_upper.startswith(\"OBJECT:\")\n            ):\n                parts = val_upper.split(\":\", 1)\n                if len(parts) == 2 and parts[1].strip():\n                    is_valid_dynamic = True\n            validated = is_valid_standard or is_valid_dynamic\n            error_message += f\" The Data Type should be one of the following: {allowed_types} or follow the format 'SAMPLE:&lt;CODE&gt;' or 'OBJECT:&lt;CODE&gt;'\"\n        elif is_url:\n            error_message += \" It should be an URL or empty\"\n\n        if not validated:\n            self.logger.error(\n                error_message,\n                cell_value=value,\n                sheet_title=sheet.title,\n                row=row,\n                column=column,\n            )\n\n        return value or \"\"\n\n    def process_entity(\n        self,\n        sheet: \"Worksheet\",\n        start_index_row: int,\n        header_terms: list[str],\n        expected_terms: list[str],\n        entity_type: str,\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Process an entity type block in the Excel sheet and return its attributes as a dictionary.\n\n        Args:\n            sheet: The worksheet object.\n            start_index_row: The row where the current entity type begins (1-based index).\n            header_terms: List of header terms in the entity block.\n            expected_terms: List of expected terms to extract from the entity block.\n            entity_type: The type of the entity (e.g., SAMPLE_TYPE, OBJECT_TYPE).\n\n        Returns:\n            A dictionary containing the attributes of the entity.\n        \"\"\"\n        attributes: dict = {}\n        cell_value: Any = \"\"\n\n        for term in expected_terms:\n            if term not in header_terms:\n                self.logger.error(f\"{term} not found in the headers.\", term=term)\n            else:\n                term_index = header_terms.index(term)\n                cell = sheet.cell(row=start_index_row + 2, column=term_index + 1)\n                cell_value = self.extract_value(\n                    sheet,\n                    start_index_row + 2,\n                    term_index + 1,\n                    self.VALIDATION_RULES[entity_type][term].get(\"pattern\"),\n                )\n\n                # Handle boolean conversion\n                if self.VALIDATION_RULES[entity_type][term].get(\"is_bool\"):\n                    cell_value = self.str_to_bool(\n                        value=cell_value,\n                        term=term,\n                        coordinate=cell.coordinate,\n                        sheet_title=sheet.title,\n                    )\n\n                # Handle data type validation\n                elif self.VALIDATION_RULES[entity_type][term].get(\"is_data\"):\n                    allowed_types = [dt.value for dt in DataType]\n                    cell_value_upper = str(cell_value).upper()\n                    is_valid_standard = cell_value_upper in allowed_types\n                    is_valid_dynamic = False\n                    if not is_valid_standard and (\n                        cell_value_upper.startswith(\"SAMPLE:\")\n                        or cell_value_upper.startswith(\"OBJECT:\")\n                    ):\n                        parts = cell_value_upper.split(\":\", 1)\n                        if len(parts) == 2 and parts[1].strip():\n                            is_valid_dynamic = True\n                    if not is_valid_standard and not is_valid_dynamic:\n                        self.logger.error(\n                            f\"Invalid Data Type: {cell_value} in {cell.coordinate} (Sheet: {sheet.title}). Should be one of the following: {allowed_types} or follow the format 'SAMPLE:&lt;CODE&gt;' or 'OBJECT:&lt;CODE&gt;'\",\n                            term=term,\n                            cell_value=cell_value,\n                            cell_coordinate=cell.coordinate,\n                            sheet_title=sheet.title,\n                        )\n                    else:\n                        cell_value = (\n                            cell_value_upper\n                            if isinstance(cell_value, str)\n                            else cell_value\n                        )\n\n                # Handle additional validation for \"Generated code prefix\"\n                elif (\n                    self.VALIDATION_RULES[entity_type][term].get(\"extra_validation\")\n                    == \"is_reduced_version\"\n                ):\n                    if not is_reduced_version(cell_value, attributes.get(\"code\", \"\")):\n                        self.logger.warning(\n                            f\"Invalid {term} value '{cell_value}' in {cell.coordinate} (Sheet: {sheet.title}). \"\n                            f\"Generated code prefix should be part of the 'Code' {attributes.get('code', '')}.\",\n                            term=term,\n                            cell_value=cell_value,\n                            cell_coordinate=cell.coordinate,\n                            sheet_title=sheet.title,\n                        )\n\n                # Handle validation script (allows empty but must match pattern if provided)\n                elif (\n                    self.VALIDATION_RULES[entity_type][term].get(\"allow_empty\")\n                    and not cell_value\n                ):\n                    cell_value = None\n\n                # Handle URL template validation (allows empty but must be a valid URL)\n                elif (\n                    self.VALIDATION_RULES[entity_type][term].get(\"is_url\")\n                    and cell_value\n                ):\n                    url_pattern = self.VALIDATION_RULES[entity_type][term].get(\n                        \"pattern\"\n                    )\n                    if not re.match(url_pattern, str(cell_value)):\n                        self.logger.error(\n                            f\"Invalid URL format: {cell_value} in {cell.coordinate} (Sheet: {sheet.title})\",\n                            cell_value=cell_value,\n                            cell_coordinate=cell.coordinate,\n                            sheet_title=sheet.title,\n                        )\n\n                # Add the extracted value to the attributes dictionary\n                attributes[self.VALIDATION_RULES[entity_type][term].get(\"key\")] = (\n                    cell_value\n                )\n\n        if self.row_cell_info:\n            attributes[\"row_location\"] = f\"A{start_index_row}\"\n        return attributes\n\n    def properties_to_dict(\n        self, sheet: \"Worksheet\", start_index_row: int, last_non_empty_row: int\n    ) -&gt; dict[str, dict[str, Any]]:\n        \"\"\"\n        Extracts properties from an Entity type block in the Excel sheet and returns them as a dictionary.\n\n        Args:\n            sheet: The worksheet object.\n            start_index_row: Row where the current entity type begins (1-based index).\n            last_non_empty_row: Row where the current entity type finish (1-based index).\n\n        Returns:\n            A dictionary where each key is a property code and the value is a dictionary\n            containing the attributes of the property.\n        \"\"\"\n        property_dict: dict = {}\n        expected_terms = [\n            \"Code\",\n            \"Description\",\n            \"Mandatory\",\n            \"Show in edit views\",\n            \"Section\",\n            \"Property label\",\n            \"Data type\",\n            \"Vocabulary code\",\n            \"Metadata\",\n            \"Dynamic script\",\n            # ! these are not used\n            # \"Unique\",\n            # \"Internal assignment\",\n        ]\n\n        # Determine the header row index\n        header_index = start_index_row + 3\n        row_headers = [(cell.value, cell.coordinate) for cell in sheet[header_index]]\n        # And store how many properties are for the entity\n        n_properties = last_non_empty_row - header_index\n        if n_properties &lt; 0:\n            self.logger.error(\n                f\"No properties found for the entity in sheet {sheet.title} starting at row {start_index_row}.\"\n            )\n            return property_dict\n\n        # Initialize a dictionary to store extracted columns\n        extracted_columns: dict[str, list] = {term: [] for term in expected_terms}\n        if self.row_cell_info:\n            extracted_columns[\"row_location\"] = []\n\n        # Extract columns for each expected term\n        for i, (term, coordinate) in enumerate(row_headers):\n            if term not in expected_terms:\n                log_func = (\n                    self.logger.warning\n                    if term\n                    in (\n                        \"Mandatory\",\n                        \"Show in edit views\",\n                        \"Section\",\n                        \"Metadata\",\n                        \"Dynamic script\",\n                        \"Vocabulary code\",\n                        # ! these are not used\n                        # \"Unique\",\n                        # \"Internal assignment\",\n                    )\n                    else self.logger.error\n                )\n                log_func(f\"'{term}' not found in the properties headers.\", term=term)\n                continue\n\n            # Excel column letter from the coordinate\n            term_letter = coordinate[0]\n\n            # Extract values from the column\n            for cell_property in sheet[term_letter][header_index:last_non_empty_row]:\n                extracted_columns[term].append(\n                    self.process_term(\n                        term, cell_property.value, cell_property.coordinate, sheet.title\n                    )\n                )\n                if self.row_cell_info:\n                    extracted_columns[\"row_location\"].append(cell_property.coordinate)\n\n        # Combine extracted values into a dictionary\n        for i in range(n_properties):\n            code = extracted_columns.get(\"Code\", [])\n            if not code:\n                self.logger.error(\n                    f\"'Code' not found in the properties headers for sheet {sheet.title}.\"\n                )\n                return property_dict\n            code = code[i]\n            property_dict[code] = {\"permId\": code, \"code\": code}\n            for key, pybis_val in {\n                \"Description\": \"description\",\n                \"Section\": \"section\",\n                \"Mandatory\": \"mandatory\",\n                \"Show in edit views\": \"show_in_edit_views\",\n                \"Property label\": \"label\",\n                \"Data type\": \"dataType\",\n                \"Vocabulary code\": \"vocabularyCode\",\n            }.items():\n                data_column = extracted_columns.get(key, [])\n                if not data_column:\n                    continue\n                cell_value = data_column[i]\n                if key == \"Data type\":\n                    object_code = None\n                    normalized_value = (\n                        str(cell_value).upper()\n                        if isinstance(cell_value, str)\n                        else cell_value\n                    )\n                    if isinstance(normalized_value, str) and \":\" in normalized_value:\n                        prefix, dynamic_code = normalized_value.split(\":\", 1)\n                        if prefix in (\"SAMPLE\", \"OBJECT\") and dynamic_code.strip():\n                            object_code = dynamic_code.strip()\n                            normalized_value = DataType.OBJECT.value\n                    property_dict[code][pybis_val] = normalized_value\n                    if object_code:\n                        property_dict[code][\"objectCode\"] = object_code\n                else:\n                    property_dict[code][pybis_val] = cell_value\n            if self.row_cell_info:\n                property_dict[code][\"row_location\"] = (\n                    extracted_columns.get(\"row_location\")[i],\n                )\n            # Only add optional fields if they exist in extracted_columns\n            optional_fields = [\n                \"Metadata\",\n                \"Dynamic script\",\n                \"Unique\",\n                \"Internal assignment\",\n            ]\n            for field in optional_fields:\n                if (\n                    field in extracted_columns\n                ):  # Check if the field exists in the extracted columns\n                    if extracted_columns[field][i] == \"\":\n                        extracted_columns[field][i] = None\n                    property_dict[extracted_columns[\"Code\"][i]][\n                        field.lower().replace(\" \", \"_\")\n                    ] = extracted_columns[field][i]\n\n        return property_dict\n\n    def terms_to_dict(\n        self, sheet: \"Worksheet\", start_index_row: int, last_non_empty_row: int\n    ) -&gt; dict[str, dict[str, Any]]:\n        \"\"\"\n        Extracts terms from a Vocabulary block in the Excel sheet and returns them as a dictionary.\n\n        Args:\n            sheet: The worksheet object.\n            start_index_row: Row where the current entity type begins (1-based index).\n            last_non_empty_row: Row where the current entity type finish (1-based index).\n\n        Returns:\n            A dictionary where each key is a vocabulary term code and the value is a dictionary\n            containing the attributes of the vocabulary term.\n        \"\"\"\n        terms_dict = {}\n        expected_terms = [\"Code\", \"Description\", \"Url template\", \"Label\", \"Official\"]\n\n        header_index = start_index_row + 3\n        row_headers = [cell.value for cell in sheet[header_index]]\n\n        # Initialize a dictionary to store extracted columns\n        extracted_columns: dict[str, list] = {term: [] for term in expected_terms}\n\n        # Helper function to process each term\n        def process_term_cell(term, cell_value, coordinate, sheet_title):\n            if term == \"Official\":\n                return self.str_to_bool(\n                    value=cell_value,\n                    term=term,\n                    coordinate=coordinate,\n                    sheet_title=sheet_title,\n                )\n            return self.get_and_check_property(\n                value=cell_value,\n                term=term,\n                coordinate=coordinate,\n                sheet_title=sheet_title,\n                is_code=(term == \"Code\"),\n                is_url=(term == \"Url template\"),\n            )\n\n        # Extract columns for each expected term\n        for term in expected_terms:\n            if term not in row_headers:\n                self.logger.warning(\n                    f\"{term} not found in the properties headers.\", term=term\n                )\n                continue\n\n            # Get column index and Excel letter\n            term_index = row_headers.index(term) + 1\n            term_letter = self.index_to_excel_column(term_index)\n\n            # Extract values from the column\n            for cell in sheet[term_letter][header_index:last_non_empty_row]:\n                extracted_columns[term].append(\n                    process_term_cell(term, cell.value, cell.coordinate, sheet.title)\n                )\n\n        if not extracted_columns.get(\"Code\"):\n            self.logger.error(\n                f\"The required 'Code' column for terms was not found in sheet {sheet.title}.\"\n            )\n            return {}\n\n        # Combine extracted values into a dictionary safely\n        for i in range(len(extracted_columns[\"Code\"])):\n            code = extracted_columns[\"Code\"][i]\n            terms_dict[code] = {\n                \"permId\": code,\n                \"code\": code,\n            }\n            for key, pybis_val in {\n                \"Description\": \"descriptions\",\n                \"Url template\": \"url_template\",\n                \"Label\": \"label\",\n                \"Official\": \"official\",\n            }.items():\n                values = extracted_columns.get(key, [])\n                if len(values) &gt; i:\n                    terms_dict[code][pybis_val] = values[i]\n\n        return terms_dict\n\n    def block_to_entity_dict(\n        self,\n        sheet: \"Worksheet\",\n        start_index_row: int,\n        last_non_empty_row: int,\n        complete_dict: dict[str, Any],\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Extracts entity attributes from an Excel sheet block and returns them as a dictionary.\n        \"\"\"\n        attributes_dict: dict = {}\n\n        # Get the entity type\n        entity_type = sheet[f\"A{start_index_row}\"].value\n        if entity_type not in self.VALIDATION_RULES:\n            raise ValueError(f\"Invalid entity type: {entity_type}\")\n\n        # Get the header terms\n        header_terms = [cell.value for cell in sheet[start_index_row + 1]]\n\n        # Process entity data using the helper function\n        attributes_dict = self.process_entity(\n            sheet,\n            start_index_row,\n            header_terms,\n            list(self.VALIDATION_RULES[entity_type].keys()),\n            entity_type,\n        )\n\n        # Extract additional attributes if necessary\n        if entity_type in {\n            \"SAMPLE_TYPE\",\n            \"OBJECT_TYPE\",\n            \"EXPERIMENT_TYPE\",\n            \"DATASET_TYPE\",\n        }:\n            attributes_dict[\"properties\"] = (\n                self.properties_to_dict(sheet, start_index_row, last_non_empty_row)\n                or {}\n            )\n\n        elif entity_type == \"VOCABULARY_TYPE\":\n            attributes_dict[\"terms\"] = (\n                self.terms_to_dict(sheet, start_index_row, last_non_empty_row) or {}\n            )\n\n        # Add the entity to the complete dictionary\n        complete_dict[attributes_dict[\"code\"]] = attributes_dict\n\n        # Return sorted dictionary\n        return dict(sorted(complete_dict.items(), key=lambda item: item[0].count(\".\")))\n\n    def excel_to_entities(self) -&gt; dict[str, dict[str, Any]]:\n        \"\"\"\n        Extracts entities from an Excel file and returns them as a dictionary.\n\n        Returns:\n            dict[str, dict[str, Any]]: A dictionary where each key is a normalized sheet name and the value is a dictionary\n            containing the extracted entities. Returns an empty dictionary if all sheets are empty.\n        \"\"\"\n        sheets_dict: dict[str, dict[str, Any]] = {}\n        sheet_names = self.workbook.sheetnames\n        has_content = False  # Track if any sheet has valid content\n\n        for i, sheet_name in enumerate(sheet_names):\n            normalized_sheet_name = sheet_name.lower().replace(\" \", \"_\")\n            sheet = self.workbook[sheet_name]\n            start_row = 1\n\n            # **Check if the sheet is empty**\n            if all(\n                sheet.cell(row=row, column=col).value in (None, \"\")\n                for row in range(1, sheet.max_row + 1)\n                for col in range(1, sheet.max_column + 1)\n            ):\n                self.logger.info(f\"Skipping empty sheet: {sheet_name}\")\n                continue  # Move to the next sheet\n\n            sheets_dict[normalized_sheet_name] = {}\n\n            consecutive_empty_rows = 0  # Track consecutive empty rows\n            while start_row &lt;= sheet.max_row:\n                # **Check for two consecutive empty rows**\n                is_row_empty = all(\n                    sheet.cell(row=start_row, column=col).value in (None, \"\")\n                    for col in range(1, sheet.max_column + 1)\n                )\n\n                if is_row_empty:\n                    consecutive_empty_rows += 1\n                    if consecutive_empty_rows &gt;= 2:\n                        # **Reached the end of the sheet, move to the next**\n                        if i == len(sheet_names) - 1:\n                            self.logger.info(\n                                f\"Last sheet {sheet_name} processed. End of the file reached.\"\n                            )\n                        else:\n                            self.logger.info(\n                                f\"End of the current sheet {sheet_name} reached. Switching to next sheet...\"\n                            )\n                        break  # Stop processing this sheet\n                else:\n                    consecutive_empty_rows = 0  # Reset if we find a non-empty row\n\n                    # **Process the entity block**\n                    last_non_empty_row = self.get_last_non_empty_row(sheet, start_row)\n                    if last_non_empty_row is None:\n                        break  # No more valid blocks\n\n                    sheets_dict[normalized_sheet_name] = self.block_to_entity_dict(\n                        sheet,\n                        start_row,\n                        last_non_empty_row,\n                        sheets_dict[normalized_sheet_name],\n                    )\n                    has_content = True  # Found valid content\n\n                    # Move to the next entity block\n                    start_row = last_non_empty_row + 1\n                    continue  # Continue loop without increasing consecutive_empty_rows\n\n                start_row += 1  # Move to the next row\n\n        # **If no sheets had content, return an empty dictionary**\n        if not has_content:\n            self.logger.warning(\n                \"No valid data found in any sheets. Returning empty dictionary.\"\n            )\n            return {}\n\n        return sheets_dict\n</code></pre>"},{"location":"references/api/#bam_masterdata.excel.excel_to_entities.MasterdataExcelExtractor.VALIDATION_RULES","title":"<code>VALIDATION_RULES = {}</code>","text":""},{"location":"references/api/#bam_masterdata.excel.excel_to_entities.MasterdataExcelExtractor.excel_path","title":"<code>excel_path = excel_path</code>","text":""},{"location":"references/api/#bam_masterdata.excel.excel_to_entities.MasterdataExcelExtractor.row_cell_info","title":"<code>row_cell_info = kwargs.get('row_cell_info', False)</code>","text":""},{"location":"references/api/#bam_masterdata.excel.excel_to_entities.MasterdataExcelExtractor.workbook","title":"<code>workbook = openpyxl.load_workbook(excel_path)</code>","text":""},{"location":"references/api/#bam_masterdata.excel.excel_to_entities.MasterdataExcelExtractor.logger","title":"<code>logger = kwargs.get('logger', logger)</code>","text":""},{"location":"references/api/#bam_masterdata.excel.excel_to_entities.MasterdataExcelExtractor.__init__","title":"<code>__init__(excel_path, **kwargs)</code>","text":"<p>Initialize the MasterdataExtractor.</p> Source code in <code>bam_masterdata/excel/excel_to_entities.py</code> <pre><code>def __init__(self, excel_path: str, **kwargs):\n    \"\"\"Initialize the MasterdataExtractor.\"\"\"\n    self.excel_path = excel_path\n    self.row_cell_info = kwargs.get(\"row_cell_info\", False)\n    self.workbook = openpyxl.load_workbook(excel_path)\n    self.logger = kwargs.get(\"logger\", logger)\n\n    # Load validation rules at initialization\n    if not MasterdataExcelExtractor.VALIDATION_RULES:\n        self.VALIDATION_RULES = load_validation_rules(\n            self.logger,\n            os.path.join(VALIDATION_RULES_DIR, \"excel_validation_rules.json\"),\n        )\n</code></pre>"},{"location":"references/api/#bam_masterdata.excel.excel_to_entities.MasterdataExcelExtractor.index_to_excel_column","title":"<code>index_to_excel_column(index)</code>","text":"<p>Converts a 1-based index to an Excel column name.</p> PARAMETER DESCRIPTION <code>index</code> <p>The 1-based index to convert.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The corresponding Excel column name.</p> Source code in <code>bam_masterdata/excel/excel_to_entities.py</code> <pre><code>def index_to_excel_column(self, index: int) -&gt; str:\n    \"\"\"\n    Converts a 1-based index to an Excel column name.\n\n    Args:\n        index: The 1-based index to convert.\n\n    Returns:\n        The corresponding Excel column name.\n    \"\"\"\n    if not index &gt;= 1:\n        raise ValueError(\"Index must be a positive integer starting from 1.\")\n\n    column = \"\"\n    while index &gt; 0:\n        index, remainder = divmod(index - 1, 26)\n        column = chr(65 + remainder) + column\n    return column\n</code></pre>"},{"location":"references/api/#bam_masterdata.excel.excel_to_entities.MasterdataExcelExtractor.get_last_non_empty_row","title":"<code>get_last_non_empty_row(sheet, start_index)</code>","text":"<p>Finds the last non-empty row before encountering a completely empty row.</p> PARAMETER DESCRIPTION <code>sheet</code> <p>The worksheet object.</p> <p> TYPE: <code>Worksheet</code> </p> <code>start_index</code> <p>The row number to start checking from (1-based index).</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>int | None</code> <p>The row number of the last non-empty row before an empty row is encountered,</p> <code>int | None</code> <p>or None if no non-empty rows are found starting from the given index.</p> Source code in <code>bam_masterdata/excel/excel_to_entities.py</code> <pre><code>def get_last_non_empty_row(\n    self, sheet: \"Worksheet\", start_index: int\n) -&gt; int | None:\n    \"\"\"\n    Finds the last non-empty row before encountering a completely empty row.\n\n    Args:\n        sheet: The worksheet object.\n        start_index: The row number to start checking from (1-based index).\n\n    Returns:\n        The row number of the last non-empty row before an empty row is encountered,\n        or None if no non-empty rows are found starting from the given index.\n    \"\"\"\n    if start_index &lt; 1 or start_index &gt; sheet.max_row:\n        raise ValueError(\n            f\"Invalid start index: {start_index}. It must be between 1 and {sheet.max_row}.\"\n        )\n\n    last_non_empty_row = None\n    for row in range(start_index, sheet.max_row + 1):\n        if all(\n            sheet.cell(row=row, column=col).value in (None, \"\")\n            for col in range(1, sheet.max_column + 1)\n        ):\n            return last_non_empty_row  # Return the last non-empty row before the current empty row\n\n        last_non_empty_row = row  # Update the last non-empty row\n\n    return last_non_empty_row  # If no empty row is encountered, return the last non-empty row\n</code></pre>"},{"location":"references/api/#bam_masterdata.excel.excel_to_entities.MasterdataExcelExtractor.str_to_bool","title":"<code>str_to_bool(value, term, coordinate, sheet_title)</code>","text":"<p>Converts a string to a boolean value.</p> PARAMETER DESCRIPTION <code>value</code> <p>The string to convert.</p> <p> TYPE: <code>str | bool | None</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>The boolean value.</p> Source code in <code>bam_masterdata/excel/excel_to_entities.py</code> <pre><code>def str_to_bool(\n    self,\n    value: str | bool | None,\n    term: str,\n    coordinate: str,\n    sheet_title: str,\n) -&gt; bool:\n    \"\"\"\n    Converts a string to a boolean value.\n\n    Args:\n        value: The string to convert.\n\n    Returns:\n        The boolean value.\n    \"\"\"\n    # No `value` provided\n    if not value:\n        return False\n\n    val = str(value).strip().lower()\n    if val not in [\"true\", \"false\"]:\n        self.logger.error(\n            f\"Invalid {term.lower()} value found in the {term} column at position {coordinate} in {sheet_title}. Accepted values: TRUE or FALSE.\",\n            term=term,\n            cell_value=val,\n            cell_coordinate=coordinate,\n            sheet_title=sheet_title,\n        )\n    return val == \"true\"\n</code></pre>"},{"location":"references/api/#bam_masterdata.excel.excel_to_entities.MasterdataExcelExtractor.get_and_check_property","title":"<code>get_and_check_property(value, term, coordinate, sheet_title, is_description=False, is_code=False, is_data=False, is_url=False)</code>","text":"<p>Gets a property and checks its format.</p> PARAMETER DESCRIPTION <code>value</code> <p>The string to convert.</p> <p> TYPE: <code>str | bool | None</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The property.</p> Source code in <code>bam_masterdata/excel/excel_to_entities.py</code> <pre><code>def get_and_check_property(\n    self,\n    value: str | bool | None,\n    term: str,\n    coordinate: str,\n    sheet_title: str,\n    is_description: bool = False,\n    is_code: bool = False,\n    is_data: bool = False,\n    is_url: bool = False,\n) -&gt; str:\n    \"\"\"\n    Gets a property and checks its format.\n\n    Args:\n        value: The string to convert.\n\n    Returns:\n        The property.\n    \"\"\"\n\n    # No `value` provided\n    if not value:\n        return \"\"\n\n    val = str(value)\n    error_message = f\"Invalid {term.lower()} value found in the {term} column at position {coordinate} in {sheet_title}.\"\n    if is_description:\n        if not re.match(r\".*//.*\", val):\n            self.logger.error(\n                error_message\n                + \"Description should follow the schema: English Description + '//' + German Description. \",\n                term=term,\n                cell_value=val,\n                cell_coordinate=coordinate,\n                sheet_title=sheet_title,\n            )\n    elif is_code:\n        if not re.match(r\"^\\$?[A-Z0-9_.]+$\", val):\n            self.logger.error(\n                error_message,\n                term=term,\n                cell_value=val,\n                cell_coordinate=coordinate,\n                sheet_title=sheet_title,\n            )\n    elif is_data:\n        # Normalize data type to uppercase and allow dynamic SAMPLE/OBJECT codes\n        val_upper = val.upper()\n        allowed_types = [dt.value for dt in DataType]\n        is_valid_standard = val_upper in allowed_types\n        is_valid_dynamic = False\n\n        if not is_valid_standard and (\n            val_upper.startswith(\"SAMPLE:\") or val_upper.startswith(\"OBJECT:\")\n        ):\n            parts = val_upper.split(\":\", 1)\n            if len(parts) == 2 and parts[1].strip():\n                is_valid_dynamic = True\n\n        if not is_valid_standard and not is_valid_dynamic:\n            self.logger.error(\n                error_message\n                + f\"The Data Type should be one of the following: {allowed_types} or follow the format 'SAMPLE:&lt;CODE&gt;' or 'OBJECT:&lt;CODE&gt;'\",\n                term=term,\n                cell_value=val_upper,\n                cell_coordinate=coordinate,\n                sheet_title=sheet_title,\n            )\n        val = val_upper\n    elif is_url:\n        if not re.match(\n            r\"https?://(?:www\\.)?[a-zA-Z0-9-._~:/?#@!$&amp;'()*+,;=%]+\", val\n        ):\n            self.logger.error(\n                error_message,\n                term=term,\n                cell_value=val,\n                cell_coordinate=coordinate,\n                sheet_title=sheet_title,\n            )\n    else:\n        if not re.match(r\".*\", val):\n            self.logger.error(\n                error_message,\n                term=term,\n                cell_value=val,\n                cell_coordinate=coordinate,\n                sheet_title=sheet_title,\n            )\n    return val\n</code></pre>"},{"location":"references/api/#bam_masterdata.excel.excel_to_entities.MasterdataExcelExtractor.process_term","title":"<code>process_term(term, cell_value, coordinate, sheet_title)</code>","text":"<p>Processes a term by converting it to a boolean if necessary or checking its validity.</p> PARAMETER DESCRIPTION <code>term</code> <p>The term being processed.</p> <p> TYPE: <code>str</code> </p> <code>cell_value</code> <p>The value of the cell.</p> <p> TYPE: <code>Any</code> </p> <code>coordinate</code> <p>The coordinate of the cell in the sheet.</p> <p> TYPE: <code>str</code> </p> <code>sheet_title</code> <p>The title of the sheet.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>The processed value, either as a boolean or the original value after validation.</p> Source code in <code>bam_masterdata/excel/excel_to_entities.py</code> <pre><code>def process_term(\n    self, term: str, cell_value: Any, coordinate: str, sheet_title: str\n) -&gt; Any:\n    \"\"\"\n    Processes a term by converting it to a boolean if necessary or checking its validity.\n\n    Args:\n        term: The term being processed.\n        cell_value: The value of the cell.\n        coordinate: The coordinate of the cell in the sheet.\n        sheet_title: The title of the sheet.\n\n    Returns:\n        The processed value, either as a boolean or the original value after validation.\n    \"\"\"\n    # Check if the term is a boolean type\n    if term in (\"Mandatory\", \"Show in edit views\"):\n        return self.str_to_bool(\n            value=cell_value,\n            term=term,\n            coordinate=coordinate,\n            sheet_title=sheet_title,\n        )\n    # Check and validate the property\n    return self.get_and_check_property(\n        value=cell_value,\n        term=term,\n        coordinate=coordinate,\n        sheet_title=sheet_title,\n        is_code=(term in [\"Code\", \"Vocabulary code\"]),\n        is_data=(term == \"Data type\"),\n    )\n</code></pre>"},{"location":"references/api/#bam_masterdata.excel.excel_to_entities.MasterdataExcelExtractor.extract_value","title":"<code>extract_value(sheet, row, column, validation_pattern=None, is_description=False, is_data=False, is_url=False)</code>","text":"<p>Extracts and validates a value from a specified cell in the Excel sheet.</p> PARAMETER DESCRIPTION <code>sheet</code> <p>The worksheet object.</p> <p> TYPE: <code>Worksheet</code> </p> <code>row</code> <p>The row number of the cell (1-based index).</p> <p> TYPE: <code>int</code> </p> <code>column</code> <p>The column number of the cell (1-based index).</p> <p> TYPE: <code>int</code> </p> <code>validation_pattern</code> <p>Optional regex pattern to validate the cell value.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>is_description</code> <p>Flag indicating if the value is a description.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>is_data</code> <p>Flag indicating if the value is a data type.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>is_url</code> <p>Flag indicating if the value is a URL.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The extracted and validated cell value as a string. Returns an empty string if the value is invalid or not provided.</p> Source code in <code>bam_masterdata/excel/excel_to_entities.py</code> <pre><code>def extract_value(\n    self,\n    sheet: \"Worksheet\",\n    row: int,\n    column: int,\n    validation_pattern: str = None,\n    is_description: bool = False,\n    is_data: bool = False,\n    is_url: bool = False,\n) -&gt; str:\n    \"\"\"\n    Extracts and validates a value from a specified cell in the Excel sheet.\n\n    Args:\n        sheet: The worksheet object.\n        row: The row number of the cell (1-based index).\n        column: The column number of the cell (1-based index).\n        validation_pattern: Optional regex pattern to validate the cell value.\n        is_description: Flag indicating if the value is a description.\n        is_data: Flag indicating if the value is a data type.\n        is_url: Flag indicating if the value is a URL.\n\n    Returns:\n        The extracted and validated cell value as a string. Returns an empty string if the value is invalid or not provided.\n    \"\"\"\n    value = sheet.cell(row=row, column=column).value\n\n    # No `value` provided\n    if not value:\n        return \"\"\n\n    validated = (\n        bool(re.match(validation_pattern, str(value)))\n        if validation_pattern\n        else True\n    )\n    error_message = f\"Invalid value '{value}' at row {row}, column {column} in sheet {sheet.title}\"\n\n    if is_description:\n        error_message += \" Description should follow the schema: English Description + '//' + German Description.\"\n    elif is_data:\n        val_upper = str(value).upper()\n        allowed_types = [dt.value for dt in DataType]\n        is_valid_standard = val_upper in allowed_types\n        is_valid_dynamic = False\n        if not is_valid_standard and (\n            val_upper.startswith(\"SAMPLE:\") or val_upper.startswith(\"OBJECT:\")\n        ):\n            parts = val_upper.split(\":\", 1)\n            if len(parts) == 2 and parts[1].strip():\n                is_valid_dynamic = True\n        validated = is_valid_standard or is_valid_dynamic\n        error_message += f\" The Data Type should be one of the following: {allowed_types} or follow the format 'SAMPLE:&lt;CODE&gt;' or 'OBJECT:&lt;CODE&gt;'\"\n    elif is_url:\n        error_message += \" It should be an URL or empty\"\n\n    if not validated:\n        self.logger.error(\n            error_message,\n            cell_value=value,\n            sheet_title=sheet.title,\n            row=row,\n            column=column,\n        )\n\n    return value or \"\"\n</code></pre>"},{"location":"references/api/#bam_masterdata.excel.excel_to_entities.MasterdataExcelExtractor.process_entity","title":"<code>process_entity(sheet, start_index_row, header_terms, expected_terms, entity_type)</code>","text":"<p>Process an entity type block in the Excel sheet and return its attributes as a dictionary.</p> PARAMETER DESCRIPTION <code>sheet</code> <p>The worksheet object.</p> <p> TYPE: <code>Worksheet</code> </p> <code>start_index_row</code> <p>The row where the current entity type begins (1-based index).</p> <p> TYPE: <code>int</code> </p> <code>header_terms</code> <p>List of header terms in the entity block.</p> <p> TYPE: <code>list[str]</code> </p> <code>expected_terms</code> <p>List of expected terms to extract from the entity block.</p> <p> TYPE: <code>list[str]</code> </p> <code>entity_type</code> <p>The type of the entity (e.g., SAMPLE_TYPE, OBJECT_TYPE).</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>A dictionary containing the attributes of the entity.</p> Source code in <code>bam_masterdata/excel/excel_to_entities.py</code> <pre><code>def process_entity(\n    self,\n    sheet: \"Worksheet\",\n    start_index_row: int,\n    header_terms: list[str],\n    expected_terms: list[str],\n    entity_type: str,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Process an entity type block in the Excel sheet and return its attributes as a dictionary.\n\n    Args:\n        sheet: The worksheet object.\n        start_index_row: The row where the current entity type begins (1-based index).\n        header_terms: List of header terms in the entity block.\n        expected_terms: List of expected terms to extract from the entity block.\n        entity_type: The type of the entity (e.g., SAMPLE_TYPE, OBJECT_TYPE).\n\n    Returns:\n        A dictionary containing the attributes of the entity.\n    \"\"\"\n    attributes: dict = {}\n    cell_value: Any = \"\"\n\n    for term in expected_terms:\n        if term not in header_terms:\n            self.logger.error(f\"{term} not found in the headers.\", term=term)\n        else:\n            term_index = header_terms.index(term)\n            cell = sheet.cell(row=start_index_row + 2, column=term_index + 1)\n            cell_value = self.extract_value(\n                sheet,\n                start_index_row + 2,\n                term_index + 1,\n                self.VALIDATION_RULES[entity_type][term].get(\"pattern\"),\n            )\n\n            # Handle boolean conversion\n            if self.VALIDATION_RULES[entity_type][term].get(\"is_bool\"):\n                cell_value = self.str_to_bool(\n                    value=cell_value,\n                    term=term,\n                    coordinate=cell.coordinate,\n                    sheet_title=sheet.title,\n                )\n\n            # Handle data type validation\n            elif self.VALIDATION_RULES[entity_type][term].get(\"is_data\"):\n                allowed_types = [dt.value for dt in DataType]\n                cell_value_upper = str(cell_value).upper()\n                is_valid_standard = cell_value_upper in allowed_types\n                is_valid_dynamic = False\n                if not is_valid_standard and (\n                    cell_value_upper.startswith(\"SAMPLE:\")\n                    or cell_value_upper.startswith(\"OBJECT:\")\n                ):\n                    parts = cell_value_upper.split(\":\", 1)\n                    if len(parts) == 2 and parts[1].strip():\n                        is_valid_dynamic = True\n                if not is_valid_standard and not is_valid_dynamic:\n                    self.logger.error(\n                        f\"Invalid Data Type: {cell_value} in {cell.coordinate} (Sheet: {sheet.title}). Should be one of the following: {allowed_types} or follow the format 'SAMPLE:&lt;CODE&gt;' or 'OBJECT:&lt;CODE&gt;'\",\n                        term=term,\n                        cell_value=cell_value,\n                        cell_coordinate=cell.coordinate,\n                        sheet_title=sheet.title,\n                    )\n                else:\n                    cell_value = (\n                        cell_value_upper\n                        if isinstance(cell_value, str)\n                        else cell_value\n                    )\n\n            # Handle additional validation for \"Generated code prefix\"\n            elif (\n                self.VALIDATION_RULES[entity_type][term].get(\"extra_validation\")\n                == \"is_reduced_version\"\n            ):\n                if not is_reduced_version(cell_value, attributes.get(\"code\", \"\")):\n                    self.logger.warning(\n                        f\"Invalid {term} value '{cell_value}' in {cell.coordinate} (Sheet: {sheet.title}). \"\n                        f\"Generated code prefix should be part of the 'Code' {attributes.get('code', '')}.\",\n                        term=term,\n                        cell_value=cell_value,\n                        cell_coordinate=cell.coordinate,\n                        sheet_title=sheet.title,\n                    )\n\n            # Handle validation script (allows empty but must match pattern if provided)\n            elif (\n                self.VALIDATION_RULES[entity_type][term].get(\"allow_empty\")\n                and not cell_value\n            ):\n                cell_value = None\n\n            # Handle URL template validation (allows empty but must be a valid URL)\n            elif (\n                self.VALIDATION_RULES[entity_type][term].get(\"is_url\")\n                and cell_value\n            ):\n                url_pattern = self.VALIDATION_RULES[entity_type][term].get(\n                    \"pattern\"\n                )\n                if not re.match(url_pattern, str(cell_value)):\n                    self.logger.error(\n                        f\"Invalid URL format: {cell_value} in {cell.coordinate} (Sheet: {sheet.title})\",\n                        cell_value=cell_value,\n                        cell_coordinate=cell.coordinate,\n                        sheet_title=sheet.title,\n                    )\n\n            # Add the extracted value to the attributes dictionary\n            attributes[self.VALIDATION_RULES[entity_type][term].get(\"key\")] = (\n                cell_value\n            )\n\n    if self.row_cell_info:\n        attributes[\"row_location\"] = f\"A{start_index_row}\"\n    return attributes\n</code></pre>"},{"location":"references/api/#bam_masterdata.excel.excel_to_entities.MasterdataExcelExtractor.properties_to_dict","title":"<code>properties_to_dict(sheet, start_index_row, last_non_empty_row)</code>","text":"<p>Extracts properties from an Entity type block in the Excel sheet and returns them as a dictionary.</p> PARAMETER DESCRIPTION <code>sheet</code> <p>The worksheet object.</p> <p> TYPE: <code>Worksheet</code> </p> <code>start_index_row</code> <p>Row where the current entity type begins (1-based index).</p> <p> TYPE: <code>int</code> </p> <code>last_non_empty_row</code> <p>Row where the current entity type finish (1-based index).</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>dict[str, dict[str, Any]]</code> <p>A dictionary where each key is a property code and the value is a dictionary</p> <code>dict[str, dict[str, Any]]</code> <p>containing the attributes of the property.</p> Source code in <code>bam_masterdata/excel/excel_to_entities.py</code> <pre><code>def properties_to_dict(\n    self, sheet: \"Worksheet\", start_index_row: int, last_non_empty_row: int\n) -&gt; dict[str, dict[str, Any]]:\n    \"\"\"\n    Extracts properties from an Entity type block in the Excel sheet and returns them as a dictionary.\n\n    Args:\n        sheet: The worksheet object.\n        start_index_row: Row where the current entity type begins (1-based index).\n        last_non_empty_row: Row where the current entity type finish (1-based index).\n\n    Returns:\n        A dictionary where each key is a property code and the value is a dictionary\n        containing the attributes of the property.\n    \"\"\"\n    property_dict: dict = {}\n    expected_terms = [\n        \"Code\",\n        \"Description\",\n        \"Mandatory\",\n        \"Show in edit views\",\n        \"Section\",\n        \"Property label\",\n        \"Data type\",\n        \"Vocabulary code\",\n        \"Metadata\",\n        \"Dynamic script\",\n        # ! these are not used\n        # \"Unique\",\n        # \"Internal assignment\",\n    ]\n\n    # Determine the header row index\n    header_index = start_index_row + 3\n    row_headers = [(cell.value, cell.coordinate) for cell in sheet[header_index]]\n    # And store how many properties are for the entity\n    n_properties = last_non_empty_row - header_index\n    if n_properties &lt; 0:\n        self.logger.error(\n            f\"No properties found for the entity in sheet {sheet.title} starting at row {start_index_row}.\"\n        )\n        return property_dict\n\n    # Initialize a dictionary to store extracted columns\n    extracted_columns: dict[str, list] = {term: [] for term in expected_terms}\n    if self.row_cell_info:\n        extracted_columns[\"row_location\"] = []\n\n    # Extract columns for each expected term\n    for i, (term, coordinate) in enumerate(row_headers):\n        if term not in expected_terms:\n            log_func = (\n                self.logger.warning\n                if term\n                in (\n                    \"Mandatory\",\n                    \"Show in edit views\",\n                    \"Section\",\n                    \"Metadata\",\n                    \"Dynamic script\",\n                    \"Vocabulary code\",\n                    # ! these are not used\n                    # \"Unique\",\n                    # \"Internal assignment\",\n                )\n                else self.logger.error\n            )\n            log_func(f\"'{term}' not found in the properties headers.\", term=term)\n            continue\n\n        # Excel column letter from the coordinate\n        term_letter = coordinate[0]\n\n        # Extract values from the column\n        for cell_property in sheet[term_letter][header_index:last_non_empty_row]:\n            extracted_columns[term].append(\n                self.process_term(\n                    term, cell_property.value, cell_property.coordinate, sheet.title\n                )\n            )\n            if self.row_cell_info:\n                extracted_columns[\"row_location\"].append(cell_property.coordinate)\n\n    # Combine extracted values into a dictionary\n    for i in range(n_properties):\n        code = extracted_columns.get(\"Code\", [])\n        if not code:\n            self.logger.error(\n                f\"'Code' not found in the properties headers for sheet {sheet.title}.\"\n            )\n            return property_dict\n        code = code[i]\n        property_dict[code] = {\"permId\": code, \"code\": code}\n        for key, pybis_val in {\n            \"Description\": \"description\",\n            \"Section\": \"section\",\n            \"Mandatory\": \"mandatory\",\n            \"Show in edit views\": \"show_in_edit_views\",\n            \"Property label\": \"label\",\n            \"Data type\": \"dataType\",\n            \"Vocabulary code\": \"vocabularyCode\",\n        }.items():\n            data_column = extracted_columns.get(key, [])\n            if not data_column:\n                continue\n            cell_value = data_column[i]\n            if key == \"Data type\":\n                object_code = None\n                normalized_value = (\n                    str(cell_value).upper()\n                    if isinstance(cell_value, str)\n                    else cell_value\n                )\n                if isinstance(normalized_value, str) and \":\" in normalized_value:\n                    prefix, dynamic_code = normalized_value.split(\":\", 1)\n                    if prefix in (\"SAMPLE\", \"OBJECT\") and dynamic_code.strip():\n                        object_code = dynamic_code.strip()\n                        normalized_value = DataType.OBJECT.value\n                property_dict[code][pybis_val] = normalized_value\n                if object_code:\n                    property_dict[code][\"objectCode\"] = object_code\n            else:\n                property_dict[code][pybis_val] = cell_value\n        if self.row_cell_info:\n            property_dict[code][\"row_location\"] = (\n                extracted_columns.get(\"row_location\")[i],\n            )\n        # Only add optional fields if they exist in extracted_columns\n        optional_fields = [\n            \"Metadata\",\n            \"Dynamic script\",\n            \"Unique\",\n            \"Internal assignment\",\n        ]\n        for field in optional_fields:\n            if (\n                field in extracted_columns\n            ):  # Check if the field exists in the extracted columns\n                if extracted_columns[field][i] == \"\":\n                    extracted_columns[field][i] = None\n                property_dict[extracted_columns[\"Code\"][i]][\n                    field.lower().replace(\" \", \"_\")\n                ] = extracted_columns[field][i]\n\n    return property_dict\n</code></pre>"},{"location":"references/api/#bam_masterdata.excel.excel_to_entities.MasterdataExcelExtractor.terms_to_dict","title":"<code>terms_to_dict(sheet, start_index_row, last_non_empty_row)</code>","text":"<p>Extracts terms from a Vocabulary block in the Excel sheet and returns them as a dictionary.</p> PARAMETER DESCRIPTION <code>sheet</code> <p>The worksheet object.</p> <p> TYPE: <code>Worksheet</code> </p> <code>start_index_row</code> <p>Row where the current entity type begins (1-based index).</p> <p> TYPE: <code>int</code> </p> <code>last_non_empty_row</code> <p>Row where the current entity type finish (1-based index).</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>dict[str, dict[str, Any]]</code> <p>A dictionary where each key is a vocabulary term code and the value is a dictionary</p> <code>dict[str, dict[str, Any]]</code> <p>containing the attributes of the vocabulary term.</p> Source code in <code>bam_masterdata/excel/excel_to_entities.py</code> <pre><code>def terms_to_dict(\n    self, sheet: \"Worksheet\", start_index_row: int, last_non_empty_row: int\n) -&gt; dict[str, dict[str, Any]]:\n    \"\"\"\n    Extracts terms from a Vocabulary block in the Excel sheet and returns them as a dictionary.\n\n    Args:\n        sheet: The worksheet object.\n        start_index_row: Row where the current entity type begins (1-based index).\n        last_non_empty_row: Row where the current entity type finish (1-based index).\n\n    Returns:\n        A dictionary where each key is a vocabulary term code and the value is a dictionary\n        containing the attributes of the vocabulary term.\n    \"\"\"\n    terms_dict = {}\n    expected_terms = [\"Code\", \"Description\", \"Url template\", \"Label\", \"Official\"]\n\n    header_index = start_index_row + 3\n    row_headers = [cell.value for cell in sheet[header_index]]\n\n    # Initialize a dictionary to store extracted columns\n    extracted_columns: dict[str, list] = {term: [] for term in expected_terms}\n\n    # Helper function to process each term\n    def process_term_cell(term, cell_value, coordinate, sheet_title):\n        if term == \"Official\":\n            return self.str_to_bool(\n                value=cell_value,\n                term=term,\n                coordinate=coordinate,\n                sheet_title=sheet_title,\n            )\n        return self.get_and_check_property(\n            value=cell_value,\n            term=term,\n            coordinate=coordinate,\n            sheet_title=sheet_title,\n            is_code=(term == \"Code\"),\n            is_url=(term == \"Url template\"),\n        )\n\n    # Extract columns for each expected term\n    for term in expected_terms:\n        if term not in row_headers:\n            self.logger.warning(\n                f\"{term} not found in the properties headers.\", term=term\n            )\n            continue\n\n        # Get column index and Excel letter\n        term_index = row_headers.index(term) + 1\n        term_letter = self.index_to_excel_column(term_index)\n\n        # Extract values from the column\n        for cell in sheet[term_letter][header_index:last_non_empty_row]:\n            extracted_columns[term].append(\n                process_term_cell(term, cell.value, cell.coordinate, sheet.title)\n            )\n\n    if not extracted_columns.get(\"Code\"):\n        self.logger.error(\n            f\"The required 'Code' column for terms was not found in sheet {sheet.title}.\"\n        )\n        return {}\n\n    # Combine extracted values into a dictionary safely\n    for i in range(len(extracted_columns[\"Code\"])):\n        code = extracted_columns[\"Code\"][i]\n        terms_dict[code] = {\n            \"permId\": code,\n            \"code\": code,\n        }\n        for key, pybis_val in {\n            \"Description\": \"descriptions\",\n            \"Url template\": \"url_template\",\n            \"Label\": \"label\",\n            \"Official\": \"official\",\n        }.items():\n            values = extracted_columns.get(key, [])\n            if len(values) &gt; i:\n                terms_dict[code][pybis_val] = values[i]\n\n    return terms_dict\n</code></pre>"},{"location":"references/api/#bam_masterdata.excel.excel_to_entities.MasterdataExcelExtractor.block_to_entity_dict","title":"<code>block_to_entity_dict(sheet, start_index_row, last_non_empty_row, complete_dict)</code>","text":"<p>Extracts entity attributes from an Excel sheet block and returns them as a dictionary.</p> Source code in <code>bam_masterdata/excel/excel_to_entities.py</code> <pre><code>def block_to_entity_dict(\n    self,\n    sheet: \"Worksheet\",\n    start_index_row: int,\n    last_non_empty_row: int,\n    complete_dict: dict[str, Any],\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Extracts entity attributes from an Excel sheet block and returns them as a dictionary.\n    \"\"\"\n    attributes_dict: dict = {}\n\n    # Get the entity type\n    entity_type = sheet[f\"A{start_index_row}\"].value\n    if entity_type not in self.VALIDATION_RULES:\n        raise ValueError(f\"Invalid entity type: {entity_type}\")\n\n    # Get the header terms\n    header_terms = [cell.value for cell in sheet[start_index_row + 1]]\n\n    # Process entity data using the helper function\n    attributes_dict = self.process_entity(\n        sheet,\n        start_index_row,\n        header_terms,\n        list(self.VALIDATION_RULES[entity_type].keys()),\n        entity_type,\n    )\n\n    # Extract additional attributes if necessary\n    if entity_type in {\n        \"SAMPLE_TYPE\",\n        \"OBJECT_TYPE\",\n        \"EXPERIMENT_TYPE\",\n        \"DATASET_TYPE\",\n    }:\n        attributes_dict[\"properties\"] = (\n            self.properties_to_dict(sheet, start_index_row, last_non_empty_row)\n            or {}\n        )\n\n    elif entity_type == \"VOCABULARY_TYPE\":\n        attributes_dict[\"terms\"] = (\n            self.terms_to_dict(sheet, start_index_row, last_non_empty_row) or {}\n        )\n\n    # Add the entity to the complete dictionary\n    complete_dict[attributes_dict[\"code\"]] = attributes_dict\n\n    # Return sorted dictionary\n    return dict(sorted(complete_dict.items(), key=lambda item: item[0].count(\".\")))\n</code></pre>"},{"location":"references/api/#bam_masterdata.excel.excel_to_entities.MasterdataExcelExtractor.excel_to_entities","title":"<code>excel_to_entities()</code>","text":"<p>Extracts entities from an Excel file and returns them as a dictionary.</p> RETURNS DESCRIPTION <code>dict[str, dict[str, Any]]</code> <p>dict[str, dict[str, Any]]: A dictionary where each key is a normalized sheet name and the value is a dictionary</p> <code>dict[str, dict[str, Any]]</code> <p>containing the extracted entities. Returns an empty dictionary if all sheets are empty.</p> Source code in <code>bam_masterdata/excel/excel_to_entities.py</code> <pre><code>def excel_to_entities(self) -&gt; dict[str, dict[str, Any]]:\n    \"\"\"\n    Extracts entities from an Excel file and returns them as a dictionary.\n\n    Returns:\n        dict[str, dict[str, Any]]: A dictionary where each key is a normalized sheet name and the value is a dictionary\n        containing the extracted entities. Returns an empty dictionary if all sheets are empty.\n    \"\"\"\n    sheets_dict: dict[str, dict[str, Any]] = {}\n    sheet_names = self.workbook.sheetnames\n    has_content = False  # Track if any sheet has valid content\n\n    for i, sheet_name in enumerate(sheet_names):\n        normalized_sheet_name = sheet_name.lower().replace(\" \", \"_\")\n        sheet = self.workbook[sheet_name]\n        start_row = 1\n\n        # **Check if the sheet is empty**\n        if all(\n            sheet.cell(row=row, column=col).value in (None, \"\")\n            for row in range(1, sheet.max_row + 1)\n            for col in range(1, sheet.max_column + 1)\n        ):\n            self.logger.info(f\"Skipping empty sheet: {sheet_name}\")\n            continue  # Move to the next sheet\n\n        sheets_dict[normalized_sheet_name] = {}\n\n        consecutive_empty_rows = 0  # Track consecutive empty rows\n        while start_row &lt;= sheet.max_row:\n            # **Check for two consecutive empty rows**\n            is_row_empty = all(\n                sheet.cell(row=start_row, column=col).value in (None, \"\")\n                for col in range(1, sheet.max_column + 1)\n            )\n\n            if is_row_empty:\n                consecutive_empty_rows += 1\n                if consecutive_empty_rows &gt;= 2:\n                    # **Reached the end of the sheet, move to the next**\n                    if i == len(sheet_names) - 1:\n                        self.logger.info(\n                            f\"Last sheet {sheet_name} processed. End of the file reached.\"\n                        )\n                    else:\n                        self.logger.info(\n                            f\"End of the current sheet {sheet_name} reached. Switching to next sheet...\"\n                        )\n                    break  # Stop processing this sheet\n            else:\n                consecutive_empty_rows = 0  # Reset if we find a non-empty row\n\n                # **Process the entity block**\n                last_non_empty_row = self.get_last_non_empty_row(sheet, start_row)\n                if last_non_empty_row is None:\n                    break  # No more valid blocks\n\n                sheets_dict[normalized_sheet_name] = self.block_to_entity_dict(\n                    sheet,\n                    start_row,\n                    last_non_empty_row,\n                    sheets_dict[normalized_sheet_name],\n                )\n                has_content = True  # Found valid content\n\n                # Move to the next entity block\n                start_row = last_non_empty_row + 1\n                continue  # Continue loop without increasing consecutive_empty_rows\n\n            start_row += 1  # Move to the next row\n\n    # **If no sheets had content, return an empty dictionary**\n    if not has_content:\n        self.logger.warning(\n            \"No valid data found in any sheets. Returning empty dictionary.\"\n        )\n        return {}\n\n    return sheets_dict\n</code></pre>"},{"location":"references/api/#bam_masterdata.cli.entities_to_excel","title":"<code>bam_masterdata.cli.entities_to_excel</code>","text":""},{"location":"references/api/#bam_masterdata.cli.entities_to_excel.entities_to_excel","title":"<code>entities_to_excel(worksheet, module_path, definitions_module)</code>","text":"<p>Export entities to the Excel file. The Python modules are imported using the function <code>import_module</code>, and their contents are inspected (using <code>inspect</code>) to find the classes in the datamodel containing <code>defs</code> and with a <code>model_to_json</code> method defined. Each row is then appended to the <code>worksheet</code>.</p> PARAMETER DESCRIPTION <code>worksheet</code> <p>The worksheet to append the entities.</p> <p> TYPE: <code>Worksheet</code> </p> <code>module_path</code> <p>Path to the Python module file.</p> <p> TYPE: <code>str</code> </p> <code>definitions_module</code> <p>The module containing the definitions of the entities. This is used to match the header definitions of the entities.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>bam_masterdata/cli/entities_to_excel.py</code> <pre><code>def entities_to_excel(\n    worksheet: \"Worksheet\",\n    module_path: str,\n    definitions_module: Any,\n) -&gt; None:\n    \"\"\"\n    Export entities to the Excel file. The Python modules are imported using the function `import_module`,\n    and their contents are inspected (using `inspect`) to find the classes in the datamodel containing\n    `defs` and with a `model_to_json` method defined. Each row is then appended to the `worksheet`.\n\n    Args:\n        worksheet (Worksheet): The worksheet to append the entities.\n        module_path (str): Path to the Python module file.\n        definitions_module (Any): The module containing the definitions of the entities. This is used\n            to match the header definitions of the entities.\n    \"\"\"\n    def_members = inspect.getmembers(definitions_module, inspect.isclass)\n    module = import_module(module_path=module_path)\n\n    # Inspect Python modules and their objects and print them to Excel\n    for _, obj in inspect.getmembers(module, inspect.isclass):\n        # Ensure the class has the `model_to_json` method\n        if not hasattr(obj, \"defs\") or not callable(getattr(obj, \"model_to_json\")):\n            continue\n\n        obj_instance = obj()\n\n        # Entity title\n        obj_definitions = obj_instance.defs\n        worksheet.append([obj_definitions.excel_name])\n\n        # Entity header definitions and values\n        for def_name, def_cls in def_members:\n            if def_name == obj_definitions.name:\n                break\n        # Appending headers and values in worksheet\n        excel_headers = []\n        header_values = []\n        for field, excel_header in obj_definitions.excel_headers_map.items():\n            header_values.append(getattr(obj_definitions, field))\n            excel_headers.append(excel_header)\n        worksheet.append(excel_headers)\n        worksheet.append(header_values)\n\n        # Properties assignment for ObjectType, DatasetType, and CollectionType\n        if obj_instance.base_name in [\"ObjectType\", \"DatasetType\", \"CollectionType\"]:\n            if not obj_instance.properties:\n                continue\n            worksheet.append(\n                list(obj_instance.properties[0].excel_headers_map.values())\n            )\n            for prop in obj_instance.properties:\n                row = []\n                for field in prop.excel_headers_map.keys():\n                    if field == \"data_type\":\n                        if prop.data_type == DataType.OBJECT and getattr(\n                            prop, \"object_code\", None\n                        ):\n                            val = f\"SAMPLE:{prop.object_code}\"\n                        else:\n                            val = prop.data_type.value\n                    else:\n                        val = getattr(prop, field)\n                    row.append(val)\n            worksheet.append(row)\n        # Terms assignment for VocabularyType\n        elif obj_instance.base_name == \"VocabularyType\":\n            if not obj_instance.terms:\n                continue\n            worksheet.append(list(obj_instance.terms[0].excel_headers_map.values()))\n            for term in obj_instance.terms:\n                worksheet.append(\n                    getattr(term, f_set) for f_set in term.excel_headers_map.keys()\n                )\n        worksheet.append([\"\"])  # empty row after entity definitions\n</code></pre>"},{"location":"references/api/#bam_masterdata.cli.entities_to_rdf","title":"<code>bam_masterdata.cli.entities_to_rdf</code>","text":""},{"location":"references/api/#bam_masterdata.cli.entities_to_rdf.BAM","title":"<code>BAM = Namespace('https://bamresearch.github.io/bam-masterdata/#')</code>","text":""},{"location":"references/api/#bam_masterdata.cli.entities_to_rdf.PROV","title":"<code>PROV = Namespace('http://www.w3.org/ns/prov#')</code>","text":""},{"location":"references/api/#bam_masterdata.cli.entities_to_rdf.rdf_graph_init","title":"<code>rdf_graph_init(g)</code>","text":"<p>Initialize the RDF graph with base namespaces, annotation properties, and internal BAM properties. This function also creates placeholders for PropertyType and other entity types. The graph is to be printed out in RDF/XML format in the <code>entities_to_rdf</code> function.</p> PARAMETER DESCRIPTION <code>g</code> <p>The RDF graph to be initialized.</p> <p> TYPE: <code>Graph</code> </p> Source code in <code>bam_masterdata/cli/entities_to_rdf.py</code> <pre><code>def rdf_graph_init(g: \"Graph\") -&gt; None:\n    \"\"\"\n    Initialize the RDF graph with base namespaces, annotation properties, and internal BAM properties. This\n    function also creates placeholders for PropertyType and other entity types. The graph is to be printed out\n    in RDF/XML format in the `entities_to_rdf` function.\n\n    Args:\n        g (Graph): The RDF graph to be initialized.\n    \"\"\"\n    # Adding base namespaces\n    g.bind(\"dc\", DC)\n    g.bind(\"owl\", OWL)\n    g.bind(\"rdf\", RDF)\n    g.bind(\"rdfs\", RDFS)\n    g.bind(\"bam\", BAM)\n    g.bind(\"prov\", PROV)\n\n    # Adding annotation properties from base namespaces\n    annotation_props = [\n        RDFS.label,\n        RDFS.comment,\n        DC.identifier,\n    ]\n    for prop in annotation_props:\n        g.add((prop, RDF.type, OWL.AnnotationProperty))\n\n    # Custom annotation properties from openBIS: `dataType`, `propertyLabel\n    custom_annotation_props = {\n        BAM[\n            \"dataType\"\n        ]: \"\"\"Represents the data type of a property as defined in the openBIS platform.\n        This annotation is used to ensure alignment with the native data types in openBIS,\n        facilitating seamless integration and data exchange.\n\n        The allowed values for this annotation correspond directly to the openBIS type system,\n        including BOOLEAN, CONTROLLEDVOCABULARY, DATE, HYPERLINK, INTEGER, MULTILINE_VARCHAR, OBJECT,\n        REAL, TIMESTAMP, VARCHAR, and XML.\n\n        While `bam:dataType` is primarily intended for internal usage with openBIS, mappings to\n        standard vocabularies such as `xsd` (e.g., `xsd:boolean`, `xsd:string`) are possible to use and documented to\n        enhance external interoperability. The full mapping is:\n        - BOOLEAN: xsd:boolean\n        - CONTROLLEDVOCABULARY: xsd:string\n        - DATE: xsd:date\n        - HYPERLINK: xsd:anyURI\n        - INTEGER: xsd:integer\n        - MULTILINE_VARCHAR: xsd:string\n        - OBJECT: bam:ObjectType\n        - REAL: xsd:decimal\n        - TIMESTAMP: xsd:dateTime\n        - VARCHAR: xsd:string\n        - XML: xsd:string\"\"\",\n        BAM[\n            \"propertyLabel\"\n        ]: \"\"\"A UI-specific annotation used in openBIS to provide an alternative label for a property\n        displayed in the frontend. Not intended for semantic reasoning or interoperability beyond openBIS.\"\"\",\n    }\n    for custom_prop, custom_prop_def in custom_annotation_props.items():\n        g.add((custom_prop, RDF.type, OWL.AnnotationProperty))\n        g.add(\n            (\n                custom_prop,\n                RDFS.label,\n                Literal(f\"bam:{custom_prop.split('/')[-1]}\", lang=\"en\"),\n            )\n        )\n        g.add((custom_prop, RDFS.comment, Literal(custom_prop_def, lang=\"en\")))\n\n    # Internal BAM properties\n    # ? `section`, `ordinal`, `show_in_edit_views`?\n    bam_props_uri = {\n        BAM[\"hasMandatoryProperty\"]: [\n            (RDF.type, OWL.ObjectProperty),\n            # (RDFS.domain, OWL.Class),\n            (RDFS.range, BAM.PropertyType),\n            (RDFS.label, Literal(\"hasMandatoryProperty\", lang=\"en\")),\n            (\n                RDFS.comment,\n                Literal(\n                    \"The property must be mandatorily filled when creating the object in openBIS.\",\n                    lang=\"en\",\n                ),\n            ),\n        ],\n        BAM[\"hasOptionalProperty\"]: [\n            (RDF.type, OWL.ObjectProperty),\n            # (RDFS.domain, OWL.Class),\n            (RDFS.range, BAM.PropertyType),\n            (RDFS.label, Literal(\"hasOptionalProperty\", lang=\"en\")),\n            (\n                RDFS.comment,\n                Literal(\n                    \"The property is optionally filled when creating the object in openBIS.\",\n                    lang=\"en\",\n                ),\n            ),\n        ],\n        BAM[\"referenceTo\"]: [\n            (RDF.type, OWL.ObjectProperty),\n            (RDFS.domain, BAM.PropertyType),  # Restricting domain to PropertyType\n            # (RDFS.range, OWL.Class),  # Explicitly setting range to ObjectType\n            (RDFS.label, Literal(\"referenceTo\", lang=\"en\")),\n            (\n                RDFS.comment,\n                Literal(\n                    \"The property is referencing an object existing in openBIS.\",\n                    lang=\"en\",\n                ),\n            ),\n        ],\n    }\n    for prop_uri, obj_properties in bam_props_uri.items():\n        for prop in obj_properties:  # type: ignore\n            g.add((prop_uri, prop[0], prop[1]))  # type: ignore\n\n    # Adding base PropertyType and other objects as placeholders\n    # ! add only PropertyType\n    prop_type_description = \"\"\"A conceptual placeholder used to define and organize properties as first-class entities.\n        PropertyType is used to place properties and define their metadata, separating properties from the\n        entities they describe.\n\n        In integration scenarios:\n        - PropertyType can align with `BFO:Quality` for inherent attributes.\n        - PropertyType can represent `BFO:Role` if properties serve functional purposes.\n        - PropertyType can be treated as a `prov:Entity` when properties participate in provenance relationships.\"\"\"\n    for entity in [\"PropertyType\", \"ObjectType\", \"CollectionType\", \"DatasetType\"]:\n        entity_uri = BAM[entity]\n        g.add((entity_uri, RDF.type, OWL.Thing))\n        g.add((entity_uri, RDFS.label, Literal(entity, lang=\"en\")))\n        if entity == \"PropertyType\":\n            g.add((entity_uri, RDFS.comment, Literal(prop_type_description, lang=\"en\")))\n</code></pre>"},{"location":"references/api/#bam_masterdata.cli.entities_to_rdf.entities_to_rdf","title":"<code>entities_to_rdf(graph, module_path, logger)</code>","text":"<p>Convert the entities defined in the specified module to RDF triples and add them to the graph. The function uses the <code>model_to_rdf</code> method defined in each class to convert the class attributes to RDF triples. The function also adds the PropertyType and other entity types as placeholders in the graph.</p> PARAMETER DESCRIPTION <code>graph</code> <p>The RDF graph to which the entities are added.</p> <p> TYPE: <code>Graph</code> </p> <code>module_path</code> <p>The path to the module containing the entities to be converted.</p> <p> TYPE: <code>str</code> </p> <code>logger</code> <p>The logger to log messages.</p> <p> TYPE: <code>BoundLoggerLazyProxy</code> </p> Source code in <code>bam_masterdata/cli/entities_to_rdf.py</code> <pre><code>def entities_to_rdf(\n    graph: \"Graph\", module_path: str, logger: \"BoundLoggerLazyProxy\"\n) -&gt; None:\n    \"\"\"\n    Convert the entities defined in the specified module to RDF triples and add them to the graph. The function\n    uses the `model_to_rdf` method defined in each class to convert the class attributes to RDF triples. The\n    function also adds the PropertyType and other entity types as placeholders in the graph.\n\n    Args:\n        graph (Graph): The RDF graph to which the entities are added.\n        module_path (str): The path to the module containing the entities to be converted.\n        logger (BoundLoggerLazyProxy): The logger to log messages.\n    \"\"\"\n    rdf_graph_init(graph)\n\n    module = import_module(module_path=module_path)\n\n    # All datamodel modules\n    # OBJECT/DATASET/COLLECTION TYPES\n    # skos:prefLabel used for class names\n    # skos:definition used for `description` (en, de)\n    # dc:identifier used for `code`  # ! only defined for internal codes with $ symbol\n    # parents defined from `code`\n    # assigned properties can be Mandatory or Optional, can be PropertyType or ObjectType\n    # ? For OBJECT TYPES\n    # ? `generated_code_prefix`, `auto_generate_codes`?\n    for name, obj in inspect.getmembers(module, inspect.isclass):\n        # Ensure the class has the `model_to_rdf` method\n        if not hasattr(obj, \"defs\") or not callable(getattr(obj, \"model_to_rdf\")):\n            continue\n        try:\n            # Instantiate the class and call the method\n            entity = obj()\n            entity.model_to_rdf(namespace=BAM, graph=graph, logger=logger)\n        except Exception as err:\n            click.echo(f\"Failed to process class {name} in {module_path}: {err}\")\n</code></pre>"},{"location":"references/api/#bam_masterdata.cli.run_parser","title":"<code>bam_masterdata.cli.run_parser</code>","text":""},{"location":"references/api/#bam_masterdata.cli.run_parser.run_parser","title":"<code>run_parser(openbis=None, space_name='', project_name='PROJECT', collection_name='', files_parser={}, collection_type='COLLECTION')</code>","text":"<p>Run the parsers on the specified files and collect the results. login with save_token=True don't forget!!</p> PARAMETER DESCRIPTION <code>openbis</code> <p>An instance of the Openbis class from pyBIS, already logged in.</p> <p> TYPE: <code>Openbis</code> DEFAULT: <code>None</code> </p> <code>space_name</code> <p>The space in openBIS where the entities will be stored.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>project_name</code> <p>The project in openBIS where the entities will be stored.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'PROJECT'</code> </p> <code>collection_name</code> <p>The collection in openBIS where the entities will be stored.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>files_parser</code> <p>A dictionary where keys are parser instances and values are lists of file paths to be parsed. E.g., {MasterdataParserExample(): [\"path/to/file.json\", \"path/to/another_file.json\"]}</p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> <code>collection_type</code> <p>The type of collection to create in openBIS. Options are \"COLLECTION\" or \"DEFAULT_EXPERIMENT\". Defaults to \"COLLECTION\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'COLLECTION'</code> </p> Source code in <code>bam_masterdata/cli/run_parser.py</code> <pre><code>def run_parser(\n    openbis: Openbis | None = None,\n    space_name: str = \"\",\n    project_name: str = \"PROJECT\",\n    collection_name: str = \"\",\n    files_parser: dict[AbstractParser, list[str]] = {},\n    collection_type: str = \"COLLECTION\",\n) -&gt; None:\n    \"\"\"\n    Run the parsers on the specified files and collect the results.\n    login with save_token=True don't forget!!\n\n    Args:\n        openbis (Openbis): An instance of the Openbis class from pyBIS, already logged in.\n        space_name (str): The space in openBIS where the entities will be stored.\n        project_name (str): The project in openBIS where the entities will be stored.\n        collection_name (str): The collection in openBIS where the entities will be stored.\n        files_parser (dict): A dictionary where keys are parser instances and values are lists of file paths to be parsed. E.g., {MasterdataParserExample(): [\"path/to/file.json\", \"path/to/another_file.json\"]}\n        collection_type (str): The type of collection to create in openBIS. Options are \"COLLECTION\" or \"DEFAULT_EXPERIMENT\". Defaults to \"COLLECTION\".\n    \"\"\"\n    # Ensure openbis is provided\n    if openbis is None:\n        logger.error(\"An instance of Openbis must be provided for the parser to run.\")\n        return\n    # Ensure the space, project, and collection are set\n    if not project_name:\n        logger.error(\"The Project name must be specified for the parser to run.\")\n        return\n    # Ensure the files_parser is not empty\n    if not files_parser:\n        logger.error(\n            \"No files or parsers to parse. Please provide valid file paths or contact an Admin to add missing parser.\"\n        )\n        return\n    # Ensure collection_type is valid\n    if collection_type not in [\"COLLECTION\", \"DEFAULT_EXPERIMENT\"]:\n        logger.error(\n            f\"Invalid collection_type '{collection_type}'. Must be either 'COLLECTION' or 'DEFAULT_EXPERIMENT'.\"\n        )\n        return\n\n    # Specify the space\n    try:\n        space = openbis.get_space(space_name)\n    except Exception:\n        space = None\n    # If space is not found, use the user space\n    if space is None:\n        # user name as default space\n        for s in openbis.get_spaces():\n            if s.code.endswith(openbis.username.upper()):\n                space = s\n                logger.warning(\n                    f\"Space {space_name} does not exist in openBIS. \"\n                    f\"Loading space for {openbis.username}.\"\n                )\n                break\n        # no space found\n        if space is None:\n            logger.error(\n                f\"No usable Space for {openbis.username} in openBIS. Please create it first or notify an Admin.\"\n            )\n            return\n\n    # Get project if `project_name` already exists under the space or create a new one if it does not\n    if project_name.upper() in [p.code for p in space.get_projects()]:\n        project = space.get_project(project_name)\n    else:\n        logger.info(\"Replacing project code with uppercase and underscores.\")\n        project = space.new_project(\n            code=project_name.replace(\" \", \"_\").upper(),\n            description=\"New project created via automated parsing with `bam_masterdata`.\",\n        )\n    project.save()\n\n    # Create a new pybis `COLLECTION` to store the generated objects\n    if not collection_name:\n        logger.info(\n            \"No Collection name specified. Attaching objects directly to Project.\"\n        )\n        collection_openbis = project\n    else:\n        if collection_name.upper() in [c.code for c in project.get_collections()]:\n            collection_openbis = space.get_collection(\n                f\"/{space_name}/{project_name}/{collection_name}\".upper()\n            )\n        else:\n            logger.info(\"Replacing collection code with uppercase and underscores.\")\n            collection_openbis = openbis.new_collection(\n                code=collection_name.replace(\" \", \"_\").upper(),\n                type=collection_type,\n                project=project,\n            )\n        collection_openbis.save()\n\n    # Create a bam_masterdata CollectionType instance for storing parsed results\n    collection = CollectionType()\n    # Iterate over each parser and its associated files and store them in `collection`\n    for parser, files in files_parser.items():\n        parser.parse(files, collection, logger=logger)\n\n    # Map the objects added to CollectionType to objects in openBIS using pyBIS\n    openbis_id_map = {}\n    for object_id, object_instance in collection.attached_objects.items():\n        # Map PropertyTypeAssignment to pybis props dictionary\n        obj_props = {}\n        for key in object_instance._properties.keys():\n            value = getattr(object_instance, key, None)\n            if value is None or isinstance(value, PropertyTypeAssignment):\n                continue\n\n            # Handle OBJECT data type properties\n            property_metadata = object_instance._property_metadata[key]\n            if property_metadata.data_type == \"OBJECT\":\n                if isinstance(value, str):\n                    # Value is a path string, verify it exists in openBIS\n                    try:\n                        referenced_object = openbis.get_object(value)\n                        # Use the identifier from the fetched object\n                        obj_props[property_metadata.code.lower()] = (\n                            referenced_object.identifier\n                        )\n                    except Exception as e:\n                        logger.error(\n                            f\"Failed to resolve OBJECT reference '{value}' for property '{key}': {e}\"\n                        )\n                        continue\n                elif isinstance(value, ObjectType):\n                    # Value is an ObjectType instance, construct the path\n                    if not value.code:\n                        logger.warning(\n                            f\"OBJECT reference for property '{key}' has no code, skipping\"\n                        )\n                        continue\n                    # Construct the identifier path\n                    # Try to find this object in the openbis_id_map first (if it's being created in the same batch)\n                    referenced_identifier = None\n                    for obj_id, obj_inst in collection.attached_objects.items():\n                        if obj_inst is value and obj_id in openbis_id_map:\n                            referenced_identifier = openbis_id_map[obj_id]\n                            break\n\n                    if not referenced_identifier:\n                        # Construct identifier from the object's code\n                        # Assume it's in the same space/project as the current object\n                        if not collection_name:\n                            referenced_identifier = (\n                                f\"/{space_name}/{project_name}/{value.code}\"\n                            )\n                        else:\n                            referenced_identifier = f\"/{space_name}/{project_name}/{collection_name}/{value.code}\"\n\n                    obj_props[property_metadata.code.lower()] = referenced_identifier\n                else:\n                    # Unexpected type, skip\n                    logger.warning(\n                        f\"Unexpected type for OBJECT property '{key}': {type(value).__name__}\"\n                    )\n                    continue\n            else:\n                # Not an OBJECT type, handle normally\n                obj_props[property_metadata.code.lower()] = value\n\n        # Check if object already exists in openBIS, and if so, notify and get for updating properties\n        if not object_instance.code:\n            if not collection_name:\n                object_openbis = openbis.new_object(\n                    type=object_instance.defs.code,\n                    space=space,\n                    project=project,\n                    props=obj_props,\n                )\n            else:\n                object_openbis = openbis.new_object(\n                    type=object_instance.defs.code,\n                    space=space,\n                    project=project,\n                    collection=collection_openbis,\n                    props=obj_props,\n                )\n            object_openbis.save()\n        else:\n            identifier = (\n                f\"/{space_name}/{project_name}/{object_instance.code}\"\n                if not collection_name\n                else f\"/{space_name}/{project_name}/{collection_name}/{object_instance.code}\"\n            )\n            try:\n                object_openbis = space.get_object(identifier)\n                object_openbis.set_props(obj_props)  # update properties\n            except Exception:\n                logger.info(\n                    f\"Object with code {object_instance.code} does not exist in openBIS, creating new one.\"\n                )\n                if not collection_name:\n                    object_openbis = openbis.new_object(\n                        type=object_instance.defs.code,\n                        code=object_instance.code,\n                        space=space,\n                        project=project,\n                        props=obj_props,\n                    )\n                else:\n                    object_openbis = openbis.new_object(\n                        type=object_instance.defs.code,\n                        code=object_instance.code,\n                        space=space,\n                        project=project,\n                        collection=collection_openbis,\n                        props=obj_props,\n                    )\n            object_openbis.save()\n            logger.info(\n                f\"Object {identifier} already exists in openBIS, updating properties.\"\n            )\n\n        # save local and openbis IDs to map parent-child relationships\n        openbis_id_map[object_id] = object_openbis.identifier\n\n    # Storing files as datasets in openBIS\n    for files in files_parser.values():\n        try:\n            if not collection_name:\n                # ! This won't work on a project -&gt; datasets only attached to collections in pyBIS\n                dataset = openbis.new_dataset(\n                    type=\"RAW_DATA\",\n                    files=files,\n                    project=project,\n                )\n            else:\n                dataset = openbis.new_dataset(\n                    type=\"RAW_DATA\",\n                    files=files,\n                    collection=collection_openbis,\n                )\n            dataset.save()\n        except Exception as e:\n            logger.warning(f\"Error uploading files {files} to openBIS: {e}\")\n            continue\n        logger.info(f\"Files uploaded to openBIS collection {collection_name}.\")\n\n    # Map parent-child relationships\n    for parent_id, child_id in collection.relationships.values():\n        if parent_id in openbis_id_map and child_id in openbis_id_map:\n            parent_openbis_id = openbis_id_map[parent_id]\n            child_openbis_id = openbis_id_map[child_id]\n\n            child_openbis = openbis.get_object(child_openbis_id)\n            child_openbis.add_parents(parent_openbis_id)\n            child_openbis.save()\n\n            logger.info(\n                f\"Linked child {child_openbis_id} to parent {parent_openbis_id} in collection {collection_name}.\"\n            )\n</code></pre>"},{"location":"references/api/#bam_masterdata.excel.excel_to_entities","title":"<code>bam_masterdata.excel.excel_to_entities</code>","text":""},{"location":"references/api/#bam_masterdata.excel.excel_to_entities.MasterdataExcelExtractor","title":"<code>MasterdataExcelExtractor</code>","text":"Source code in <code>bam_masterdata/excel/excel_to_entities.py</code> <pre><code>class MasterdataExcelExtractor:\n    # TODO move these validation rules to a separate json\n    VALIDATION_RULES: dict[str, dict[str, dict[str, Any]]] = {}\n\n    def __init__(self, excel_path: str, **kwargs):\n        \"\"\"Initialize the MasterdataExtractor.\"\"\"\n        self.excel_path = excel_path\n        self.row_cell_info = kwargs.get(\"row_cell_info\", False)\n        self.workbook = openpyxl.load_workbook(excel_path)\n        self.logger = kwargs.get(\"logger\", logger)\n\n        # Load validation rules at initialization\n        if not MasterdataExcelExtractor.VALIDATION_RULES:\n            self.VALIDATION_RULES = load_validation_rules(\n                self.logger,\n                os.path.join(VALIDATION_RULES_DIR, \"excel_validation_rules.json\"),\n            )\n\n    def index_to_excel_column(self, index: int) -&gt; str:\n        \"\"\"\n        Converts a 1-based index to an Excel column name.\n\n        Args:\n            index: The 1-based index to convert.\n\n        Returns:\n            The corresponding Excel column name.\n        \"\"\"\n        if not index &gt;= 1:\n            raise ValueError(\"Index must be a positive integer starting from 1.\")\n\n        column = \"\"\n        while index &gt; 0:\n            index, remainder = divmod(index - 1, 26)\n            column = chr(65 + remainder) + column\n        return column\n\n    def get_last_non_empty_row(\n        self, sheet: \"Worksheet\", start_index: int\n    ) -&gt; int | None:\n        \"\"\"\n        Finds the last non-empty row before encountering a completely empty row.\n\n        Args:\n            sheet: The worksheet object.\n            start_index: The row number to start checking from (1-based index).\n\n        Returns:\n            The row number of the last non-empty row before an empty row is encountered,\n            or None if no non-empty rows are found starting from the given index.\n        \"\"\"\n        if start_index &lt; 1 or start_index &gt; sheet.max_row:\n            raise ValueError(\n                f\"Invalid start index: {start_index}. It must be between 1 and {sheet.max_row}.\"\n            )\n\n        last_non_empty_row = None\n        for row in range(start_index, sheet.max_row + 1):\n            if all(\n                sheet.cell(row=row, column=col).value in (None, \"\")\n                for col in range(1, sheet.max_column + 1)\n            ):\n                return last_non_empty_row  # Return the last non-empty row before the current empty row\n\n            last_non_empty_row = row  # Update the last non-empty row\n\n        return last_non_empty_row  # If no empty row is encountered, return the last non-empty row\n\n    def str_to_bool(\n        self,\n        value: str | bool | None,\n        term: str,\n        coordinate: str,\n        sheet_title: str,\n    ) -&gt; bool:\n        \"\"\"\n        Converts a string to a boolean value.\n\n        Args:\n            value: The string to convert.\n\n        Returns:\n            The boolean value.\n        \"\"\"\n        # No `value` provided\n        if not value:\n            return False\n\n        val = str(value).strip().lower()\n        if val not in [\"true\", \"false\"]:\n            self.logger.error(\n                f\"Invalid {term.lower()} value found in the {term} column at position {coordinate} in {sheet_title}. Accepted values: TRUE or FALSE.\",\n                term=term,\n                cell_value=val,\n                cell_coordinate=coordinate,\n                sheet_title=sheet_title,\n            )\n        return val == \"true\"\n\n    def get_and_check_property(\n        self,\n        value: str | bool | None,\n        term: str,\n        coordinate: str,\n        sheet_title: str,\n        is_description: bool = False,\n        is_code: bool = False,\n        is_data: bool = False,\n        is_url: bool = False,\n    ) -&gt; str:\n        \"\"\"\n        Gets a property and checks its format.\n\n        Args:\n            value: The string to convert.\n\n        Returns:\n            The property.\n        \"\"\"\n\n        # No `value` provided\n        if not value:\n            return \"\"\n\n        val = str(value)\n        error_message = f\"Invalid {term.lower()} value found in the {term} column at position {coordinate} in {sheet_title}.\"\n        if is_description:\n            if not re.match(r\".*//.*\", val):\n                self.logger.error(\n                    error_message\n                    + \"Description should follow the schema: English Description + '//' + German Description. \",\n                    term=term,\n                    cell_value=val,\n                    cell_coordinate=coordinate,\n                    sheet_title=sheet_title,\n                )\n        elif is_code:\n            if not re.match(r\"^\\$?[A-Z0-9_.]+$\", val):\n                self.logger.error(\n                    error_message,\n                    term=term,\n                    cell_value=val,\n                    cell_coordinate=coordinate,\n                    sheet_title=sheet_title,\n                )\n        elif is_data:\n            # Normalize data type to uppercase and allow dynamic SAMPLE/OBJECT codes\n            val_upper = val.upper()\n            allowed_types = [dt.value for dt in DataType]\n            is_valid_standard = val_upper in allowed_types\n            is_valid_dynamic = False\n\n            if not is_valid_standard and (\n                val_upper.startswith(\"SAMPLE:\") or val_upper.startswith(\"OBJECT:\")\n            ):\n                parts = val_upper.split(\":\", 1)\n                if len(parts) == 2 and parts[1].strip():\n                    is_valid_dynamic = True\n\n            if not is_valid_standard and not is_valid_dynamic:\n                self.logger.error(\n                    error_message\n                    + f\"The Data Type should be one of the following: {allowed_types} or follow the format 'SAMPLE:&lt;CODE&gt;' or 'OBJECT:&lt;CODE&gt;'\",\n                    term=term,\n                    cell_value=val_upper,\n                    cell_coordinate=coordinate,\n                    sheet_title=sheet_title,\n                )\n            val = val_upper\n        elif is_url:\n            if not re.match(\n                r\"https?://(?:www\\.)?[a-zA-Z0-9-._~:/?#@!$&amp;'()*+,;=%]+\", val\n            ):\n                self.logger.error(\n                    error_message,\n                    term=term,\n                    cell_value=val,\n                    cell_coordinate=coordinate,\n                    sheet_title=sheet_title,\n                )\n        else:\n            if not re.match(r\".*\", val):\n                self.logger.error(\n                    error_message,\n                    term=term,\n                    cell_value=val,\n                    cell_coordinate=coordinate,\n                    sheet_title=sheet_title,\n                )\n        return val\n\n    # Helper function to process each term\n    def process_term(\n        self, term: str, cell_value: Any, coordinate: str, sheet_title: str\n    ) -&gt; Any:\n        \"\"\"\n        Processes a term by converting it to a boolean if necessary or checking its validity.\n\n        Args:\n            term: The term being processed.\n            cell_value: The value of the cell.\n            coordinate: The coordinate of the cell in the sheet.\n            sheet_title: The title of the sheet.\n\n        Returns:\n            The processed value, either as a boolean or the original value after validation.\n        \"\"\"\n        # Check if the term is a boolean type\n        if term in (\"Mandatory\", \"Show in edit views\"):\n            return self.str_to_bool(\n                value=cell_value,\n                term=term,\n                coordinate=coordinate,\n                sheet_title=sheet_title,\n            )\n        # Check and validate the property\n        return self.get_and_check_property(\n            value=cell_value,\n            term=term,\n            coordinate=coordinate,\n            sheet_title=sheet_title,\n            is_code=(term in [\"Code\", \"Vocabulary code\"]),\n            is_data=(term == \"Data type\"),\n        )\n\n    def extract_value(\n        self,\n        sheet: \"Worksheet\",\n        row: int,\n        column: int,\n        validation_pattern: str = None,\n        is_description: bool = False,\n        is_data: bool = False,\n        is_url: bool = False,\n    ) -&gt; str:\n        \"\"\"\n        Extracts and validates a value from a specified cell in the Excel sheet.\n\n        Args:\n            sheet: The worksheet object.\n            row: The row number of the cell (1-based index).\n            column: The column number of the cell (1-based index).\n            validation_pattern: Optional regex pattern to validate the cell value.\n            is_description: Flag indicating if the value is a description.\n            is_data: Flag indicating if the value is a data type.\n            is_url: Flag indicating if the value is a URL.\n\n        Returns:\n            The extracted and validated cell value as a string. Returns an empty string if the value is invalid or not provided.\n        \"\"\"\n        value = sheet.cell(row=row, column=column).value\n\n        # No `value` provided\n        if not value:\n            return \"\"\n\n        validated = (\n            bool(re.match(validation_pattern, str(value)))\n            if validation_pattern\n            else True\n        )\n        error_message = f\"Invalid value '{value}' at row {row}, column {column} in sheet {sheet.title}\"\n\n        if is_description:\n            error_message += \" Description should follow the schema: English Description + '//' + German Description.\"\n        elif is_data:\n            val_upper = str(value).upper()\n            allowed_types = [dt.value for dt in DataType]\n            is_valid_standard = val_upper in allowed_types\n            is_valid_dynamic = False\n            if not is_valid_standard and (\n                val_upper.startswith(\"SAMPLE:\") or val_upper.startswith(\"OBJECT:\")\n            ):\n                parts = val_upper.split(\":\", 1)\n                if len(parts) == 2 and parts[1].strip():\n                    is_valid_dynamic = True\n            validated = is_valid_standard or is_valid_dynamic\n            error_message += f\" The Data Type should be one of the following: {allowed_types} or follow the format 'SAMPLE:&lt;CODE&gt;' or 'OBJECT:&lt;CODE&gt;'\"\n        elif is_url:\n            error_message += \" It should be an URL or empty\"\n\n        if not validated:\n            self.logger.error(\n                error_message,\n                cell_value=value,\n                sheet_title=sheet.title,\n                row=row,\n                column=column,\n            )\n\n        return value or \"\"\n\n    def process_entity(\n        self,\n        sheet: \"Worksheet\",\n        start_index_row: int,\n        header_terms: list[str],\n        expected_terms: list[str],\n        entity_type: str,\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Process an entity type block in the Excel sheet and return its attributes as a dictionary.\n\n        Args:\n            sheet: The worksheet object.\n            start_index_row: The row where the current entity type begins (1-based index).\n            header_terms: List of header terms in the entity block.\n            expected_terms: List of expected terms to extract from the entity block.\n            entity_type: The type of the entity (e.g., SAMPLE_TYPE, OBJECT_TYPE).\n\n        Returns:\n            A dictionary containing the attributes of the entity.\n        \"\"\"\n        attributes: dict = {}\n        cell_value: Any = \"\"\n\n        for term in expected_terms:\n            if term not in header_terms:\n                self.logger.error(f\"{term} not found in the headers.\", term=term)\n            else:\n                term_index = header_terms.index(term)\n                cell = sheet.cell(row=start_index_row + 2, column=term_index + 1)\n                cell_value = self.extract_value(\n                    sheet,\n                    start_index_row + 2,\n                    term_index + 1,\n                    self.VALIDATION_RULES[entity_type][term].get(\"pattern\"),\n                )\n\n                # Handle boolean conversion\n                if self.VALIDATION_RULES[entity_type][term].get(\"is_bool\"):\n                    cell_value = self.str_to_bool(\n                        value=cell_value,\n                        term=term,\n                        coordinate=cell.coordinate,\n                        sheet_title=sheet.title,\n                    )\n\n                # Handle data type validation\n                elif self.VALIDATION_RULES[entity_type][term].get(\"is_data\"):\n                    allowed_types = [dt.value for dt in DataType]\n                    cell_value_upper = str(cell_value).upper()\n                    is_valid_standard = cell_value_upper in allowed_types\n                    is_valid_dynamic = False\n                    if not is_valid_standard and (\n                        cell_value_upper.startswith(\"SAMPLE:\")\n                        or cell_value_upper.startswith(\"OBJECT:\")\n                    ):\n                        parts = cell_value_upper.split(\":\", 1)\n                        if len(parts) == 2 and parts[1].strip():\n                            is_valid_dynamic = True\n                    if not is_valid_standard and not is_valid_dynamic:\n                        self.logger.error(\n                            f\"Invalid Data Type: {cell_value} in {cell.coordinate} (Sheet: {sheet.title}). Should be one of the following: {allowed_types} or follow the format 'SAMPLE:&lt;CODE&gt;' or 'OBJECT:&lt;CODE&gt;'\",\n                            term=term,\n                            cell_value=cell_value,\n                            cell_coordinate=cell.coordinate,\n                            sheet_title=sheet.title,\n                        )\n                    else:\n                        cell_value = (\n                            cell_value_upper\n                            if isinstance(cell_value, str)\n                            else cell_value\n                        )\n\n                # Handle additional validation for \"Generated code prefix\"\n                elif (\n                    self.VALIDATION_RULES[entity_type][term].get(\"extra_validation\")\n                    == \"is_reduced_version\"\n                ):\n                    if not is_reduced_version(cell_value, attributes.get(\"code\", \"\")):\n                        self.logger.warning(\n                            f\"Invalid {term} value '{cell_value}' in {cell.coordinate} (Sheet: {sheet.title}). \"\n                            f\"Generated code prefix should be part of the 'Code' {attributes.get('code', '')}.\",\n                            term=term,\n                            cell_value=cell_value,\n                            cell_coordinate=cell.coordinate,\n                            sheet_title=sheet.title,\n                        )\n\n                # Handle validation script (allows empty but must match pattern if provided)\n                elif (\n                    self.VALIDATION_RULES[entity_type][term].get(\"allow_empty\")\n                    and not cell_value\n                ):\n                    cell_value = None\n\n                # Handle URL template validation (allows empty but must be a valid URL)\n                elif (\n                    self.VALIDATION_RULES[entity_type][term].get(\"is_url\")\n                    and cell_value\n                ):\n                    url_pattern = self.VALIDATION_RULES[entity_type][term].get(\n                        \"pattern\"\n                    )\n                    if not re.match(url_pattern, str(cell_value)):\n                        self.logger.error(\n                            f\"Invalid URL format: {cell_value} in {cell.coordinate} (Sheet: {sheet.title})\",\n                            cell_value=cell_value,\n                            cell_coordinate=cell.coordinate,\n                            sheet_title=sheet.title,\n                        )\n\n                # Add the extracted value to the attributes dictionary\n                attributes[self.VALIDATION_RULES[entity_type][term].get(\"key\")] = (\n                    cell_value\n                )\n\n        if self.row_cell_info:\n            attributes[\"row_location\"] = f\"A{start_index_row}\"\n        return attributes\n\n    def properties_to_dict(\n        self, sheet: \"Worksheet\", start_index_row: int, last_non_empty_row: int\n    ) -&gt; dict[str, dict[str, Any]]:\n        \"\"\"\n        Extracts properties from an Entity type block in the Excel sheet and returns them as a dictionary.\n\n        Args:\n            sheet: The worksheet object.\n            start_index_row: Row where the current entity type begins (1-based index).\n            last_non_empty_row: Row where the current entity type finish (1-based index).\n\n        Returns:\n            A dictionary where each key is a property code and the value is a dictionary\n            containing the attributes of the property.\n        \"\"\"\n        property_dict: dict = {}\n        expected_terms = [\n            \"Code\",\n            \"Description\",\n            \"Mandatory\",\n            \"Show in edit views\",\n            \"Section\",\n            \"Property label\",\n            \"Data type\",\n            \"Vocabulary code\",\n            \"Metadata\",\n            \"Dynamic script\",\n            # ! these are not used\n            # \"Unique\",\n            # \"Internal assignment\",\n        ]\n\n        # Determine the header row index\n        header_index = start_index_row + 3\n        row_headers = [(cell.value, cell.coordinate) for cell in sheet[header_index]]\n        # And store how many properties are for the entity\n        n_properties = last_non_empty_row - header_index\n        if n_properties &lt; 0:\n            self.logger.error(\n                f\"No properties found for the entity in sheet {sheet.title} starting at row {start_index_row}.\"\n            )\n            return property_dict\n\n        # Initialize a dictionary to store extracted columns\n        extracted_columns: dict[str, list] = {term: [] for term in expected_terms}\n        if self.row_cell_info:\n            extracted_columns[\"row_location\"] = []\n\n        # Extract columns for each expected term\n        for i, (term, coordinate) in enumerate(row_headers):\n            if term not in expected_terms:\n                log_func = (\n                    self.logger.warning\n                    if term\n                    in (\n                        \"Mandatory\",\n                        \"Show in edit views\",\n                        \"Section\",\n                        \"Metadata\",\n                        \"Dynamic script\",\n                        \"Vocabulary code\",\n                        # ! these are not used\n                        # \"Unique\",\n                        # \"Internal assignment\",\n                    )\n                    else self.logger.error\n                )\n                log_func(f\"'{term}' not found in the properties headers.\", term=term)\n                continue\n\n            # Excel column letter from the coordinate\n            term_letter = coordinate[0]\n\n            # Extract values from the column\n            for cell_property in sheet[term_letter][header_index:last_non_empty_row]:\n                extracted_columns[term].append(\n                    self.process_term(\n                        term, cell_property.value, cell_property.coordinate, sheet.title\n                    )\n                )\n                if self.row_cell_info:\n                    extracted_columns[\"row_location\"].append(cell_property.coordinate)\n\n        # Combine extracted values into a dictionary\n        for i in range(n_properties):\n            code = extracted_columns.get(\"Code\", [])\n            if not code:\n                self.logger.error(\n                    f\"'Code' not found in the properties headers for sheet {sheet.title}.\"\n                )\n                return property_dict\n            code = code[i]\n            property_dict[code] = {\"permId\": code, \"code\": code}\n            for key, pybis_val in {\n                \"Description\": \"description\",\n                \"Section\": \"section\",\n                \"Mandatory\": \"mandatory\",\n                \"Show in edit views\": \"show_in_edit_views\",\n                \"Property label\": \"label\",\n                \"Data type\": \"dataType\",\n                \"Vocabulary code\": \"vocabularyCode\",\n            }.items():\n                data_column = extracted_columns.get(key, [])\n                if not data_column:\n                    continue\n                cell_value = data_column[i]\n                if key == \"Data type\":\n                    object_code = None\n                    normalized_value = (\n                        str(cell_value).upper()\n                        if isinstance(cell_value, str)\n                        else cell_value\n                    )\n                    if isinstance(normalized_value, str) and \":\" in normalized_value:\n                        prefix, dynamic_code = normalized_value.split(\":\", 1)\n                        if prefix in (\"SAMPLE\", \"OBJECT\") and dynamic_code.strip():\n                            object_code = dynamic_code.strip()\n                            normalized_value = DataType.OBJECT.value\n                    property_dict[code][pybis_val] = normalized_value\n                    if object_code:\n                        property_dict[code][\"objectCode\"] = object_code\n                else:\n                    property_dict[code][pybis_val] = cell_value\n            if self.row_cell_info:\n                property_dict[code][\"row_location\"] = (\n                    extracted_columns.get(\"row_location\")[i],\n                )\n            # Only add optional fields if they exist in extracted_columns\n            optional_fields = [\n                \"Metadata\",\n                \"Dynamic script\",\n                \"Unique\",\n                \"Internal assignment\",\n            ]\n            for field in optional_fields:\n                if (\n                    field in extracted_columns\n                ):  # Check if the field exists in the extracted columns\n                    if extracted_columns[field][i] == \"\":\n                        extracted_columns[field][i] = None\n                    property_dict[extracted_columns[\"Code\"][i]][\n                        field.lower().replace(\" \", \"_\")\n                    ] = extracted_columns[field][i]\n\n        return property_dict\n\n    def terms_to_dict(\n        self, sheet: \"Worksheet\", start_index_row: int, last_non_empty_row: int\n    ) -&gt; dict[str, dict[str, Any]]:\n        \"\"\"\n        Extracts terms from a Vocabulary block in the Excel sheet and returns them as a dictionary.\n\n        Args:\n            sheet: The worksheet object.\n            start_index_row: Row where the current entity type begins (1-based index).\n            last_non_empty_row: Row where the current entity type finish (1-based index).\n\n        Returns:\n            A dictionary where each key is a vocabulary term code and the value is a dictionary\n            containing the attributes of the vocabulary term.\n        \"\"\"\n        terms_dict = {}\n        expected_terms = [\"Code\", \"Description\", \"Url template\", \"Label\", \"Official\"]\n\n        header_index = start_index_row + 3\n        row_headers = [cell.value for cell in sheet[header_index]]\n\n        # Initialize a dictionary to store extracted columns\n        extracted_columns: dict[str, list] = {term: [] for term in expected_terms}\n\n        # Helper function to process each term\n        def process_term_cell(term, cell_value, coordinate, sheet_title):\n            if term == \"Official\":\n                return self.str_to_bool(\n                    value=cell_value,\n                    term=term,\n                    coordinate=coordinate,\n                    sheet_title=sheet_title,\n                )\n            return self.get_and_check_property(\n                value=cell_value,\n                term=term,\n                coordinate=coordinate,\n                sheet_title=sheet_title,\n                is_code=(term == \"Code\"),\n                is_url=(term == \"Url template\"),\n            )\n\n        # Extract columns for each expected term\n        for term in expected_terms:\n            if term not in row_headers:\n                self.logger.warning(\n                    f\"{term} not found in the properties headers.\", term=term\n                )\n                continue\n\n            # Get column index and Excel letter\n            term_index = row_headers.index(term) + 1\n            term_letter = self.index_to_excel_column(term_index)\n\n            # Extract values from the column\n            for cell in sheet[term_letter][header_index:last_non_empty_row]:\n                extracted_columns[term].append(\n                    process_term_cell(term, cell.value, cell.coordinate, sheet.title)\n                )\n\n        if not extracted_columns.get(\"Code\"):\n            self.logger.error(\n                f\"The required 'Code' column for terms was not found in sheet {sheet.title}.\"\n            )\n            return {}\n\n        # Combine extracted values into a dictionary safely\n        for i in range(len(extracted_columns[\"Code\"])):\n            code = extracted_columns[\"Code\"][i]\n            terms_dict[code] = {\n                \"permId\": code,\n                \"code\": code,\n            }\n            for key, pybis_val in {\n                \"Description\": \"descriptions\",\n                \"Url template\": \"url_template\",\n                \"Label\": \"label\",\n                \"Official\": \"official\",\n            }.items():\n                values = extracted_columns.get(key, [])\n                if len(values) &gt; i:\n                    terms_dict[code][pybis_val] = values[i]\n\n        return terms_dict\n\n    def block_to_entity_dict(\n        self,\n        sheet: \"Worksheet\",\n        start_index_row: int,\n        last_non_empty_row: int,\n        complete_dict: dict[str, Any],\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Extracts entity attributes from an Excel sheet block and returns them as a dictionary.\n        \"\"\"\n        attributes_dict: dict = {}\n\n        # Get the entity type\n        entity_type = sheet[f\"A{start_index_row}\"].value\n        if entity_type not in self.VALIDATION_RULES:\n            raise ValueError(f\"Invalid entity type: {entity_type}\")\n\n        # Get the header terms\n        header_terms = [cell.value for cell in sheet[start_index_row + 1]]\n\n        # Process entity data using the helper function\n        attributes_dict = self.process_entity(\n            sheet,\n            start_index_row,\n            header_terms,\n            list(self.VALIDATION_RULES[entity_type].keys()),\n            entity_type,\n        )\n\n        # Extract additional attributes if necessary\n        if entity_type in {\n            \"SAMPLE_TYPE\",\n            \"OBJECT_TYPE\",\n            \"EXPERIMENT_TYPE\",\n            \"DATASET_TYPE\",\n        }:\n            attributes_dict[\"properties\"] = (\n                self.properties_to_dict(sheet, start_index_row, last_non_empty_row)\n                or {}\n            )\n\n        elif entity_type == \"VOCABULARY_TYPE\":\n            attributes_dict[\"terms\"] = (\n                self.terms_to_dict(sheet, start_index_row, last_non_empty_row) or {}\n            )\n\n        # Add the entity to the complete dictionary\n        complete_dict[attributes_dict[\"code\"]] = attributes_dict\n\n        # Return sorted dictionary\n        return dict(sorted(complete_dict.items(), key=lambda item: item[0].count(\".\")))\n\n    def excel_to_entities(self) -&gt; dict[str, dict[str, Any]]:\n        \"\"\"\n        Extracts entities from an Excel file and returns them as a dictionary.\n\n        Returns:\n            dict[str, dict[str, Any]]: A dictionary where each key is a normalized sheet name and the value is a dictionary\n            containing the extracted entities. Returns an empty dictionary if all sheets are empty.\n        \"\"\"\n        sheets_dict: dict[str, dict[str, Any]] = {}\n        sheet_names = self.workbook.sheetnames\n        has_content = False  # Track if any sheet has valid content\n\n        for i, sheet_name in enumerate(sheet_names):\n            normalized_sheet_name = sheet_name.lower().replace(\" \", \"_\")\n            sheet = self.workbook[sheet_name]\n            start_row = 1\n\n            # **Check if the sheet is empty**\n            if all(\n                sheet.cell(row=row, column=col).value in (None, \"\")\n                for row in range(1, sheet.max_row + 1)\n                for col in range(1, sheet.max_column + 1)\n            ):\n                self.logger.info(f\"Skipping empty sheet: {sheet_name}\")\n                continue  # Move to the next sheet\n\n            sheets_dict[normalized_sheet_name] = {}\n\n            consecutive_empty_rows = 0  # Track consecutive empty rows\n            while start_row &lt;= sheet.max_row:\n                # **Check for two consecutive empty rows**\n                is_row_empty = all(\n                    sheet.cell(row=start_row, column=col).value in (None, \"\")\n                    for col in range(1, sheet.max_column + 1)\n                )\n\n                if is_row_empty:\n                    consecutive_empty_rows += 1\n                    if consecutive_empty_rows &gt;= 2:\n                        # **Reached the end of the sheet, move to the next**\n                        if i == len(sheet_names) - 1:\n                            self.logger.info(\n                                f\"Last sheet {sheet_name} processed. End of the file reached.\"\n                            )\n                        else:\n                            self.logger.info(\n                                f\"End of the current sheet {sheet_name} reached. Switching to next sheet...\"\n                            )\n                        break  # Stop processing this sheet\n                else:\n                    consecutive_empty_rows = 0  # Reset if we find a non-empty row\n\n                    # **Process the entity block**\n                    last_non_empty_row = self.get_last_non_empty_row(sheet, start_row)\n                    if last_non_empty_row is None:\n                        break  # No more valid blocks\n\n                    sheets_dict[normalized_sheet_name] = self.block_to_entity_dict(\n                        sheet,\n                        start_row,\n                        last_non_empty_row,\n                        sheets_dict[normalized_sheet_name],\n                    )\n                    has_content = True  # Found valid content\n\n                    # Move to the next entity block\n                    start_row = last_non_empty_row + 1\n                    continue  # Continue loop without increasing consecutive_empty_rows\n\n                start_row += 1  # Move to the next row\n\n        # **If no sheets had content, return an empty dictionary**\n        if not has_content:\n            self.logger.warning(\n                \"No valid data found in any sheets. Returning empty dictionary.\"\n            )\n            return {}\n\n        return sheets_dict\n</code></pre>"},{"location":"references/api/#bam_masterdata.excel.excel_to_entities.MasterdataExcelExtractor.VALIDATION_RULES","title":"<code>VALIDATION_RULES = {}</code>","text":""},{"location":"references/api/#bam_masterdata.excel.excel_to_entities.MasterdataExcelExtractor.excel_path","title":"<code>excel_path = excel_path</code>","text":""},{"location":"references/api/#bam_masterdata.excel.excel_to_entities.MasterdataExcelExtractor.row_cell_info","title":"<code>row_cell_info = kwargs.get('row_cell_info', False)</code>","text":""},{"location":"references/api/#bam_masterdata.excel.excel_to_entities.MasterdataExcelExtractor.workbook","title":"<code>workbook = openpyxl.load_workbook(excel_path)</code>","text":""},{"location":"references/api/#bam_masterdata.excel.excel_to_entities.MasterdataExcelExtractor.logger","title":"<code>logger = kwargs.get('logger', logger)</code>","text":""},{"location":"references/api/#bam_masterdata.excel.excel_to_entities.MasterdataExcelExtractor.__init__","title":"<code>__init__(excel_path, **kwargs)</code>","text":"<p>Initialize the MasterdataExtractor.</p> Source code in <code>bam_masterdata/excel/excel_to_entities.py</code> <pre><code>def __init__(self, excel_path: str, **kwargs):\n    \"\"\"Initialize the MasterdataExtractor.\"\"\"\n    self.excel_path = excel_path\n    self.row_cell_info = kwargs.get(\"row_cell_info\", False)\n    self.workbook = openpyxl.load_workbook(excel_path)\n    self.logger = kwargs.get(\"logger\", logger)\n\n    # Load validation rules at initialization\n    if not MasterdataExcelExtractor.VALIDATION_RULES:\n        self.VALIDATION_RULES = load_validation_rules(\n            self.logger,\n            os.path.join(VALIDATION_RULES_DIR, \"excel_validation_rules.json\"),\n        )\n</code></pre>"},{"location":"references/api/#bam_masterdata.excel.excel_to_entities.MasterdataExcelExtractor.index_to_excel_column","title":"<code>index_to_excel_column(index)</code>","text":"<p>Converts a 1-based index to an Excel column name.</p> PARAMETER DESCRIPTION <code>index</code> <p>The 1-based index to convert.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The corresponding Excel column name.</p> Source code in <code>bam_masterdata/excel/excel_to_entities.py</code> <pre><code>def index_to_excel_column(self, index: int) -&gt; str:\n    \"\"\"\n    Converts a 1-based index to an Excel column name.\n\n    Args:\n        index: The 1-based index to convert.\n\n    Returns:\n        The corresponding Excel column name.\n    \"\"\"\n    if not index &gt;= 1:\n        raise ValueError(\"Index must be a positive integer starting from 1.\")\n\n    column = \"\"\n    while index &gt; 0:\n        index, remainder = divmod(index - 1, 26)\n        column = chr(65 + remainder) + column\n    return column\n</code></pre>"},{"location":"references/api/#bam_masterdata.excel.excel_to_entities.MasterdataExcelExtractor.get_last_non_empty_row","title":"<code>get_last_non_empty_row(sheet, start_index)</code>","text":"<p>Finds the last non-empty row before encountering a completely empty row.</p> PARAMETER DESCRIPTION <code>sheet</code> <p>The worksheet object.</p> <p> TYPE: <code>Worksheet</code> </p> <code>start_index</code> <p>The row number to start checking from (1-based index).</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>int | None</code> <p>The row number of the last non-empty row before an empty row is encountered,</p> <code>int | None</code> <p>or None if no non-empty rows are found starting from the given index.</p> Source code in <code>bam_masterdata/excel/excel_to_entities.py</code> <pre><code>def get_last_non_empty_row(\n    self, sheet: \"Worksheet\", start_index: int\n) -&gt; int | None:\n    \"\"\"\n    Finds the last non-empty row before encountering a completely empty row.\n\n    Args:\n        sheet: The worksheet object.\n        start_index: The row number to start checking from (1-based index).\n\n    Returns:\n        The row number of the last non-empty row before an empty row is encountered,\n        or None if no non-empty rows are found starting from the given index.\n    \"\"\"\n    if start_index &lt; 1 or start_index &gt; sheet.max_row:\n        raise ValueError(\n            f\"Invalid start index: {start_index}. It must be between 1 and {sheet.max_row}.\"\n        )\n\n    last_non_empty_row = None\n    for row in range(start_index, sheet.max_row + 1):\n        if all(\n            sheet.cell(row=row, column=col).value in (None, \"\")\n            for col in range(1, sheet.max_column + 1)\n        ):\n            return last_non_empty_row  # Return the last non-empty row before the current empty row\n\n        last_non_empty_row = row  # Update the last non-empty row\n\n    return last_non_empty_row  # If no empty row is encountered, return the last non-empty row\n</code></pre>"},{"location":"references/api/#bam_masterdata.excel.excel_to_entities.MasterdataExcelExtractor.str_to_bool","title":"<code>str_to_bool(value, term, coordinate, sheet_title)</code>","text":"<p>Converts a string to a boolean value.</p> PARAMETER DESCRIPTION <code>value</code> <p>The string to convert.</p> <p> TYPE: <code>str | bool | None</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>The boolean value.</p> Source code in <code>bam_masterdata/excel/excel_to_entities.py</code> <pre><code>def str_to_bool(\n    self,\n    value: str | bool | None,\n    term: str,\n    coordinate: str,\n    sheet_title: str,\n) -&gt; bool:\n    \"\"\"\n    Converts a string to a boolean value.\n\n    Args:\n        value: The string to convert.\n\n    Returns:\n        The boolean value.\n    \"\"\"\n    # No `value` provided\n    if not value:\n        return False\n\n    val = str(value).strip().lower()\n    if val not in [\"true\", \"false\"]:\n        self.logger.error(\n            f\"Invalid {term.lower()} value found in the {term} column at position {coordinate} in {sheet_title}. Accepted values: TRUE or FALSE.\",\n            term=term,\n            cell_value=val,\n            cell_coordinate=coordinate,\n            sheet_title=sheet_title,\n        )\n    return val == \"true\"\n</code></pre>"},{"location":"references/api/#bam_masterdata.excel.excel_to_entities.MasterdataExcelExtractor.get_and_check_property","title":"<code>get_and_check_property(value, term, coordinate, sheet_title, is_description=False, is_code=False, is_data=False, is_url=False)</code>","text":"<p>Gets a property and checks its format.</p> PARAMETER DESCRIPTION <code>value</code> <p>The string to convert.</p> <p> TYPE: <code>str | bool | None</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The property.</p> Source code in <code>bam_masterdata/excel/excel_to_entities.py</code> <pre><code>def get_and_check_property(\n    self,\n    value: str | bool | None,\n    term: str,\n    coordinate: str,\n    sheet_title: str,\n    is_description: bool = False,\n    is_code: bool = False,\n    is_data: bool = False,\n    is_url: bool = False,\n) -&gt; str:\n    \"\"\"\n    Gets a property and checks its format.\n\n    Args:\n        value: The string to convert.\n\n    Returns:\n        The property.\n    \"\"\"\n\n    # No `value` provided\n    if not value:\n        return \"\"\n\n    val = str(value)\n    error_message = f\"Invalid {term.lower()} value found in the {term} column at position {coordinate} in {sheet_title}.\"\n    if is_description:\n        if not re.match(r\".*//.*\", val):\n            self.logger.error(\n                error_message\n                + \"Description should follow the schema: English Description + '//' + German Description. \",\n                term=term,\n                cell_value=val,\n                cell_coordinate=coordinate,\n                sheet_title=sheet_title,\n            )\n    elif is_code:\n        if not re.match(r\"^\\$?[A-Z0-9_.]+$\", val):\n            self.logger.error(\n                error_message,\n                term=term,\n                cell_value=val,\n                cell_coordinate=coordinate,\n                sheet_title=sheet_title,\n            )\n    elif is_data:\n        # Normalize data type to uppercase and allow dynamic SAMPLE/OBJECT codes\n        val_upper = val.upper()\n        allowed_types = [dt.value for dt in DataType]\n        is_valid_standard = val_upper in allowed_types\n        is_valid_dynamic = False\n\n        if not is_valid_standard and (\n            val_upper.startswith(\"SAMPLE:\") or val_upper.startswith(\"OBJECT:\")\n        ):\n            parts = val_upper.split(\":\", 1)\n            if len(parts) == 2 and parts[1].strip():\n                is_valid_dynamic = True\n\n        if not is_valid_standard and not is_valid_dynamic:\n            self.logger.error(\n                error_message\n                + f\"The Data Type should be one of the following: {allowed_types} or follow the format 'SAMPLE:&lt;CODE&gt;' or 'OBJECT:&lt;CODE&gt;'\",\n                term=term,\n                cell_value=val_upper,\n                cell_coordinate=coordinate,\n                sheet_title=sheet_title,\n            )\n        val = val_upper\n    elif is_url:\n        if not re.match(\n            r\"https?://(?:www\\.)?[a-zA-Z0-9-._~:/?#@!$&amp;'()*+,;=%]+\", val\n        ):\n            self.logger.error(\n                error_message,\n                term=term,\n                cell_value=val,\n                cell_coordinate=coordinate,\n                sheet_title=sheet_title,\n            )\n    else:\n        if not re.match(r\".*\", val):\n            self.logger.error(\n                error_message,\n                term=term,\n                cell_value=val,\n                cell_coordinate=coordinate,\n                sheet_title=sheet_title,\n            )\n    return val\n</code></pre>"},{"location":"references/api/#bam_masterdata.excel.excel_to_entities.MasterdataExcelExtractor.process_term","title":"<code>process_term(term, cell_value, coordinate, sheet_title)</code>","text":"<p>Processes a term by converting it to a boolean if necessary or checking its validity.</p> PARAMETER DESCRIPTION <code>term</code> <p>The term being processed.</p> <p> TYPE: <code>str</code> </p> <code>cell_value</code> <p>The value of the cell.</p> <p> TYPE: <code>Any</code> </p> <code>coordinate</code> <p>The coordinate of the cell in the sheet.</p> <p> TYPE: <code>str</code> </p> <code>sheet_title</code> <p>The title of the sheet.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>The processed value, either as a boolean or the original value after validation.</p> Source code in <code>bam_masterdata/excel/excel_to_entities.py</code> <pre><code>def process_term(\n    self, term: str, cell_value: Any, coordinate: str, sheet_title: str\n) -&gt; Any:\n    \"\"\"\n    Processes a term by converting it to a boolean if necessary or checking its validity.\n\n    Args:\n        term: The term being processed.\n        cell_value: The value of the cell.\n        coordinate: The coordinate of the cell in the sheet.\n        sheet_title: The title of the sheet.\n\n    Returns:\n        The processed value, either as a boolean or the original value after validation.\n    \"\"\"\n    # Check if the term is a boolean type\n    if term in (\"Mandatory\", \"Show in edit views\"):\n        return self.str_to_bool(\n            value=cell_value,\n            term=term,\n            coordinate=coordinate,\n            sheet_title=sheet_title,\n        )\n    # Check and validate the property\n    return self.get_and_check_property(\n        value=cell_value,\n        term=term,\n        coordinate=coordinate,\n        sheet_title=sheet_title,\n        is_code=(term in [\"Code\", \"Vocabulary code\"]),\n        is_data=(term == \"Data type\"),\n    )\n</code></pre>"},{"location":"references/api/#bam_masterdata.excel.excel_to_entities.MasterdataExcelExtractor.extract_value","title":"<code>extract_value(sheet, row, column, validation_pattern=None, is_description=False, is_data=False, is_url=False)</code>","text":"<p>Extracts and validates a value from a specified cell in the Excel sheet.</p> PARAMETER DESCRIPTION <code>sheet</code> <p>The worksheet object.</p> <p> TYPE: <code>Worksheet</code> </p> <code>row</code> <p>The row number of the cell (1-based index).</p> <p> TYPE: <code>int</code> </p> <code>column</code> <p>The column number of the cell (1-based index).</p> <p> TYPE: <code>int</code> </p> <code>validation_pattern</code> <p>Optional regex pattern to validate the cell value.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>is_description</code> <p>Flag indicating if the value is a description.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>is_data</code> <p>Flag indicating if the value is a data type.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>is_url</code> <p>Flag indicating if the value is a URL.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The extracted and validated cell value as a string. Returns an empty string if the value is invalid or not provided.</p> Source code in <code>bam_masterdata/excel/excel_to_entities.py</code> <pre><code>def extract_value(\n    self,\n    sheet: \"Worksheet\",\n    row: int,\n    column: int,\n    validation_pattern: str = None,\n    is_description: bool = False,\n    is_data: bool = False,\n    is_url: bool = False,\n) -&gt; str:\n    \"\"\"\n    Extracts and validates a value from a specified cell in the Excel sheet.\n\n    Args:\n        sheet: The worksheet object.\n        row: The row number of the cell (1-based index).\n        column: The column number of the cell (1-based index).\n        validation_pattern: Optional regex pattern to validate the cell value.\n        is_description: Flag indicating if the value is a description.\n        is_data: Flag indicating if the value is a data type.\n        is_url: Flag indicating if the value is a URL.\n\n    Returns:\n        The extracted and validated cell value as a string. Returns an empty string if the value is invalid or not provided.\n    \"\"\"\n    value = sheet.cell(row=row, column=column).value\n\n    # No `value` provided\n    if not value:\n        return \"\"\n\n    validated = (\n        bool(re.match(validation_pattern, str(value)))\n        if validation_pattern\n        else True\n    )\n    error_message = f\"Invalid value '{value}' at row {row}, column {column} in sheet {sheet.title}\"\n\n    if is_description:\n        error_message += \" Description should follow the schema: English Description + '//' + German Description.\"\n    elif is_data:\n        val_upper = str(value).upper()\n        allowed_types = [dt.value for dt in DataType]\n        is_valid_standard = val_upper in allowed_types\n        is_valid_dynamic = False\n        if not is_valid_standard and (\n            val_upper.startswith(\"SAMPLE:\") or val_upper.startswith(\"OBJECT:\")\n        ):\n            parts = val_upper.split(\":\", 1)\n            if len(parts) == 2 and parts[1].strip():\n                is_valid_dynamic = True\n        validated = is_valid_standard or is_valid_dynamic\n        error_message += f\" The Data Type should be one of the following: {allowed_types} or follow the format 'SAMPLE:&lt;CODE&gt;' or 'OBJECT:&lt;CODE&gt;'\"\n    elif is_url:\n        error_message += \" It should be an URL or empty\"\n\n    if not validated:\n        self.logger.error(\n            error_message,\n            cell_value=value,\n            sheet_title=sheet.title,\n            row=row,\n            column=column,\n        )\n\n    return value or \"\"\n</code></pre>"},{"location":"references/api/#bam_masterdata.excel.excel_to_entities.MasterdataExcelExtractor.process_entity","title":"<code>process_entity(sheet, start_index_row, header_terms, expected_terms, entity_type)</code>","text":"<p>Process an entity type block in the Excel sheet and return its attributes as a dictionary.</p> PARAMETER DESCRIPTION <code>sheet</code> <p>The worksheet object.</p> <p> TYPE: <code>Worksheet</code> </p> <code>start_index_row</code> <p>The row where the current entity type begins (1-based index).</p> <p> TYPE: <code>int</code> </p> <code>header_terms</code> <p>List of header terms in the entity block.</p> <p> TYPE: <code>list[str]</code> </p> <code>expected_terms</code> <p>List of expected terms to extract from the entity block.</p> <p> TYPE: <code>list[str]</code> </p> <code>entity_type</code> <p>The type of the entity (e.g., SAMPLE_TYPE, OBJECT_TYPE).</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>A dictionary containing the attributes of the entity.</p> Source code in <code>bam_masterdata/excel/excel_to_entities.py</code> <pre><code>def process_entity(\n    self,\n    sheet: \"Worksheet\",\n    start_index_row: int,\n    header_terms: list[str],\n    expected_terms: list[str],\n    entity_type: str,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Process an entity type block in the Excel sheet and return its attributes as a dictionary.\n\n    Args:\n        sheet: The worksheet object.\n        start_index_row: The row where the current entity type begins (1-based index).\n        header_terms: List of header terms in the entity block.\n        expected_terms: List of expected terms to extract from the entity block.\n        entity_type: The type of the entity (e.g., SAMPLE_TYPE, OBJECT_TYPE).\n\n    Returns:\n        A dictionary containing the attributes of the entity.\n    \"\"\"\n    attributes: dict = {}\n    cell_value: Any = \"\"\n\n    for term in expected_terms:\n        if term not in header_terms:\n            self.logger.error(f\"{term} not found in the headers.\", term=term)\n        else:\n            term_index = header_terms.index(term)\n            cell = sheet.cell(row=start_index_row + 2, column=term_index + 1)\n            cell_value = self.extract_value(\n                sheet,\n                start_index_row + 2,\n                term_index + 1,\n                self.VALIDATION_RULES[entity_type][term].get(\"pattern\"),\n            )\n\n            # Handle boolean conversion\n            if self.VALIDATION_RULES[entity_type][term].get(\"is_bool\"):\n                cell_value = self.str_to_bool(\n                    value=cell_value,\n                    term=term,\n                    coordinate=cell.coordinate,\n                    sheet_title=sheet.title,\n                )\n\n            # Handle data type validation\n            elif self.VALIDATION_RULES[entity_type][term].get(\"is_data\"):\n                allowed_types = [dt.value for dt in DataType]\n                cell_value_upper = str(cell_value).upper()\n                is_valid_standard = cell_value_upper in allowed_types\n                is_valid_dynamic = False\n                if not is_valid_standard and (\n                    cell_value_upper.startswith(\"SAMPLE:\")\n                    or cell_value_upper.startswith(\"OBJECT:\")\n                ):\n                    parts = cell_value_upper.split(\":\", 1)\n                    if len(parts) == 2 and parts[1].strip():\n                        is_valid_dynamic = True\n                if not is_valid_standard and not is_valid_dynamic:\n                    self.logger.error(\n                        f\"Invalid Data Type: {cell_value} in {cell.coordinate} (Sheet: {sheet.title}). Should be one of the following: {allowed_types} or follow the format 'SAMPLE:&lt;CODE&gt;' or 'OBJECT:&lt;CODE&gt;'\",\n                        term=term,\n                        cell_value=cell_value,\n                        cell_coordinate=cell.coordinate,\n                        sheet_title=sheet.title,\n                    )\n                else:\n                    cell_value = (\n                        cell_value_upper\n                        if isinstance(cell_value, str)\n                        else cell_value\n                    )\n\n            # Handle additional validation for \"Generated code prefix\"\n            elif (\n                self.VALIDATION_RULES[entity_type][term].get(\"extra_validation\")\n                == \"is_reduced_version\"\n            ):\n                if not is_reduced_version(cell_value, attributes.get(\"code\", \"\")):\n                    self.logger.warning(\n                        f\"Invalid {term} value '{cell_value}' in {cell.coordinate} (Sheet: {sheet.title}). \"\n                        f\"Generated code prefix should be part of the 'Code' {attributes.get('code', '')}.\",\n                        term=term,\n                        cell_value=cell_value,\n                        cell_coordinate=cell.coordinate,\n                        sheet_title=sheet.title,\n                    )\n\n            # Handle validation script (allows empty but must match pattern if provided)\n            elif (\n                self.VALIDATION_RULES[entity_type][term].get(\"allow_empty\")\n                and not cell_value\n            ):\n                cell_value = None\n\n            # Handle URL template validation (allows empty but must be a valid URL)\n            elif (\n                self.VALIDATION_RULES[entity_type][term].get(\"is_url\")\n                and cell_value\n            ):\n                url_pattern = self.VALIDATION_RULES[entity_type][term].get(\n                    \"pattern\"\n                )\n                if not re.match(url_pattern, str(cell_value)):\n                    self.logger.error(\n                        f\"Invalid URL format: {cell_value} in {cell.coordinate} (Sheet: {sheet.title})\",\n                        cell_value=cell_value,\n                        cell_coordinate=cell.coordinate,\n                        sheet_title=sheet.title,\n                    )\n\n            # Add the extracted value to the attributes dictionary\n            attributes[self.VALIDATION_RULES[entity_type][term].get(\"key\")] = (\n                cell_value\n            )\n\n    if self.row_cell_info:\n        attributes[\"row_location\"] = f\"A{start_index_row}\"\n    return attributes\n</code></pre>"},{"location":"references/api/#bam_masterdata.excel.excel_to_entities.MasterdataExcelExtractor.properties_to_dict","title":"<code>properties_to_dict(sheet, start_index_row, last_non_empty_row)</code>","text":"<p>Extracts properties from an Entity type block in the Excel sheet and returns them as a dictionary.</p> PARAMETER DESCRIPTION <code>sheet</code> <p>The worksheet object.</p> <p> TYPE: <code>Worksheet</code> </p> <code>start_index_row</code> <p>Row where the current entity type begins (1-based index).</p> <p> TYPE: <code>int</code> </p> <code>last_non_empty_row</code> <p>Row where the current entity type finish (1-based index).</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>dict[str, dict[str, Any]]</code> <p>A dictionary where each key is a property code and the value is a dictionary</p> <code>dict[str, dict[str, Any]]</code> <p>containing the attributes of the property.</p> Source code in <code>bam_masterdata/excel/excel_to_entities.py</code> <pre><code>def properties_to_dict(\n    self, sheet: \"Worksheet\", start_index_row: int, last_non_empty_row: int\n) -&gt; dict[str, dict[str, Any]]:\n    \"\"\"\n    Extracts properties from an Entity type block in the Excel sheet and returns them as a dictionary.\n\n    Args:\n        sheet: The worksheet object.\n        start_index_row: Row where the current entity type begins (1-based index).\n        last_non_empty_row: Row where the current entity type finish (1-based index).\n\n    Returns:\n        A dictionary where each key is a property code and the value is a dictionary\n        containing the attributes of the property.\n    \"\"\"\n    property_dict: dict = {}\n    expected_terms = [\n        \"Code\",\n        \"Description\",\n        \"Mandatory\",\n        \"Show in edit views\",\n        \"Section\",\n        \"Property label\",\n        \"Data type\",\n        \"Vocabulary code\",\n        \"Metadata\",\n        \"Dynamic script\",\n        # ! these are not used\n        # \"Unique\",\n        # \"Internal assignment\",\n    ]\n\n    # Determine the header row index\n    header_index = start_index_row + 3\n    row_headers = [(cell.value, cell.coordinate) for cell in sheet[header_index]]\n    # And store how many properties are for the entity\n    n_properties = last_non_empty_row - header_index\n    if n_properties &lt; 0:\n        self.logger.error(\n            f\"No properties found for the entity in sheet {sheet.title} starting at row {start_index_row}.\"\n        )\n        return property_dict\n\n    # Initialize a dictionary to store extracted columns\n    extracted_columns: dict[str, list] = {term: [] for term in expected_terms}\n    if self.row_cell_info:\n        extracted_columns[\"row_location\"] = []\n\n    # Extract columns for each expected term\n    for i, (term, coordinate) in enumerate(row_headers):\n        if term not in expected_terms:\n            log_func = (\n                self.logger.warning\n                if term\n                in (\n                    \"Mandatory\",\n                    \"Show in edit views\",\n                    \"Section\",\n                    \"Metadata\",\n                    \"Dynamic script\",\n                    \"Vocabulary code\",\n                    # ! these are not used\n                    # \"Unique\",\n                    # \"Internal assignment\",\n                )\n                else self.logger.error\n            )\n            log_func(f\"'{term}' not found in the properties headers.\", term=term)\n            continue\n\n        # Excel column letter from the coordinate\n        term_letter = coordinate[0]\n\n        # Extract values from the column\n        for cell_property in sheet[term_letter][header_index:last_non_empty_row]:\n            extracted_columns[term].append(\n                self.process_term(\n                    term, cell_property.value, cell_property.coordinate, sheet.title\n                )\n            )\n            if self.row_cell_info:\n                extracted_columns[\"row_location\"].append(cell_property.coordinate)\n\n    # Combine extracted values into a dictionary\n    for i in range(n_properties):\n        code = extracted_columns.get(\"Code\", [])\n        if not code:\n            self.logger.error(\n                f\"'Code' not found in the properties headers for sheet {sheet.title}.\"\n            )\n            return property_dict\n        code = code[i]\n        property_dict[code] = {\"permId\": code, \"code\": code}\n        for key, pybis_val in {\n            \"Description\": \"description\",\n            \"Section\": \"section\",\n            \"Mandatory\": \"mandatory\",\n            \"Show in edit views\": \"show_in_edit_views\",\n            \"Property label\": \"label\",\n            \"Data type\": \"dataType\",\n            \"Vocabulary code\": \"vocabularyCode\",\n        }.items():\n            data_column = extracted_columns.get(key, [])\n            if not data_column:\n                continue\n            cell_value = data_column[i]\n            if key == \"Data type\":\n                object_code = None\n                normalized_value = (\n                    str(cell_value).upper()\n                    if isinstance(cell_value, str)\n                    else cell_value\n                )\n                if isinstance(normalized_value, str) and \":\" in normalized_value:\n                    prefix, dynamic_code = normalized_value.split(\":\", 1)\n                    if prefix in (\"SAMPLE\", \"OBJECT\") and dynamic_code.strip():\n                        object_code = dynamic_code.strip()\n                        normalized_value = DataType.OBJECT.value\n                property_dict[code][pybis_val] = normalized_value\n                if object_code:\n                    property_dict[code][\"objectCode\"] = object_code\n            else:\n                property_dict[code][pybis_val] = cell_value\n        if self.row_cell_info:\n            property_dict[code][\"row_location\"] = (\n                extracted_columns.get(\"row_location\")[i],\n            )\n        # Only add optional fields if they exist in extracted_columns\n        optional_fields = [\n            \"Metadata\",\n            \"Dynamic script\",\n            \"Unique\",\n            \"Internal assignment\",\n        ]\n        for field in optional_fields:\n            if (\n                field in extracted_columns\n            ):  # Check if the field exists in the extracted columns\n                if extracted_columns[field][i] == \"\":\n                    extracted_columns[field][i] = None\n                property_dict[extracted_columns[\"Code\"][i]][\n                    field.lower().replace(\" \", \"_\")\n                ] = extracted_columns[field][i]\n\n    return property_dict\n</code></pre>"},{"location":"references/api/#bam_masterdata.excel.excel_to_entities.MasterdataExcelExtractor.terms_to_dict","title":"<code>terms_to_dict(sheet, start_index_row, last_non_empty_row)</code>","text":"<p>Extracts terms from a Vocabulary block in the Excel sheet and returns them as a dictionary.</p> PARAMETER DESCRIPTION <code>sheet</code> <p>The worksheet object.</p> <p> TYPE: <code>Worksheet</code> </p> <code>start_index_row</code> <p>Row where the current entity type begins (1-based index).</p> <p> TYPE: <code>int</code> </p> <code>last_non_empty_row</code> <p>Row where the current entity type finish (1-based index).</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>dict[str, dict[str, Any]]</code> <p>A dictionary where each key is a vocabulary term code and the value is a dictionary</p> <code>dict[str, dict[str, Any]]</code> <p>containing the attributes of the vocabulary term.</p> Source code in <code>bam_masterdata/excel/excel_to_entities.py</code> <pre><code>def terms_to_dict(\n    self, sheet: \"Worksheet\", start_index_row: int, last_non_empty_row: int\n) -&gt; dict[str, dict[str, Any]]:\n    \"\"\"\n    Extracts terms from a Vocabulary block in the Excel sheet and returns them as a dictionary.\n\n    Args:\n        sheet: The worksheet object.\n        start_index_row: Row where the current entity type begins (1-based index).\n        last_non_empty_row: Row where the current entity type finish (1-based index).\n\n    Returns:\n        A dictionary where each key is a vocabulary term code and the value is a dictionary\n        containing the attributes of the vocabulary term.\n    \"\"\"\n    terms_dict = {}\n    expected_terms = [\"Code\", \"Description\", \"Url template\", \"Label\", \"Official\"]\n\n    header_index = start_index_row + 3\n    row_headers = [cell.value for cell in sheet[header_index]]\n\n    # Initialize a dictionary to store extracted columns\n    extracted_columns: dict[str, list] = {term: [] for term in expected_terms}\n\n    # Helper function to process each term\n    def process_term_cell(term, cell_value, coordinate, sheet_title):\n        if term == \"Official\":\n            return self.str_to_bool(\n                value=cell_value,\n                term=term,\n                coordinate=coordinate,\n                sheet_title=sheet_title,\n            )\n        return self.get_and_check_property(\n            value=cell_value,\n            term=term,\n            coordinate=coordinate,\n            sheet_title=sheet_title,\n            is_code=(term == \"Code\"),\n            is_url=(term == \"Url template\"),\n        )\n\n    # Extract columns for each expected term\n    for term in expected_terms:\n        if term not in row_headers:\n            self.logger.warning(\n                f\"{term} not found in the properties headers.\", term=term\n            )\n            continue\n\n        # Get column index and Excel letter\n        term_index = row_headers.index(term) + 1\n        term_letter = self.index_to_excel_column(term_index)\n\n        # Extract values from the column\n        for cell in sheet[term_letter][header_index:last_non_empty_row]:\n            extracted_columns[term].append(\n                process_term_cell(term, cell.value, cell.coordinate, sheet.title)\n            )\n\n    if not extracted_columns.get(\"Code\"):\n        self.logger.error(\n            f\"The required 'Code' column for terms was not found in sheet {sheet.title}.\"\n        )\n        return {}\n\n    # Combine extracted values into a dictionary safely\n    for i in range(len(extracted_columns[\"Code\"])):\n        code = extracted_columns[\"Code\"][i]\n        terms_dict[code] = {\n            \"permId\": code,\n            \"code\": code,\n        }\n        for key, pybis_val in {\n            \"Description\": \"descriptions\",\n            \"Url template\": \"url_template\",\n            \"Label\": \"label\",\n            \"Official\": \"official\",\n        }.items():\n            values = extracted_columns.get(key, [])\n            if len(values) &gt; i:\n                terms_dict[code][pybis_val] = values[i]\n\n    return terms_dict\n</code></pre>"},{"location":"references/api/#bam_masterdata.excel.excel_to_entities.MasterdataExcelExtractor.block_to_entity_dict","title":"<code>block_to_entity_dict(sheet, start_index_row, last_non_empty_row, complete_dict)</code>","text":"<p>Extracts entity attributes from an Excel sheet block and returns them as a dictionary.</p> Source code in <code>bam_masterdata/excel/excel_to_entities.py</code> <pre><code>def block_to_entity_dict(\n    self,\n    sheet: \"Worksheet\",\n    start_index_row: int,\n    last_non_empty_row: int,\n    complete_dict: dict[str, Any],\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Extracts entity attributes from an Excel sheet block and returns them as a dictionary.\n    \"\"\"\n    attributes_dict: dict = {}\n\n    # Get the entity type\n    entity_type = sheet[f\"A{start_index_row}\"].value\n    if entity_type not in self.VALIDATION_RULES:\n        raise ValueError(f\"Invalid entity type: {entity_type}\")\n\n    # Get the header terms\n    header_terms = [cell.value for cell in sheet[start_index_row + 1]]\n\n    # Process entity data using the helper function\n    attributes_dict = self.process_entity(\n        sheet,\n        start_index_row,\n        header_terms,\n        list(self.VALIDATION_RULES[entity_type].keys()),\n        entity_type,\n    )\n\n    # Extract additional attributes if necessary\n    if entity_type in {\n        \"SAMPLE_TYPE\",\n        \"OBJECT_TYPE\",\n        \"EXPERIMENT_TYPE\",\n        \"DATASET_TYPE\",\n    }:\n        attributes_dict[\"properties\"] = (\n            self.properties_to_dict(sheet, start_index_row, last_non_empty_row)\n            or {}\n        )\n\n    elif entity_type == \"VOCABULARY_TYPE\":\n        attributes_dict[\"terms\"] = (\n            self.terms_to_dict(sheet, start_index_row, last_non_empty_row) or {}\n        )\n\n    # Add the entity to the complete dictionary\n    complete_dict[attributes_dict[\"code\"]] = attributes_dict\n\n    # Return sorted dictionary\n    return dict(sorted(complete_dict.items(), key=lambda item: item[0].count(\".\")))\n</code></pre>"},{"location":"references/api/#bam_masterdata.excel.excel_to_entities.MasterdataExcelExtractor.excel_to_entities","title":"<code>excel_to_entities()</code>","text":"<p>Extracts entities from an Excel file and returns them as a dictionary.</p> RETURNS DESCRIPTION <code>dict[str, dict[str, Any]]</code> <p>dict[str, dict[str, Any]]: A dictionary where each key is a normalized sheet name and the value is a dictionary</p> <code>dict[str, dict[str, Any]]</code> <p>containing the extracted entities. Returns an empty dictionary if all sheets are empty.</p> Source code in <code>bam_masterdata/excel/excel_to_entities.py</code> <pre><code>def excel_to_entities(self) -&gt; dict[str, dict[str, Any]]:\n    \"\"\"\n    Extracts entities from an Excel file and returns them as a dictionary.\n\n    Returns:\n        dict[str, dict[str, Any]]: A dictionary where each key is a normalized sheet name and the value is a dictionary\n        containing the extracted entities. Returns an empty dictionary if all sheets are empty.\n    \"\"\"\n    sheets_dict: dict[str, dict[str, Any]] = {}\n    sheet_names = self.workbook.sheetnames\n    has_content = False  # Track if any sheet has valid content\n\n    for i, sheet_name in enumerate(sheet_names):\n        normalized_sheet_name = sheet_name.lower().replace(\" \", \"_\")\n        sheet = self.workbook[sheet_name]\n        start_row = 1\n\n        # **Check if the sheet is empty**\n        if all(\n            sheet.cell(row=row, column=col).value in (None, \"\")\n            for row in range(1, sheet.max_row + 1)\n            for col in range(1, sheet.max_column + 1)\n        ):\n            self.logger.info(f\"Skipping empty sheet: {sheet_name}\")\n            continue  # Move to the next sheet\n\n        sheets_dict[normalized_sheet_name] = {}\n\n        consecutive_empty_rows = 0  # Track consecutive empty rows\n        while start_row &lt;= sheet.max_row:\n            # **Check for two consecutive empty rows**\n            is_row_empty = all(\n                sheet.cell(row=start_row, column=col).value in (None, \"\")\n                for col in range(1, sheet.max_column + 1)\n            )\n\n            if is_row_empty:\n                consecutive_empty_rows += 1\n                if consecutive_empty_rows &gt;= 2:\n                    # **Reached the end of the sheet, move to the next**\n                    if i == len(sheet_names) - 1:\n                        self.logger.info(\n                            f\"Last sheet {sheet_name} processed. End of the file reached.\"\n                        )\n                    else:\n                        self.logger.info(\n                            f\"End of the current sheet {sheet_name} reached. Switching to next sheet...\"\n                        )\n                    break  # Stop processing this sheet\n            else:\n                consecutive_empty_rows = 0  # Reset if we find a non-empty row\n\n                # **Process the entity block**\n                last_non_empty_row = self.get_last_non_empty_row(sheet, start_row)\n                if last_non_empty_row is None:\n                    break  # No more valid blocks\n\n                sheets_dict[normalized_sheet_name] = self.block_to_entity_dict(\n                    sheet,\n                    start_row,\n                    last_non_empty_row,\n                    sheets_dict[normalized_sheet_name],\n                )\n                has_content = True  # Found valid content\n\n                # Move to the next entity block\n                start_row = last_non_empty_row + 1\n                continue  # Continue loop without increasing consecutive_empty_rows\n\n            start_row += 1  # Move to the next row\n\n    # **If no sheets had content, return an empty dictionary**\n    if not has_content:\n        self.logger.warning(\n            \"No valid data found in any sheets. Returning empty dictionary.\"\n        )\n        return {}\n\n    return sheets_dict\n</code></pre>"},{"location":"references/api/#bam_masterdata.openbis.login","title":"<code>bam_masterdata.openbis.login</code>","text":""},{"location":"references/api/#bam_masterdata.openbis.login.ologin","title":"<code>ologin(url='')</code>","text":"<p>Connect to openBIS using the credentials stored in the environment variables.</p> <p>If an existing Openbis session is provided, the session is returned.</p> PARAMETER DESCRIPTION <code>url</code> <p>The URL of the openBIS instance. Defaults to the value of the <code>OPENBIS_URL</code> environment variable.</p> <p> TYPE: <code>(str, Openbis)</code> DEFAULT: <code>''</code> </p> RETURNS DESCRIPTION <code>Openbis</code> <p>Openbis object for the specific openBIS instance defined in <code>URL</code>.</p> <p> TYPE: <code>Openbis</code> </p> Source code in <code>bam_masterdata/openbis/login.py</code> <pre><code>def ologin(url: str | Openbis = \"\") -&gt; Openbis:\n    \"\"\"\n    Connect to openBIS using the credentials stored in the environment variables.\n\n    If an existing Openbis session is provided, the session is returned.\n\n    Args:\n        url (str, Openbis): The URL of the openBIS instance. Defaults to the value of the `OPENBIS_URL` environment variable.\n        An existing Openbis session can be provided.\n\n    Returns:\n        Openbis: Openbis object for the specific openBIS instance defined in `URL`.\n    \"\"\"\n    if not isinstance(url, Openbis):\n        o = Openbis(url)\n\n    if not o.is_session_activ():\n        o.login(\n            environ(\"OPENBIS_USERNAME\"),\n            environ(\"OPENBIS_PASSWORD\"),\n            save_token=True,\n        )\n\n    return o\n</code></pre>"},{"location":"references/api/#bam_masterdata.openbis.get_entities","title":"<code>bam_masterdata.openbis.get_entities</code>","text":""},{"location":"references/api/#bam_masterdata.openbis.get_entities.OpenbisEntities","title":"<code>OpenbisEntities</code>","text":"<p>Class to get openBIS entities and their attributes as dictionaries to be printed in the Python modules of <code>bam_masterdata/datamodel/</code>.</p> Source code in <code>bam_masterdata/openbis/get_entities.py</code> <pre><code>class OpenbisEntities:\n    \"\"\"\n    Class to get openBIS entities and their attributes as dictionaries to be printed in the\n    Python modules of `bam_masterdata/datamodel/`.\n    \"\"\"\n\n    def __init__(self, url: str = \"\"):\n        self.openbis = ologin(url=url)\n\n    def _get_formatted_dict(self, entity_name: str):\n        # entity_name is property_types, collection_types, dataset_types, object_types, or vocabularies\n        entity_types = getattr(self.openbis, f\"get_{entity_name}\")().df.to_dict(\n            orient=\"records\"\n        )\n        return {entry[\"code\"]: entry for entry in entity_types}\n\n    def _assign_properties(self, entity_name: str, formatted_dict: dict) -&gt; None:\n        for entity in getattr(self.openbis, f\"get_{entity_name}\")():\n            perm_id = entity.permId  # Unique identifier for the entity\n            assignments = entity.get_property_assignments()\n\n            if assignments:\n                # Convert property assignments to list of dictionaries\n                assignments_dict = assignments.df.to_dict(orient=\"records\")\n\n                # Create a dictionary of properties using the correct permId\n                properties = {}\n                for entry in assignments_dict:\n                    property_perm_id = self.openbis.get_property_type(\n                        entry.get(\"code\", {})\n                    ).permId\n                    if property_perm_id:\n                        # Include the desired property fields\n                        properties[property_perm_id] = {\n                            \"@type\": entry.get(\n                                \"@type\", \"as.dto.property.PropertyAssignment\"\n                            ),\n                            \"@id\": entry.get(\"@id\", None),\n                            \"fetchOptions\": entry.get(\"fetchOptions\", None),\n                            \"permId\": property_perm_id,\n                            \"section\": entry.get(\"section\", \"\"),\n                            \"ordinal\": entry.get(\"ordinal\", None),\n                            \"mandatory\": entry.get(\"mandatory\", False),\n                            \"showInEditView\": entry.get(\"showInEditView\", False),\n                            \"showRawValueInForms\": entry.get(\n                                \"showRawValueInForms\", False\n                            ),\n                            \"semanticAnnotations\": entry.get(\n                                \"semanticAnnotations\", None\n                            ),\n                            \"semanticAnnotationsInherited\": entry.get(\n                                \"semanticAnnotationsInherited\", False\n                            ),\n                            \"registrator\": entry.get(\"registrator\", None),\n                            \"registrationDate\": entry.get(\"registrationDate\", None),\n                            \"plugin\": entry.get(\"plugin\", \"\"),\n                        }\n                for prop in assignments:\n                    prop = prop.get_property_type()\n                    properties[prop.permId].update(\n                        {\n                            \"label\": prop.label,\n                            \"description\": prop.description,\n                            \"dataType\": prop.dataType,\n                        }\n                    )\n\n                # Add properties to the object type in formatted_dict\n                formatted_dict[perm_id][\"properties\"] = properties\n            else:\n                # If no properties, add an empty dictionary\n                formatted_dict[perm_id][\"properties\"] = {}\n\n    def get_property_dict(self) -&gt; dict:\n        \"\"\"\n        Get the property types from openBIS and return them as a dictionary where the keys\n        are the property type `code` and the value is a dictionary of attributes assigned to that\n        property type.\n\n        Returns:\n            dict: Dictionary of property types with their attributes.\n        \"\"\"\n        formatted_dict = self._get_formatted_dict(\"property_types\")\n\n        # We return the sorted dictionary in order to have a consistent order for inheritance\n        return dict(sorted(formatted_dict.items(), key=lambda item: item[0].count(\".\")))\n\n    def get_collection_dict(self) -&gt; dict:\n        \"\"\"\n        Get the collection types from openBIS and return them as a dictionary where the keys\n        are the collection type `code` and the value is a dictionary of attributes assigned to that\n        collection type.\n\n        Returns:\n            dict: Dictionary of collection types with their attributes.\n        \"\"\"\n        formatted_dict = self._get_formatted_dict(\"collection_types\")\n        self._assign_properties(\n            entity_name=\"collection_types\", formatted_dict=formatted_dict\n        )\n\n        # We return the sorted dictionary in order to have a consistent order for inheritance\n        return dict(sorted(formatted_dict.items(), key=lambda item: item[0].count(\".\")))\n\n    def get_dataset_dict(self) -&gt; dict:\n        \"\"\"\n        Get the dataset types from openBIS and return them as a dictionary where the keys\n        are the dataset type `code` and the value is a dictionary of attributes assigned to that\n        dataset type.\n\n        Returns:\n            dict: Dictionary of dataset types with their attributes.\n        \"\"\"\n        formatted_dict = self._get_formatted_dict(\"dataset_types\")\n        self._assign_properties(\n            entity_name=\"dataset_types\", formatted_dict=formatted_dict\n        )\n\n        # We return the sorted dictionary in order to have a consistent order for inheritance\n        return dict(sorted(formatted_dict.items(), key=lambda item: item[0].count(\".\")))\n\n    def get_object_dict(self) -&gt; dict:\n        \"\"\"\n        Get the object types from openBIS and return them as a dictionary where the keys\n        are the object type `code` and the value is a dictionary of attributes assigned to that\n        object type.\n\n        Returns:\n            dict: Dictionary of object types with their attributes.\n        \"\"\"\n        formatted_dict = self._get_formatted_dict(\"object_types\")\n        self._assign_properties(\n            entity_name=\"object_types\", formatted_dict=formatted_dict\n        )\n\n        # We return the sorted dictionary in order to have a consistent order for inheritance\n        return dict(sorted(formatted_dict.items(), key=lambda item: item[0].count(\".\")))\n\n    def get_vocabulary_dict(self) -&gt; dict:\n        \"\"\"\n        Get the vocabulary types from openBIS and return them as a dictionary where the keys\n        are the vocabulary type `code` and the value is a dictionary of attributes assigned to that\n        vocabulary type.\n\n        Returns:\n            dict: Dictionary of vocabulary types with their attributes.\n        \"\"\"\n        formatted_dict = self._get_formatted_dict(\"vocabularies\")\n\n        # Add properties to each object type\n        for voc in self.openbis.get_vocabularies():\n            code = voc.code  # Unique identifier for the object type\n\n            # BAM_FLOOR, BAM_HOUSE, BAM_LOCATION, BAM_LOCATION_COMPLETE, BAM_OE, BAM_ROOM, PERSON_STATUS\n            # are not exported due to containing sensitive information\n            if code in [\n                \"BAM_FLOOR\",\n                \"BAM_HOUSE\",\n                \"BAM_LOCATION\",\n                \"BAM_LOCATION_COMPLETE\",\n                \"BAM_OE\",\n                \"BAM_ROOM\",\n                \"PERSON_STATUS\",\n            ]:\n                continue\n            terms = voc.get_terms()\n\n            if terms:\n                # Convert property assignments to list of dictionaries\n                terms_dict = terms.df.to_dict(orient=\"records\")\n\n                # Create a dictionary of properties using the correct permId\n                voc_terms = {}\n                for entry in terms_dict:\n                    term_code = entry.get(\"code\", {})\n                    if term_code:\n                        # Include the desired property fields\n                        voc_terms[term_code] = {\n                            \"code\": term_code,\n                            \"description\": entry.get(\"description\", \"\"),\n                            \"label\": entry.get(\"label\", \"\"),\n                        }\n\n                # Add properties to the object type in formatted_dict\n                formatted_dict[code][\"terms\"] = voc_terms\n            else:\n                # If no properties, add an empty dictionary\n                formatted_dict[code][\"terms\"] = {}\n\n        # We return the sorted dictionary in order to have a consistent order for inheritance\n        return dict(sorted(formatted_dict.items(), key=lambda item: item[0].count(\".\")))\n</code></pre>"},{"location":"references/api/#bam_masterdata.openbis.get_entities.OpenbisEntities.openbis","title":"<code>openbis = ologin(url=url)</code>","text":""},{"location":"references/api/#bam_masterdata.openbis.get_entities.OpenbisEntities.__init__","title":"<code>__init__(url='')</code>","text":"Source code in <code>bam_masterdata/openbis/get_entities.py</code> <pre><code>def __init__(self, url: str = \"\"):\n    self.openbis = ologin(url=url)\n</code></pre>"},{"location":"references/api/#bam_masterdata.openbis.get_entities.OpenbisEntities.get_property_dict","title":"<code>get_property_dict()</code>","text":"<p>Get the property types from openBIS and return them as a dictionary where the keys are the property type <code>code</code> and the value is a dictionary of attributes assigned to that property type.</p> RETURNS DESCRIPTION <code>dict</code> <p>Dictionary of property types with their attributes.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>bam_masterdata/openbis/get_entities.py</code> <pre><code>def get_property_dict(self) -&gt; dict:\n    \"\"\"\n    Get the property types from openBIS and return them as a dictionary where the keys\n    are the property type `code` and the value is a dictionary of attributes assigned to that\n    property type.\n\n    Returns:\n        dict: Dictionary of property types with their attributes.\n    \"\"\"\n    formatted_dict = self._get_formatted_dict(\"property_types\")\n\n    # We return the sorted dictionary in order to have a consistent order for inheritance\n    return dict(sorted(formatted_dict.items(), key=lambda item: item[0].count(\".\")))\n</code></pre>"},{"location":"references/api/#bam_masterdata.openbis.get_entities.OpenbisEntities.get_collection_dict","title":"<code>get_collection_dict()</code>","text":"<p>Get the collection types from openBIS and return them as a dictionary where the keys are the collection type <code>code</code> and the value is a dictionary of attributes assigned to that collection type.</p> RETURNS DESCRIPTION <code>dict</code> <p>Dictionary of collection types with their attributes.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>bam_masterdata/openbis/get_entities.py</code> <pre><code>def get_collection_dict(self) -&gt; dict:\n    \"\"\"\n    Get the collection types from openBIS and return them as a dictionary where the keys\n    are the collection type `code` and the value is a dictionary of attributes assigned to that\n    collection type.\n\n    Returns:\n        dict: Dictionary of collection types with their attributes.\n    \"\"\"\n    formatted_dict = self._get_formatted_dict(\"collection_types\")\n    self._assign_properties(\n        entity_name=\"collection_types\", formatted_dict=formatted_dict\n    )\n\n    # We return the sorted dictionary in order to have a consistent order for inheritance\n    return dict(sorted(formatted_dict.items(), key=lambda item: item[0].count(\".\")))\n</code></pre>"},{"location":"references/api/#bam_masterdata.openbis.get_entities.OpenbisEntities.get_dataset_dict","title":"<code>get_dataset_dict()</code>","text":"<p>Get the dataset types from openBIS and return them as a dictionary where the keys are the dataset type <code>code</code> and the value is a dictionary of attributes assigned to that dataset type.</p> RETURNS DESCRIPTION <code>dict</code> <p>Dictionary of dataset types with their attributes.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>bam_masterdata/openbis/get_entities.py</code> <pre><code>def get_dataset_dict(self) -&gt; dict:\n    \"\"\"\n    Get the dataset types from openBIS and return them as a dictionary where the keys\n    are the dataset type `code` and the value is a dictionary of attributes assigned to that\n    dataset type.\n\n    Returns:\n        dict: Dictionary of dataset types with their attributes.\n    \"\"\"\n    formatted_dict = self._get_formatted_dict(\"dataset_types\")\n    self._assign_properties(\n        entity_name=\"dataset_types\", formatted_dict=formatted_dict\n    )\n\n    # We return the sorted dictionary in order to have a consistent order for inheritance\n    return dict(sorted(formatted_dict.items(), key=lambda item: item[0].count(\".\")))\n</code></pre>"},{"location":"references/api/#bam_masterdata.openbis.get_entities.OpenbisEntities.get_object_dict","title":"<code>get_object_dict()</code>","text":"<p>Get the object types from openBIS and return them as a dictionary where the keys are the object type <code>code</code> and the value is a dictionary of attributes assigned to that object type.</p> RETURNS DESCRIPTION <code>dict</code> <p>Dictionary of object types with their attributes.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>bam_masterdata/openbis/get_entities.py</code> <pre><code>def get_object_dict(self) -&gt; dict:\n    \"\"\"\n    Get the object types from openBIS and return them as a dictionary where the keys\n    are the object type `code` and the value is a dictionary of attributes assigned to that\n    object type.\n\n    Returns:\n        dict: Dictionary of object types with their attributes.\n    \"\"\"\n    formatted_dict = self._get_formatted_dict(\"object_types\")\n    self._assign_properties(\n        entity_name=\"object_types\", formatted_dict=formatted_dict\n    )\n\n    # We return the sorted dictionary in order to have a consistent order for inheritance\n    return dict(sorted(formatted_dict.items(), key=lambda item: item[0].count(\".\")))\n</code></pre>"},{"location":"references/api/#bam_masterdata.openbis.get_entities.OpenbisEntities.get_vocabulary_dict","title":"<code>get_vocabulary_dict()</code>","text":"<p>Get the vocabulary types from openBIS and return them as a dictionary where the keys are the vocabulary type <code>code</code> and the value is a dictionary of attributes assigned to that vocabulary type.</p> RETURNS DESCRIPTION <code>dict</code> <p>Dictionary of vocabulary types with their attributes.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>bam_masterdata/openbis/get_entities.py</code> <pre><code>def get_vocabulary_dict(self) -&gt; dict:\n    \"\"\"\n    Get the vocabulary types from openBIS and return them as a dictionary where the keys\n    are the vocabulary type `code` and the value is a dictionary of attributes assigned to that\n    vocabulary type.\n\n    Returns:\n        dict: Dictionary of vocabulary types with their attributes.\n    \"\"\"\n    formatted_dict = self._get_formatted_dict(\"vocabularies\")\n\n    # Add properties to each object type\n    for voc in self.openbis.get_vocabularies():\n        code = voc.code  # Unique identifier for the object type\n\n        # BAM_FLOOR, BAM_HOUSE, BAM_LOCATION, BAM_LOCATION_COMPLETE, BAM_OE, BAM_ROOM, PERSON_STATUS\n        # are not exported due to containing sensitive information\n        if code in [\n            \"BAM_FLOOR\",\n            \"BAM_HOUSE\",\n            \"BAM_LOCATION\",\n            \"BAM_LOCATION_COMPLETE\",\n            \"BAM_OE\",\n            \"BAM_ROOM\",\n            \"PERSON_STATUS\",\n        ]:\n            continue\n        terms = voc.get_terms()\n\n        if terms:\n            # Convert property assignments to list of dictionaries\n            terms_dict = terms.df.to_dict(orient=\"records\")\n\n            # Create a dictionary of properties using the correct permId\n            voc_terms = {}\n            for entry in terms_dict:\n                term_code = entry.get(\"code\", {})\n                if term_code:\n                    # Include the desired property fields\n                    voc_terms[term_code] = {\n                        \"code\": term_code,\n                        \"description\": entry.get(\"description\", \"\"),\n                        \"label\": entry.get(\"label\", \"\"),\n                    }\n\n            # Add properties to the object type in formatted_dict\n            formatted_dict[code][\"terms\"] = voc_terms\n        else:\n            # If no properties, add an empty dictionary\n            formatted_dict[code][\"terms\"] = {}\n\n    # We return the sorted dictionary in order to have a consistent order for inheritance\n    return dict(sorted(formatted_dict.items(), key=lambda item: item[0].count(\".\")))\n</code></pre>"},{"location":"references/api/#bam_masterdata.checker.checker","title":"<code>bam_masterdata.checker.checker</code>","text":""},{"location":"references/api/#bam_masterdata.checker.checker.MasterdataChecker","title":"<code>MasterdataChecker</code>","text":"Source code in <code>bam_masterdata/checker/checker.py</code> <pre><code>class MasterdataChecker:\n    VALID_MODES = {\"self\", \"incoming\", \"validate\", \"compare\", \"all\", \"individual\"}\n\n    def __init__(self):\n        \"\"\"\n        Initialize the comparator with validation rules and set the datamodel directory.\n        \"\"\"\n        self.current_model: dict = None\n        self.new_entities: dict = None\n        self.logger = logger\n        self.validation_rules: dict = {}\n\n    def load_current_model(self, datamodel_dir: str = \"./bam_masterdata/datamodel/\"):\n        \"\"\"\n        Load and transform the current data model (Pydantic classes) into JSON.\n\n        Uses the default datamodel directory unless overridden.\n        \"\"\"\n        self.logger.info(f\"Loading current data model from: {datamodel_dir}\")\n        entities_dict = EntitiesDict(python_path=datamodel_dir, logger=self.logger)\n        self.current_model = entities_dict.single_json()\n\n    def load_new_entities(self, source: str):\n        \"\"\"\n        Load new entities from various sources (Python classes, Excel, etc.).\n        \"\"\"\n        self.logger.info(f\"Loading new entities from: {source}\")\n        loader = SourceLoader(source)\n        self.new_entities = loader.load()\n\n    def check(self, mode: str = \"all\") -&gt; dict:\n        \"\"\"\n        Run validations.\n\n        Modes:\n        - \"self\" -&gt; Validate only the current data model.\n        - \"incoming\" -&gt; Validate only the new entity structure.\n        - \"validate\" -&gt; Validate both the current model and new entities.\n        - \"compare\" -&gt; Compare new entities against the current model.\n        - \"all\" -&gt; Run both validation types.\n        - \"individual\" -&gt; Run individual repositories validations.\n\n        Before running, ensure that required models are loaded based on the mode.\n\n        Returns:\n            dict: Validation results.\n        \"\"\"\n        # Validate mode selection\n        if mode not in self.VALID_MODES:\n            raise ValueError(f\"Invalid mode: {mode}. Choose from {self.VALID_MODES}.\")\n\n        # Load required models based on the selected mode\n        if (\n            mode in [\"self\", \"validate\", \"compare\", \"all\", \"individual\"]\n            and self.current_model is None\n        ):\n            self.logger.info(\"Current model is missing. Loading it from local files.\")\n            self.load_current_model()\n\n        if (\n            mode in [\"incoming\", \"validate\", \"compare\", \"all\", \"individual\"]\n            and self.new_entities is None\n        ):\n            raise ValueError(\n                \"New entities must be loaded before validation in 'incoming', 'validate', 'individual', 'compare', or 'all' modes.\"\n            )\n\n        # Load the validation rules\n        if (\n            mode in [\"self\", \"incoming\", \"validate\", \"all\", \"individual\"]\n            and self.validation_rules == {}\n        ):\n            self.validation_rules = load_validation_rules(self.logger)\n\n        validator = MasterdataValidator(\n            self.new_entities, self.current_model, self.validation_rules\n        )\n        return validator.validate(mode)\n</code></pre>"},{"location":"references/api/#bam_masterdata.checker.checker.MasterdataChecker.VALID_MODES","title":"<code>VALID_MODES = {'self', 'incoming', 'validate', 'compare', 'all', 'individual'}</code>","text":""},{"location":"references/api/#bam_masterdata.checker.checker.MasterdataChecker.current_model","title":"<code>current_model = None</code>","text":""},{"location":"references/api/#bam_masterdata.checker.checker.MasterdataChecker.new_entities","title":"<code>new_entities = None</code>","text":""},{"location":"references/api/#bam_masterdata.checker.checker.MasterdataChecker.logger","title":"<code>logger = logger</code>","text":""},{"location":"references/api/#bam_masterdata.checker.checker.MasterdataChecker.validation_rules","title":"<code>validation_rules = {}</code>","text":""},{"location":"references/api/#bam_masterdata.checker.checker.MasterdataChecker.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the comparator with validation rules and set the datamodel directory.</p> Source code in <code>bam_masterdata/checker/checker.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initialize the comparator with validation rules and set the datamodel directory.\n    \"\"\"\n    self.current_model: dict = None\n    self.new_entities: dict = None\n    self.logger = logger\n    self.validation_rules: dict = {}\n</code></pre>"},{"location":"references/api/#bam_masterdata.checker.checker.MasterdataChecker.load_current_model","title":"<code>load_current_model(datamodel_dir='./bam_masterdata/datamodel/')</code>","text":"<p>Load and transform the current data model (Pydantic classes) into JSON.</p> <p>Uses the default datamodel directory unless overridden.</p> Source code in <code>bam_masterdata/checker/checker.py</code> <pre><code>def load_current_model(self, datamodel_dir: str = \"./bam_masterdata/datamodel/\"):\n    \"\"\"\n    Load and transform the current data model (Pydantic classes) into JSON.\n\n    Uses the default datamodel directory unless overridden.\n    \"\"\"\n    self.logger.info(f\"Loading current data model from: {datamodel_dir}\")\n    entities_dict = EntitiesDict(python_path=datamodel_dir, logger=self.logger)\n    self.current_model = entities_dict.single_json()\n</code></pre>"},{"location":"references/api/#bam_masterdata.checker.checker.MasterdataChecker.load_new_entities","title":"<code>load_new_entities(source)</code>","text":"<p>Load new entities from various sources (Python classes, Excel, etc.).</p> Source code in <code>bam_masterdata/checker/checker.py</code> <pre><code>def load_new_entities(self, source: str):\n    \"\"\"\n    Load new entities from various sources (Python classes, Excel, etc.).\n    \"\"\"\n    self.logger.info(f\"Loading new entities from: {source}\")\n    loader = SourceLoader(source)\n    self.new_entities = loader.load()\n</code></pre>"},{"location":"references/api/#bam_masterdata.checker.checker.MasterdataChecker.check","title":"<code>check(mode='all')</code>","text":"<p>Run validations.</p> <p>Modes: - \"self\" -&gt; Validate only the current data model. - \"incoming\" -&gt; Validate only the new entity structure. - \"validate\" -&gt; Validate both the current model and new entities. - \"compare\" -&gt; Compare new entities against the current model. - \"all\" -&gt; Run both validation types. - \"individual\" -&gt; Run individual repositories validations.</p> <p>Before running, ensure that required models are loaded based on the mode.</p> RETURNS DESCRIPTION <code>dict</code> <p>Validation results.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>bam_masterdata/checker/checker.py</code> <pre><code>def check(self, mode: str = \"all\") -&gt; dict:\n    \"\"\"\n    Run validations.\n\n    Modes:\n    - \"self\" -&gt; Validate only the current data model.\n    - \"incoming\" -&gt; Validate only the new entity structure.\n    - \"validate\" -&gt; Validate both the current model and new entities.\n    - \"compare\" -&gt; Compare new entities against the current model.\n    - \"all\" -&gt; Run both validation types.\n    - \"individual\" -&gt; Run individual repositories validations.\n\n    Before running, ensure that required models are loaded based on the mode.\n\n    Returns:\n        dict: Validation results.\n    \"\"\"\n    # Validate mode selection\n    if mode not in self.VALID_MODES:\n        raise ValueError(f\"Invalid mode: {mode}. Choose from {self.VALID_MODES}.\")\n\n    # Load required models based on the selected mode\n    if (\n        mode in [\"self\", \"validate\", \"compare\", \"all\", \"individual\"]\n        and self.current_model is None\n    ):\n        self.logger.info(\"Current model is missing. Loading it from local files.\")\n        self.load_current_model()\n\n    if (\n        mode in [\"incoming\", \"validate\", \"compare\", \"all\", \"individual\"]\n        and self.new_entities is None\n    ):\n        raise ValueError(\n            \"New entities must be loaded before validation in 'incoming', 'validate', 'individual', 'compare', or 'all' modes.\"\n        )\n\n    # Load the validation rules\n    if (\n        mode in [\"self\", \"incoming\", \"validate\", \"all\", \"individual\"]\n        and self.validation_rules == {}\n    ):\n        self.validation_rules = load_validation_rules(self.logger)\n\n    validator = MasterdataValidator(\n        self.new_entities, self.current_model, self.validation_rules\n    )\n    return validator.validate(mode)\n</code></pre>"},{"location":"references/api/#bam_masterdata.checker.masterdata_validator","title":"<code>bam_masterdata.checker.masterdata_validator</code>","text":""},{"location":"references/api/#bam_masterdata.checker.masterdata_validator.MasterdataValidator","title":"<code>MasterdataValidator</code>","text":"Source code in <code>bam_masterdata/checker/masterdata_validator.py</code> <pre><code>class MasterdataValidator:\n    def __init__(self, new_entities: dict, current_model: dict, validation_rules: dict):\n        \"\"\"\n        Initialize the validator with new and current entity data.\n\n        Args:\n            new_entities (dict): The incoming datamodel.\n            current_model (dict): The existing datamodel.\n            validation_rules (dict): The validation rules to apply.\n        \"\"\"\n        self.new_entities = new_entities\n        self.current_model = current_model\n        self.validation_rules = validation_rules\n        self.logger = logger\n        self.log_msgs: list = []\n        self.validation_results: dict = {}\n\n    def validate(self, mode: str = \"all\") -&gt; dict:\n        \"\"\"\n        Run validations based on mode:\n        - \"self\": Validate current model structure and format.\n        - \"incoming\": Validate new entities structure and format.\n        - \"validate\": Validate both current and incoming models but do not compare.\n        - \"compare\": Validate new entities against the current model.\n        - \"all\": Run both.\n        - \"individual\": Validate new entities and compare them with the current model.\n\n         Returns:\n            dict: Validation results.\n        \"\"\"\n        self.logger.debug(\"Starting validation process...\", mode=mode)\n\n        # Reset validation results before running checks\n        self.validation_results = {\n            \"current_model\": {},\n            \"incoming_model\": {},\n            \"comparisons\": {},\n        }\n\n        if mode in [\"self\", \"all\", \"validate\"]:\n            self.logger.debug(\"Validating current model...\")\n            self._validate_model(self.current_model)\n            self._extract_log_messages(\n                self.current_model, self.validation_results[\"current_model\"]\n            )\n\n        if mode in [\"incoming\", \"all\", \"validate\"]:\n            self.logger.debug(\"Validating new entities...\")\n            self._validate_model(self.new_entities)\n            self._extract_log_messages(\n                self.new_entities, self.validation_results[\"incoming_model\"]\n            )\n\n        if mode in [\"compare\", \"all\"]:\n            self.logger.debug(\"Comparing new entities with current model...\")\n            self._compare_with_current_model(mode=mode)\n            self._extract_log_messages(\n                self.new_entities, self.validation_results[\"comparisons\"]\n            )\n\n        if mode == \"individual\":\n            self.logger.debug(\n                \"Validating new entities and comparing them with current model...\"\n            )\n            self.validation_results = {\n                \"incoming_model\": {},\n                \"comparisons\": {},\n            }\n            self._validate_model(self.new_entities)\n            self._extract_log_messages(\n                self.new_entities, self.validation_results[\"incoming_model\"]\n            )\n            self._compare_with_current_model(mode=\"individual\")\n            self._extract_log_messages(\n                self.new_entities, self.validation_results[\"comparisons\"]\n            )\n\n        return self.validation_results\n\n    def _validate_model(self, model: dict) -&gt; dict:\n        \"\"\"\n        Validate the given datamodel against the validation rules.\n\n        Args:\n            model (dict): The datamodel to validate.\n\n        Returns:\n            dict: A dictionary containing ...\n        \"\"\"\n        for entity_type, entities in model.items():\n            for entity_name, entity_data in entities.items():\n                entity_id = entity_data.get(\"defs\", {}).get(\"code\", entity_name)\n\n                # Ensure _log_msgs exists\n                if \"_log_msgs\" not in entity_data:\n                    entity_data[\"_log_msgs\"] = []\n\n                if model == self.new_entities:\n                    self.logger.info(f\"Validating {entity_type} -&gt; {entity_id}\")\n\n                # Validate 'defs'\n                if \"defs\" in entity_data:\n                    row_location = entity_data[\"defs\"].get(\"row_location\", \"Unknown\")\n                    self._validate_fields(\n                        entity_data[\"defs\"],\n                        \"defs_validation\",\n                        entity_type,\n                        entity_id,\n                        row_location,\n                        entity_data,\n                    )\n\n                # Collect ordered sections for each entity\n                entity_sections = []\n                # Validate 'properties' (except for vocabulary_types, which uses 'terms')\n                if (\n                    entity_type != \"vocabulary_types\"\n                    and entity_type != \"vocabulary_type\"\n                ) and \"properties\" in entity_data:\n                    for prop in entity_data[\"properties\"]:\n                        row_location = prop.get(\"row_location\", \"Unknown\")\n\n                        # Collect section names in order\n                        section = prop.get(\"section\", \"\").strip()\n                        if section:  # Avoid empty sections\n                            entity_sections.append(\n                                {\n                                    \"code\": prop[\"code\"],\n                                    \"section\": section,\n                                    \"row_location\": row_location,\n                                }\n                            )\n\n                        # Check for deprecated `$ANNOTATIONS_STATE`\n                        if (\n                            prop[\"code\"] == \"$ANNOTATIONS_STATE\"\n                            and model == self.new_entities\n                        ):\n                            log_message = (\n                                f\"Property $ANNOTATIONS_STATE is deprecated from openBIS 20.10.7.3. \"\n                                f\"Assigned to entity '{entity_id}' at row {row_location}.\"\n                            )\n                            store_log_message(\n                                logger, entity_data, log_message, level=\"warning\"\n                            )\n\n                        self._validate_fields(\n                            prop,\n                            \"properties_validation\",\n                            entity_type,\n                            entity_id,\n                            row_location,\n                            entity_data,\n                        )\n\n                # TODO: revise if these checks about ordering of sections are truly necessary\n                # Check if \"Additional Information\" is followed only by \"Additional Information\" or \"Comments\"\n                for i in range(len(entity_sections) - 1):\n                    current_section = entity_sections[i][\"section\"]\n                    next_section = entity_sections[i + 1][\"section\"]\n                    row_location = entity_sections[i + 1][\"row_location\"]\n\n                    if (\n                        current_section == \"Additional Information\"\n                        and next_section not in {\"Additional Information\", \"Comments\"}\n                    ):\n                        log_message = (\n                            f\"Invalid section order: 'Additional Information' at row {entity_sections[i]['row_location']} \"\n                            f\"must be followed by 'Comments', but found '{next_section}' at row {row_location}.\"\n                        )\n                        store_log_message(\n                            logger, entity_data, log_message, level=\"error\"\n                        )\n\n                # Check if required properties exist in specific sections\n                required_properties = {\n                    \"Additional Information\": [\"notes\"],\n                    \"Comments\": [\"comments\", \"xmlcomments\", \"$xmlcomments\"],\n                }\n\n                # Track found properties\n                found_properties = {section: False for section in required_properties}\n\n                for entry in entity_sections:\n                    section = entry[\"section\"]\n                    property_code = entry[\"code\"]\n                    row_location = entry[\"row_location\"]\n\n                    # Check if this section is one we need to validate\n                    if section in required_properties:\n                        # Perform a case-insensitive check against the list of allowed property codes\n                        if property_code.lower() in required_properties[section]:\n                            found_properties[section] = True\n\n                # Log errors for missing required properties\n                for section, prop in required_properties.items():\n                    if (\n                        any(entry[\"section\"] == section for entry in entity_sections)\n                        and not found_properties[section]\n                    ):\n                        log_message = f\"Missing required property '{prop}' in section '{section}'.\"\n                        store_log_message(\n                            logger, entity_data, log_message, level=\"error\"\n                        )\n\n                # Validate 'terms' (only for vocabulary_types)\n                if (\n                    entity_type in [\"vocabulary_types\", \"vocabulary_type\"]\n                    and \"terms\" in entity_data\n                ):\n                    for term in entity_data[\"terms\"]:\n                        row_location = term.get(\"row_location\", \"Unknown\")\n                        self._validate_fields(\n                            term,\n                            \"terms_validation\",\n                            entity_type,\n                            entity_id,\n                            row_location,\n                            entity_data,\n                        )\n\n        return entity_data\n\n    def _validate_fields(\n        self,\n        data: dict,\n        rule_type: str,\n        entity_type: str,\n        entity_name: str,\n        row_location: str,\n        parent_entity: dict,\n    ):\n        \"\"\"\n        Validate a dictionary of fields against the corresponding validation rules.\n\n        Args:\n            data (dict): The fields to validate.\n            rule_type (str): The rule section to use (\"defs_validation\", \"properties_validation\", or \"terms_validation\").\n            entity_type (str): The entity type being validated.\n            entity_name (str): The specific entity name (ID if available).\n            row_location (str): The row where the entity is located in the source file.\n            parent_entity (dict): The entity dictionary where _log_msgs should be stored.\n        \"\"\"\n\n        # Determine where the issue is occurring (in properties, terms, or main entity fields)\n        extra_location = {\n            \"properties_validation\": \"in 'properties'.\",\n            \"terms_validation\": \"in 'terms'.\",\n        }.get(rule_type, \".\")\n\n        for field, value in data.items():\n            rule = self.validation_rules.get(rule_type, {}).get(field)\n\n            extra_location_str = f\" {extra_location} \" if extra_location else \" \"\n\n            log_message = (\n                f\"Invalid '{value}' value found in the '{field}' field at line {row_location} \"\n                f\"in entity '{entity_name}' of '{entity_type}'{extra_location_str}\"\n            )\n\n            if not rule:\n                continue  # Skip fields with no validation rules\n\n            # Handle empty fields\n            if \"allow_empty\" in rule and (value is None or value == \"\" or not value):\n                continue  # Skip check if empty fields are allowed\n\n            # Validate pattern (regex)\n            if \"pattern\" in rule and value is not None:\n                if not re.match(rule[\"pattern\"], str(value)):\n                    log_message = f\"{log_message}Invalid format.\"\n                    level = \"error\"\n                    if \"is_description\" in rule:\n                        log_message = f\"{log_message} Description should follow the schema: English Description + '//' + German Description. \"\n                        level = \"warning\"\n                    if \"is_section\" in rule:\n                        log_message = f\"{log_message} First letter of every word starts with capitalized lettter.\"\n                        level = \"warning\"\n                    store_log_message(logger, parent_entity, log_message, level=level)\n\n            # Validate boolean fields\n            if \"is_bool\" in rule and str(value).strip().lower() not in [\n                \"true\",\n                \"false\",\n            ]:\n                store_log_message(\n                    logger,\n                    parent_entity,\n                    f\"{log_message}Expected a boolean.\",\n                    level=\"error\",\n                )\n\n            # Validate data types\n            if \"is_data\" in rule:\n                str_val = str(value)\n                is_valid_standard = str_val in [dt.value for dt in DataType]\n                is_valid_dynamic = False\n\n                if not is_valid_standard and (\n                    str_val.startswith(\"OBJECT\") or str_val.startswith(\"SAMPLE\")\n                ):\n                    els = str_val.split(\":\")\n\n                    if len(els) == 2 and els[1].strip():\n                        is_valid_dynamic = True\n\n                if not is_valid_standard and not is_valid_dynamic:\n                    store_log_message(\n                        logger,\n                        parent_entity,\n                        f\"{log_message}The Data Type should be one of the following: {[dt.value for dt in DataType]} or follow the format 'SAMPLE:&lt;CODE&gt;' or 'OBJECT:&lt;CODE&gt;'\",\n                        level=\"error\",\n                    )\n\n            # Validate special cases (e.g., extra validation functions)\n            if \"extra_validation\" in rule:\n                validation_func = getattr(self, rule[\"extra_validation\"], None)\n                if validation_func == \"is_reduced_version\" and not is_reduced_version(\n                    value, entity_name\n                ):\n                    store_log_message(\n                        logger,\n                        parent_entity,\n                        f\"{log_message}The generated code should be a part of the code.\",\n                        level=\"warning\",\n                    )\n\n    def _compare_with_current_model(self, mode) -&gt; dict:\n        \"\"\"\n        Compare new entities against the current model using validation rules.\n        \"\"\"\n        self.logger.debug(\"Starting comparison with the current model...\")\n\n        new_entity = False\n\n        all_props = self.extract_property_codes(self.current_model)\n\n        for entity_type, incoming_entities in self.new_entities.items():\n            if entity_type not in self.current_model:\n                continue  # Skip if entity type does not exist in the current model\n\n            current_entities = self.current_model[entity_type]\n\n            for entity_code, incoming_entity in incoming_entities.items():\n                incoming_row_location = \"Unknown\"\n                current_entity = current_entities.get(entity_code)\n\n                # Ensure _log_msgs exists\n                if \"_log_msgs\" not in incoming_entity:\n                    incoming_entity[\"_log_msgs\"] = []\n\n                if current_entity:\n                    if mode == \"individual\":\n                        log_message = f\"The entity {entity_code} already exists in `bam-masterdata`. Please, check your classes. \"\n                        store_log_message(\n                            logger, incoming_entity, log_message, level=\"critical\"\n                        )\n                    # Compare general attributes for all entities\n                    for key, new_value in incoming_entity.get(\"defs\", {}).items():\n                        incoming_row_location = incoming_entity.get(\"defs\", {}).get(\n                            \"row_location\", \"Unknown\"\n                        )\n                        old_value = current_entity.get(\"defs\", {}).get(key)\n                        if (\n                            (key != \"code\" and key != \"row_location\")\n                            and old_value is not None\n                            and new_value != old_value\n                        ):\n                            log_message = (\n                                f\"Entity type {entity_code} has changed its attribute {key} \"\n                                f\"from '{old_value}' to '{new_value}' at row {incoming_row_location}.\"\n                            )\n                            store_log_message(\n                                logger, incoming_entity, log_message, level=\"warning\"\n                            )\n\n                    # Special case for `property_types`\n                    if entity_type in (\"property_types\", \"property_type\"):\n                        incoming_row_location = incoming_entity.get(\n                            \"row_location\", \"Unknown\"\n                        )\n                        new_data_type = incoming_entity.get(\"data_type\")\n                        old_data_type = current_entity.get(\"data_type\")\n\n                        if (\n                            new_data_type\n                            and old_data_type\n                            and new_data_type != old_data_type\n                        ):\n                            log_message = (\n                                f\"Property type {entity_code} has changed its `data_type` value from {old_data_type} to {new_data_type} at row {incoming_row_location}. \"\n                                \"This will cause that data using the Property with inconsistent versions of data type will probably break openBIS. \"\n                                \"You need to define a new property with the new data type or revise your data model.\"\n                            )\n                            store_log_message(\n                                logger, incoming_entity, log_message, level=\"critical\"\n                            )\n\n                        if (\n                            new_data_type == \"CONTROLLEDVOCABULARY\"\n                            and incoming_entity.get(\"vocabulary_code\")\n                            != current_entity.get(\"vocabulary_code\")\n                        ):\n                            old_vocabulary = current_entity.get(\"vocabulary_code\")\n                            new_vocabulary = incoming_entity.get(\"vocabulary_code\")\n                            log_message = (\n                                f\"Property type {entity_code} using controlled vocabulary has changed its `vocabulary_code` value from {old_vocabulary} to {new_vocabulary}, \"\n                                f\"at row {incoming_row_location} which means that data using a type that is not compatible with the new type will probably break openBIS. \"\n                                \"You need to define a new property with the new data type or revise your data model.\"\n                            )\n                            store_log_message(\n                                logger, incoming_entity, log_message, level=\"critical\"\n                            )\n\n                else:\n                    new_entity = True\n\n                # Compare assigned properties or terms\n                if \"properties\" in incoming_entity:\n                    self._compare_assigned_properties(\n                        entity_code,\n                        incoming_entity,\n                        current_entity,\n                        entity_type,\n                        new_entity,\n                        incoming_row_location,\n                        all_props,\n                    )\n                elif \"terms\" in incoming_entity:\n                    self._compare_assigned_properties(\n                        entity_code,\n                        incoming_entity,\n                        current_entity,\n                        entity_type,\n                        new_entity,\n                        incoming_row_location,\n                        all_props,\n                        is_terms=True,\n                    )\n\n        if not self.validation_results.get(\"comparisons\"):\n            logger.info(\n                \"No critical conflicts found between new entities compared to the current model.\"\n            )\n\n        return self.validation_results\n\n    def _compare_assigned_properties(\n        self,\n        entity_code,\n        incoming_entity,\n        current_entity,\n        entity_type,\n        new_entity,\n        incoming_row_location,\n        all_props,\n        is_terms=False,\n    ):\n        \"\"\"\n        Compares assigned properties (for ObjectType, CollectionType, etc.) or terms (for VocabularyType).\n        \"\"\"\n        incoming_props = {\n            prop[\"code\"]: prop\n            for prop in incoming_entity.get(\n                \"properties\" if not is_terms else \"terms\", []\n            )\n        }\n\n        incoming_prop_codes = set(incoming_props.keys())\n\n        if not new_entity:\n            current_props = {\n                prop[\"code\"]: prop\n                for prop in current_entity.get(\n                    \"properties\" if not is_terms else \"terms\", []\n                )\n            }\n\n            # Check for non-existing assigned properties\n            current_prop_codes = set(current_props.keys())\n\n            for prop_code in incoming_prop_codes:\n                if prop_code not in all_props and is_terms is False:\n                    log_message = (\n                        f\"The assigned property {prop_code} to the entity {entity_code} at row {incoming_props[prop_code].get('row_location')} does not exist in openBIS. \"\n                        \"Please, define it in your PropertyType section.\"\n                    )\n                    store_log_message(\n                        logger, incoming_entity, log_message, level=\"error\"\n                    )\n\n            # Check for existing changes in assigned properties\n            missing_properties = incoming_prop_codes - current_prop_codes\n            deleted_properties = current_prop_codes - incoming_prop_codes\n\n            if missing_properties or deleted_properties:\n                log_message = f\"The assigned properties to {entity_code} at row {incoming_row_location} have changed:\"\n                store_log_message(logger, incoming_entity, log_message, level=\"warning\")\n\n            # Check for missing properties\n            for missing in missing_properties:\n                log_message = f\"{missing} has been added as a new property at row {incoming_props[missing].get('row_location')}.\"\n                store_log_message(logger, incoming_entity, log_message, level=\"info\")\n\n            # Check for deleted properties\n            for deleted in deleted_properties:\n                log_message = f\"{deleted} has been deleted.\"\n                store_log_message(logger, incoming_entity, log_message, level=\"warning\")\n\n            # Check for property modifications\n            common_props = incoming_prop_codes &amp; current_prop_codes\n            for prop_code in common_props:\n                new_prop = incoming_props[prop_code]\n                old_prop = current_props[prop_code]\n\n                for key, new_value in new_prop.items():\n                    old_value = old_prop.get(key)\n                    if (\n                        (key != \"code\" and key != \"row_location\")\n                        and old_value is not None\n                        and new_value != old_value\n                    ):\n                        log_message = (\n                            f\"Assigned property {prop_code} to entity type {entity_code} has changed its attribute {key} \"\n                            f\"from '{old_value}' to '{new_value}' at row {incoming_props[prop_code].get('row_location')}.\"\n                        )\n                        store_log_message(\n                            logger, incoming_entity, log_message, level=\"warning\"\n                        )\n\n        # Check if assigned properties match another entity's properties\n        for other_entity_code, other_entity in self.current_model.get(\n            entity_type, {}\n        ).items():\n            if other_entity_code != entity_code:\n                other_entity_properties = (\n                    other_entity.get(\"properties\", [])\n                    if not is_terms\n                    else other_entity.get(\"terms\", [])\n                )\n                other_entity_props = {prop[\"code\"] for prop in other_entity_properties}\n\n                if (incoming_prop_codes == other_entity_props) and incoming_prop_codes:\n                    log_message = (\n                        \"Entity will not be imported in openBIS. \"\n                        f\"The entity {entity_code} at row {incoming_entity['defs'].get('row_location')} has the same properties defined as {other_entity_code}. \"\n                        \"Maybe they are representing the same entity?\"\n                    )\n                    store_log_message(\n                        logger, incoming_entity, log_message, level=\"warning\"\n                    )\n\n    def _extract_log_messages(self, model: dict, target_dict: dict) -&gt; None:\n        \"\"\"\n        Extracts and appends _log_msgs from the validated entities into an existing dictionary.\n\n        Args:\n            model (dict): The validated entity model.\n            target_dict (dict): The dictionary where logs should be appended.\n        \"\"\"\n        for entity_type, entities in model.items():\n            if entity_type not in target_dict:\n                target_dict[entity_type] = {}\n\n            for entity_name, entity_data in entities.items():\n                if \"_log_msgs\" in entity_data and entity_data[\"_log_msgs\"]:\n                    if entity_name not in target_dict[entity_type]:\n                        target_dict[entity_type][entity_name] = {\"_log_msgs\": []}\n\n                    # Append new messages to the existing ones\n                    target_dict[entity_type][entity_name][\"_log_msgs\"].extend(\n                        entity_data[\"_log_msgs\"]\n                    )\n\n    def extract_property_codes(self, data):\n        codes = set()\n\n        # Check if the data contains 'properties' and extract 'code'\n        if isinstance(data, dict):\n            for key, value in data.items():\n                # If the key is 'properties', collect all the 'code' values\n                if key == \"properties\" and isinstance(value, list):\n                    for property_item in value:\n                        if \"code\" in property_item:\n                            codes.add(property_item[\"code\"])\n                # Recursively check for more nested structures\n                elif isinstance(value, dict | list):\n                    codes.update(self.extract_property_codes(value))\n\n        elif isinstance(data, list):\n            for item in data:\n                codes.update(self.extract_property_codes(item))\n\n        return codes\n</code></pre>"},{"location":"references/api/#bam_masterdata.checker.masterdata_validator.MasterdataValidator.new_entities","title":"<code>new_entities = new_entities</code>","text":""},{"location":"references/api/#bam_masterdata.checker.masterdata_validator.MasterdataValidator.current_model","title":"<code>current_model = current_model</code>","text":""},{"location":"references/api/#bam_masterdata.checker.masterdata_validator.MasterdataValidator.validation_rules","title":"<code>validation_rules = validation_rules</code>","text":""},{"location":"references/api/#bam_masterdata.checker.masterdata_validator.MasterdataValidator.logger","title":"<code>logger = logger</code>","text":""},{"location":"references/api/#bam_masterdata.checker.masterdata_validator.MasterdataValidator.log_msgs","title":"<code>log_msgs = []</code>","text":""},{"location":"references/api/#bam_masterdata.checker.masterdata_validator.MasterdataValidator.validation_results","title":"<code>validation_results = {}</code>","text":""},{"location":"references/api/#bam_masterdata.checker.masterdata_validator.MasterdataValidator.__init__","title":"<code>__init__(new_entities, current_model, validation_rules)</code>","text":"<p>Initialize the validator with new and current entity data.</p> PARAMETER DESCRIPTION <code>new_entities</code> <p>The incoming datamodel.</p> <p> TYPE: <code>dict</code> </p> <code>current_model</code> <p>The existing datamodel.</p> <p> TYPE: <code>dict</code> </p> <code>validation_rules</code> <p>The validation rules to apply.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>bam_masterdata/checker/masterdata_validator.py</code> <pre><code>def __init__(self, new_entities: dict, current_model: dict, validation_rules: dict):\n    \"\"\"\n    Initialize the validator with new and current entity data.\n\n    Args:\n        new_entities (dict): The incoming datamodel.\n        current_model (dict): The existing datamodel.\n        validation_rules (dict): The validation rules to apply.\n    \"\"\"\n    self.new_entities = new_entities\n    self.current_model = current_model\n    self.validation_rules = validation_rules\n    self.logger = logger\n    self.log_msgs: list = []\n    self.validation_results: dict = {}\n</code></pre>"},{"location":"references/api/#bam_masterdata.checker.masterdata_validator.MasterdataValidator.validate","title":"<code>validate(mode='all')</code>","text":"<p>Run validations based on mode: - \"self\": Validate current model structure and format. - \"incoming\": Validate new entities structure and format. - \"validate\": Validate both current and incoming models but do not compare. - \"compare\": Validate new entities against the current model. - \"all\": Run both. - \"individual\": Validate new entities and compare them with the current model.</p> <p>Returns:     dict: Validation results.</p> Source code in <code>bam_masterdata/checker/masterdata_validator.py</code> <pre><code>def validate(self, mode: str = \"all\") -&gt; dict:\n    \"\"\"\n    Run validations based on mode:\n    - \"self\": Validate current model structure and format.\n    - \"incoming\": Validate new entities structure and format.\n    - \"validate\": Validate both current and incoming models but do not compare.\n    - \"compare\": Validate new entities against the current model.\n    - \"all\": Run both.\n    - \"individual\": Validate new entities and compare them with the current model.\n\n     Returns:\n        dict: Validation results.\n    \"\"\"\n    self.logger.debug(\"Starting validation process...\", mode=mode)\n\n    # Reset validation results before running checks\n    self.validation_results = {\n        \"current_model\": {},\n        \"incoming_model\": {},\n        \"comparisons\": {},\n    }\n\n    if mode in [\"self\", \"all\", \"validate\"]:\n        self.logger.debug(\"Validating current model...\")\n        self._validate_model(self.current_model)\n        self._extract_log_messages(\n            self.current_model, self.validation_results[\"current_model\"]\n        )\n\n    if mode in [\"incoming\", \"all\", \"validate\"]:\n        self.logger.debug(\"Validating new entities...\")\n        self._validate_model(self.new_entities)\n        self._extract_log_messages(\n            self.new_entities, self.validation_results[\"incoming_model\"]\n        )\n\n    if mode in [\"compare\", \"all\"]:\n        self.logger.debug(\"Comparing new entities with current model...\")\n        self._compare_with_current_model(mode=mode)\n        self._extract_log_messages(\n            self.new_entities, self.validation_results[\"comparisons\"]\n        )\n\n    if mode == \"individual\":\n        self.logger.debug(\n            \"Validating new entities and comparing them with current model...\"\n        )\n        self.validation_results = {\n            \"incoming_model\": {},\n            \"comparisons\": {},\n        }\n        self._validate_model(self.new_entities)\n        self._extract_log_messages(\n            self.new_entities, self.validation_results[\"incoming_model\"]\n        )\n        self._compare_with_current_model(mode=\"individual\")\n        self._extract_log_messages(\n            self.new_entities, self.validation_results[\"comparisons\"]\n        )\n\n    return self.validation_results\n</code></pre>"},{"location":"references/api/#bam_masterdata.checker.masterdata_validator.MasterdataValidator.extract_property_codes","title":"<code>extract_property_codes(data)</code>","text":"Source code in <code>bam_masterdata/checker/masterdata_validator.py</code> <pre><code>def extract_property_codes(self, data):\n    codes = set()\n\n    # Check if the data contains 'properties' and extract 'code'\n    if isinstance(data, dict):\n        for key, value in data.items():\n            # If the key is 'properties', collect all the 'code' values\n            if key == \"properties\" and isinstance(value, list):\n                for property_item in value:\n                    if \"code\" in property_item:\n                        codes.add(property_item[\"code\"])\n            # Recursively check for more nested structures\n            elif isinstance(value, dict | list):\n                codes.update(self.extract_property_codes(value))\n\n    elif isinstance(data, list):\n        for item in data:\n            codes.update(self.extract_property_codes(item))\n\n    return codes\n</code></pre>"},{"location":"references/api/#bam_masterdata.checker.source_loader","title":"<code>bam_masterdata.checker.source_loader</code>","text":""},{"location":"references/api/#bam_masterdata.checker.source_loader.SourceLoader","title":"<code>SourceLoader</code>","text":"<p>Load the entities from a source written in different formats (Python classes, Excel, etc.) as defined in the <code>source_path</code> into a dictionary.</p> Source code in <code>bam_masterdata/checker/source_loader.py</code> <pre><code>class SourceLoader:\n    \"\"\"\n    Load the entities from a source written in different formats (Python classes, Excel, etc.) as defined\n    in the `source_path` into a dictionary.\n    \"\"\"\n\n    def __init__(self, source_path: str, **kwargs):\n        self.source_path = source_path\n        self.logger = kwargs.get(\"logger\", logger)\n        self.row_cell_info = kwargs.get(\"row_cell_info\", True)\n        # Check if the path is a single .py file OR a directory containing .py files\n        if self.source_path.endswith(\".py\") or (\n            os.path.isdir(self.source_path)\n            and any(glob.glob(os.path.join(self.source_path, \"*.py\")))\n        ):\n            self.source_type = \"python\"\n        elif self.source_path.endswith(\".xlsx\"):\n            self.source_type = \"excel\"\n        else:\n            self.source_type = None\n            self.logger.warning(f\"Unsupported source type for path: {source_path}\")\n\n    def load(self) -&gt; dict:\n        \"\"\"\n        Load entities from the source path into a dictionary.\n\n        Returns:\n            dict: A dictionary containing the entities.\n        \"\"\"\n        self.logger.info(f\"Source type: {self.source_type}\")\n        if self.source_type == \"python\":\n            return convert_enums(\n                EntitiesDict(python_path=self.source_path).single_json()\n            )\n        elif self.source_type == \"excel\":\n            return self.entities_to_json()\n        else:\n            raise NotImplementedError(f\"Source type {self.source_type} not supported.\")\n\n    def entities_to_json(self) -&gt; dict:\n        \"\"\"\n        Transforms the dictionary of entities returned by the Excel extractor into a dictionary in JSON format for later check.\n\n        Returns:\n            dict: A dictionary containing the transformed entities.\n        \"\"\"\n\n        excel_entities = MasterdataExcelExtractor(\n            excel_path=self.source_path, row_cell_info=self.row_cell_info\n        ).excel_to_entities()\n\n        transformed_data = {}\n\n        for entity_type, entities in excel_entities.items():\n            transformed_data[entity_type] = {}\n\n            for entity_name, entity_data in entities.items():\n                if entity_type in (\"vocabulary_type\", \"vocabulary_types\"):\n                    transformed_entity = {\n                        \"terms\": [],  # Now placed before \"defs\"\n                        \"defs\": {  # Metadata moved to the end\n                            \"code\": entity_data.get(\"code\"),\n                            \"description\": entity_data.get(\"description\", \"\"),\n                            \"id\": format_json_id(\n                                entity_name\n                            ),  # PascalCase for entity ID\n                            \"row_location\": entity_data.get(\"row_location\"),\n                            \"url_template\": entity_data.get(\"url_template\") or None,\n                        },\n                    }\n                else:\n                    transformed_entity = {\n                        \"properties\": [],  # Now placed before \"defs\"\n                        \"defs\": {  # Metadata moved to the end\n                            \"code\": entity_data.get(\"code\"),\n                            \"description\": entity_data.get(\"description\", \"\"),\n                            \"id\": format_json_id(\n                                entity_name\n                            ),  # PascalCase for entity ID\n                            \"row_location\": entity_data.get(\"row_location\"),\n                            \"validation_script\": entity_data.get(\n                                \"validationPlugin\"\n                            ).strip()\n                            if isinstance(entity_data.get(\"validationPlugin\"), str)\n                            else None,\n                            \"iri\": entity_data.get(\"iri\") or None,  # Convert \"\" to None\n                        },\n                    }\n\n                # Handle additional fields specific to dataset_types\n                if entity_type in (\"dataset_types\", \"dataset_type\"):\n                    transformed_entity[\"defs\"][\"main_dataset_pattern\"] = (\n                        entity_data.get(\"main_dataset_pattern\")\n                    )\n                    transformed_entity[\"defs\"][\"main_dataset_path\"] = entity_data.get(\n                        \"main_dataset_path\"\n                    )\n\n                # Handle additional fields specific to object_types\n                if entity_type in (\"object_types\", \"object_type\"):\n                    transformed_entity[\"defs\"][\"generated_code_prefix\"] = (\n                        entity_data.get(\"generatedCodePrefix\")\n                    )\n                    transformed_entity[\"defs\"][\"auto_generate_codes\"] = entity_data.get(\n                        \"autoGeneratedCode\"\n                    )\n\n                # Convert properties from dict to list\n                if \"properties\" in entity_data:\n                    for prop_name, prop_data in entity_data[\"properties\"].items():\n                        data_type = prop_data.get(\"dataType\")\n                        object_code = prop_data.get(\"objectCode\")\n                        if isinstance(data_type, str):\n                            data_type_upper = data_type.upper()\n                            if \":\" in data_type_upper:\n                                prefix, dynamic_code = data_type_upper.split(\":\", 1)\n                                if (\n                                    prefix in (\"SAMPLE\", \"OBJECT\")\n                                    and dynamic_code.strip()\n                                ):\n                                    data_type_upper = \"OBJECT\"\n                                    object_code = object_code or dynamic_code.strip()\n                            data_type = data_type_upper\n\n                        transformed_property = {\n                            \"code\": prop_data.get(\"code\"),\n                            \"description\": prop_data.get(\"description\", \"\"),\n                            \"id\": format_json_id(\n                                prop_name\n                            ),  # Now correctly formatted to PascalCase\n                            \"row_location\": prop_data.get(\"row_location\"),\n                            \"iri\": prop_data.get(\"iri\") or None,  # Convert \"\" to None\n                            \"property_label\": prop_data.get(\"label\"),\n                            \"data_type\": data_type,\n                            \"vocabulary_code\": prop_data.get(\"vocabularyCode\")\n                            or None,  # Convert \"\" to None\n                            \"object_code\": object_code,\n                            \"metadata\": None,\n                            \"dynamic_script\": None,\n                            \"mandatory\": prop_data.get(\"mandatory\", False),\n                            \"show_in_edit_views\": prop_data.get(\n                                \"show_in_edit_views\", False\n                            ),\n                            \"section\": prop_data.get(\"section\", \"\"),\n                            \"unique\": None,\n                            \"internal_assignment\": None,\n                        }\n                        transformed_entity[\"properties\"].append(transformed_property)\n\n                if \"terms\" in entity_data:\n                    transformed_entity.setdefault(\"terms\", [])\n                    for term_name, term_data in entity_data.get(\"terms\", \"{}\").items():\n                        transformed_term = {\n                            \"code\": term_data.get(\"code\"),\n                            \"description\": term_data.get(\"description\", \"\"),\n                            \"id\": format_json_id(\n                                term_name\n                            ),  # Now correctly formatted to PascalCase\n                            \"row_location\": term_data.get(\"row_location\"),\n                            \"url_template\": term_data.get(\"url_template\")\n                            or None,  # Convert \"\" to None\n                            \"label\": term_data.get(\"label\"),\n                            \"official\": term_data.get(\"official\"),\n                        }\n                        transformed_entity[\"terms\"].append(transformed_term)\n\n                transformed_data[entity_type][entity_name] = transformed_entity\n\n        return transformed_data\n</code></pre>"},{"location":"references/api/#bam_masterdata.checker.source_loader.SourceLoader.source_path","title":"<code>source_path = source_path</code>","text":""},{"location":"references/api/#bam_masterdata.checker.source_loader.SourceLoader.logger","title":"<code>logger = kwargs.get('logger', logger)</code>","text":""},{"location":"references/api/#bam_masterdata.checker.source_loader.SourceLoader.row_cell_info","title":"<code>row_cell_info = kwargs.get('row_cell_info', True)</code>","text":""},{"location":"references/api/#bam_masterdata.checker.source_loader.SourceLoader.source_type","title":"<code>source_type = 'python'</code>","text":""},{"location":"references/api/#bam_masterdata.checker.source_loader.SourceLoader.__init__","title":"<code>__init__(source_path, **kwargs)</code>","text":"Source code in <code>bam_masterdata/checker/source_loader.py</code> <pre><code>def __init__(self, source_path: str, **kwargs):\n    self.source_path = source_path\n    self.logger = kwargs.get(\"logger\", logger)\n    self.row_cell_info = kwargs.get(\"row_cell_info\", True)\n    # Check if the path is a single .py file OR a directory containing .py files\n    if self.source_path.endswith(\".py\") or (\n        os.path.isdir(self.source_path)\n        and any(glob.glob(os.path.join(self.source_path, \"*.py\")))\n    ):\n        self.source_type = \"python\"\n    elif self.source_path.endswith(\".xlsx\"):\n        self.source_type = \"excel\"\n    else:\n        self.source_type = None\n        self.logger.warning(f\"Unsupported source type for path: {source_path}\")\n</code></pre>"},{"location":"references/api/#bam_masterdata.checker.source_loader.SourceLoader.load","title":"<code>load()</code>","text":"<p>Load entities from the source path into a dictionary.</p> RETURNS DESCRIPTION <code>dict</code> <p>A dictionary containing the entities.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>bam_masterdata/checker/source_loader.py</code> <pre><code>def load(self) -&gt; dict:\n    \"\"\"\n    Load entities from the source path into a dictionary.\n\n    Returns:\n        dict: A dictionary containing the entities.\n    \"\"\"\n    self.logger.info(f\"Source type: {self.source_type}\")\n    if self.source_type == \"python\":\n        return convert_enums(\n            EntitiesDict(python_path=self.source_path).single_json()\n        )\n    elif self.source_type == \"excel\":\n        return self.entities_to_json()\n    else:\n        raise NotImplementedError(f\"Source type {self.source_type} not supported.\")\n</code></pre>"},{"location":"references/api/#bam_masterdata.checker.source_loader.SourceLoader.entities_to_json","title":"<code>entities_to_json()</code>","text":"<p>Transforms the dictionary of entities returned by the Excel extractor into a dictionary in JSON format for later check.</p> RETURNS DESCRIPTION <code>dict</code> <p>A dictionary containing the transformed entities.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>bam_masterdata/checker/source_loader.py</code> <pre><code>def entities_to_json(self) -&gt; dict:\n    \"\"\"\n    Transforms the dictionary of entities returned by the Excel extractor into a dictionary in JSON format for later check.\n\n    Returns:\n        dict: A dictionary containing the transformed entities.\n    \"\"\"\n\n    excel_entities = MasterdataExcelExtractor(\n        excel_path=self.source_path, row_cell_info=self.row_cell_info\n    ).excel_to_entities()\n\n    transformed_data = {}\n\n    for entity_type, entities in excel_entities.items():\n        transformed_data[entity_type] = {}\n\n        for entity_name, entity_data in entities.items():\n            if entity_type in (\"vocabulary_type\", \"vocabulary_types\"):\n                transformed_entity = {\n                    \"terms\": [],  # Now placed before \"defs\"\n                    \"defs\": {  # Metadata moved to the end\n                        \"code\": entity_data.get(\"code\"),\n                        \"description\": entity_data.get(\"description\", \"\"),\n                        \"id\": format_json_id(\n                            entity_name\n                        ),  # PascalCase for entity ID\n                        \"row_location\": entity_data.get(\"row_location\"),\n                        \"url_template\": entity_data.get(\"url_template\") or None,\n                    },\n                }\n            else:\n                transformed_entity = {\n                    \"properties\": [],  # Now placed before \"defs\"\n                    \"defs\": {  # Metadata moved to the end\n                        \"code\": entity_data.get(\"code\"),\n                        \"description\": entity_data.get(\"description\", \"\"),\n                        \"id\": format_json_id(\n                            entity_name\n                        ),  # PascalCase for entity ID\n                        \"row_location\": entity_data.get(\"row_location\"),\n                        \"validation_script\": entity_data.get(\n                            \"validationPlugin\"\n                        ).strip()\n                        if isinstance(entity_data.get(\"validationPlugin\"), str)\n                        else None,\n                        \"iri\": entity_data.get(\"iri\") or None,  # Convert \"\" to None\n                    },\n                }\n\n            # Handle additional fields specific to dataset_types\n            if entity_type in (\"dataset_types\", \"dataset_type\"):\n                transformed_entity[\"defs\"][\"main_dataset_pattern\"] = (\n                    entity_data.get(\"main_dataset_pattern\")\n                )\n                transformed_entity[\"defs\"][\"main_dataset_path\"] = entity_data.get(\n                    \"main_dataset_path\"\n                )\n\n            # Handle additional fields specific to object_types\n            if entity_type in (\"object_types\", \"object_type\"):\n                transformed_entity[\"defs\"][\"generated_code_prefix\"] = (\n                    entity_data.get(\"generatedCodePrefix\")\n                )\n                transformed_entity[\"defs\"][\"auto_generate_codes\"] = entity_data.get(\n                    \"autoGeneratedCode\"\n                )\n\n            # Convert properties from dict to list\n            if \"properties\" in entity_data:\n                for prop_name, prop_data in entity_data[\"properties\"].items():\n                    data_type = prop_data.get(\"dataType\")\n                    object_code = prop_data.get(\"objectCode\")\n                    if isinstance(data_type, str):\n                        data_type_upper = data_type.upper()\n                        if \":\" in data_type_upper:\n                            prefix, dynamic_code = data_type_upper.split(\":\", 1)\n                            if (\n                                prefix in (\"SAMPLE\", \"OBJECT\")\n                                and dynamic_code.strip()\n                            ):\n                                data_type_upper = \"OBJECT\"\n                                object_code = object_code or dynamic_code.strip()\n                        data_type = data_type_upper\n\n                    transformed_property = {\n                        \"code\": prop_data.get(\"code\"),\n                        \"description\": prop_data.get(\"description\", \"\"),\n                        \"id\": format_json_id(\n                            prop_name\n                        ),  # Now correctly formatted to PascalCase\n                        \"row_location\": prop_data.get(\"row_location\"),\n                        \"iri\": prop_data.get(\"iri\") or None,  # Convert \"\" to None\n                        \"property_label\": prop_data.get(\"label\"),\n                        \"data_type\": data_type,\n                        \"vocabulary_code\": prop_data.get(\"vocabularyCode\")\n                        or None,  # Convert \"\" to None\n                        \"object_code\": object_code,\n                        \"metadata\": None,\n                        \"dynamic_script\": None,\n                        \"mandatory\": prop_data.get(\"mandatory\", False),\n                        \"show_in_edit_views\": prop_data.get(\n                            \"show_in_edit_views\", False\n                        ),\n                        \"section\": prop_data.get(\"section\", \"\"),\n                        \"unique\": None,\n                        \"internal_assignment\": None,\n                    }\n                    transformed_entity[\"properties\"].append(transformed_property)\n\n            if \"terms\" in entity_data:\n                transformed_entity.setdefault(\"terms\", [])\n                for term_name, term_data in entity_data.get(\"terms\", \"{}\").items():\n                    transformed_term = {\n                        \"code\": term_data.get(\"code\"),\n                        \"description\": term_data.get(\"description\", \"\"),\n                        \"id\": format_json_id(\n                            term_name\n                        ),  # Now correctly formatted to PascalCase\n                        \"row_location\": term_data.get(\"row_location\"),\n                        \"url_template\": term_data.get(\"url_template\")\n                        or None,  # Convert \"\" to None\n                        \"label\": term_data.get(\"label\"),\n                        \"official\": term_data.get(\"official\"),\n                    }\n                    transformed_entity[\"terms\"].append(transformed_term)\n\n            transformed_data[entity_type][entity_name] = transformed_entity\n\n    return transformed_data\n</code></pre>"},{"location":"references/api/#bam_masterdata.parsing.parsing","title":"<code>bam_masterdata.parsing.parsing</code>","text":""},{"location":"references/api/#bam_masterdata.parsing.parsing.AbstractParser","title":"<code>AbstractParser</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Example Abstract base class for parsers. Each parser should inherit from this class and implement the <code>parse()</code> method to populate <code>collection</code>.</p> Source code in <code>bam_masterdata/parsing/parsing.py</code> <pre><code>class AbstractParser(ABC):\n    \"\"\"\n    Example Abstract base class for parsers. Each parser should inherit from this class and implement\n    the `parse()` method to populate `collection`.\n    \"\"\"\n\n    @abstractmethod\n    def parse(\n        self,\n        files: list[str],\n        collection: CollectionType,\n        logger: \"BoundLoggerLazyProxy\",\n    ) -&gt; None:\n        \"\"\"\n        Parse the input `files` and populate the provided `collection` with object types, their metadata,\n        and their relationships.\n\n        Args:\n            files (list[str]): List of file paths to be parsed.\n            collection (CollectionType): Collection to be populated with parsed data.\n            logger (BoundLoggerLazyProxy): Logger for logging messages during parsing.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"references/api/#bam_masterdata.parsing.parsing.AbstractParser.parse","title":"<code>parse(files, collection, logger)</code>","text":"<p>Parse the input <code>files</code> and populate the provided <code>collection</code> with object types, their metadata, and their relationships.</p> PARAMETER DESCRIPTION <code>files</code> <p>List of file paths to be parsed.</p> <p> TYPE: <code>list[str]</code> </p> <code>collection</code> <p>Collection to be populated with parsed data.</p> <p> TYPE: <code>CollectionType</code> </p> <code>logger</code> <p>Logger for logging messages during parsing.</p> <p> TYPE: <code>BoundLoggerLazyProxy</code> </p> Source code in <code>bam_masterdata/parsing/parsing.py</code> <pre><code>@abstractmethod\ndef parse(\n    self,\n    files: list[str],\n    collection: CollectionType,\n    logger: \"BoundLoggerLazyProxy\",\n) -&gt; None:\n    \"\"\"\n    Parse the input `files` and populate the provided `collection` with object types, their metadata,\n    and their relationships.\n\n    Args:\n        files (list[str]): List of file paths to be parsed.\n        collection (CollectionType): Collection to be populated with parsed data.\n        logger (BoundLoggerLazyProxy): Logger for logging messages during parsing.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"references/api/#bam_masterdata.utils.utils","title":"<code>bam_masterdata.utils.utils</code>","text":""},{"location":"references/api/#bam_masterdata.utils.utils.delete_and_create_dir","title":"<code>delete_and_create_dir(directory_path, logger=logger, force_delete=False)</code>","text":"<p>Deletes the directory at <code>directory_path</code> and creates a new one in the same path.</p> PARAMETER DESCRIPTION <code>directory_path</code> <p>The directory path to delete and create the folder.</p> <p> TYPE: <code>str</code> </p> <code>logger</code> <p>The logger to log messages. Default is <code>logger</code>.</p> <p> TYPE: <code>BoundLoggerLazyProxy</code> DEFAULT: <code>logger</code> </p> <code>force_delete</code> <p>If True, the directory will be forcibly deleted if it exists.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>bam_masterdata/utils/utils.py</code> <pre><code>def delete_and_create_dir(\n    directory_path: str,\n    logger: \"BoundLoggerLazyProxy\" = logger,\n    force_delete: bool = False,\n) -&gt; None:\n    \"\"\"\n    Deletes the directory at `directory_path` and creates a new one in the same path.\n\n    Args:\n        directory_path (str): The directory path to delete and create the folder.\n        logger (BoundLoggerLazyProxy): The logger to log messages. Default is `logger`.\n        force_delete (bool): If True, the directory will be forcibly deleted if it exists.\n    \"\"\"\n    if not directory_path:\n        logger.warning(\n            \"The `directory_path` is empty. Please, provide a proper input to the function.\"\n        )\n        return None\n\n    if not force_delete:\n        logger.info(f\"Skipping the deletion of the directory at {directory_path}.\")\n        if not os.path.exists(directory_path):\n            os.makedirs(directory_path)\n        return None\n\n    if os.path.exists(directory_path):\n        try:\n            shutil.rmtree(directory_path)  # ! careful with this line\n        except PermissionError:\n            logger.error(\n                f\"Permission denied to delete the directory at {directory_path}.\"\n            )\n            return None\n    os.makedirs(directory_path)\n</code></pre>"},{"location":"references/api/#bam_masterdata.utils.utils.listdir_py_modules","title":"<code>listdir_py_modules(directory_path, logger=logger)</code>","text":"<p>Recursively goes through the <code>directory_path</code> and returns a list of all .py files that do not start with '_'. If <code>directory_path</code> is a single Python module file, it will return a list with that file.</p> PARAMETER DESCRIPTION <code>directory_path</code> <p>The directory path to search through.</p> <p> TYPE: <code>str</code> </p> <code>logger</code> <p>The logger to log messages. Default is <code>logger</code>.</p> <p> TYPE: <code>BoundLoggerLazyProxy</code> DEFAULT: <code>logger</code> </p> RETURNS DESCRIPTION <code>list[str]</code> <p>list[str]: A list of all .py files that do not start with '_'</p> Source code in <code>bam_masterdata/utils/utils.py</code> <pre><code>def listdir_py_modules(\n    directory_path: str, logger: \"BoundLoggerLazyProxy\" = logger\n) -&gt; list[str]:\n    \"\"\"\n    Recursively goes through the `directory_path` and returns a list of all .py files that do not start with '_'. If\n    `directory_path` is a single Python module file, it will return a list with that file.\n\n    Args:\n        directory_path (str): The directory path to search through.\n        logger (BoundLoggerLazyProxy): The logger to log messages. Default is `logger`.\n\n    Returns:\n        list[str]: A list of all .py files that do not start with '_'\n    \"\"\"\n    if not directory_path:\n        logger.warning(\n            \"The `directory_path` is empty. Please, provide a proper input to the function.\"\n        )\n        return []\n\n    # In case of a individual Python module file\n    if directory_path.endswith(\".py\"):\n        return [directory_path]\n    # Use glob to find all .py files recursively in a directory containing all modules\n    else:\n        files = glob.glob(os.path.join(directory_path, \"**\", \"*.py\"), recursive=True)\n    if not files:\n        logger.info(\"No Python files found in the directory.\")\n        return []\n\n    # Filter out files that start with '_'\n    # ! sorted in order to avoid using with OS sorting differently\n    return sorted(\n        [\n            f\n            for f in files\n            if not os.path.basename(f).startswith(\"_\") and \"tmp\" not in f.split(os.sep)\n        ]\n    )\n</code></pre>"},{"location":"references/api/#bam_masterdata.utils.utils.import_module","title":"<code>import_module(module_path)</code>","text":"<p>Dynamically imports a module from the given file path.</p> PARAMETER DESCRIPTION <code>module_path</code> <p>Path to the Python module file.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>module</code> <p>Imported module object.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>bam_masterdata/utils/utils.py</code> <pre><code>def import_module(module_path: str) -&gt; Any:\n    \"\"\"\n    Dynamically imports a module from the given file path.\n\n    Args:\n        module_path (str): Path to the Python module file.\n\n    Returns:\n        module: Imported module object.\n    \"\"\"\n    module_name = os.path.splitext(os.path.basename(module_path))[0]\n    spec = importlib.util.spec_from_file_location(module_name, module_path)\n    module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(module)\n    return module\n</code></pre>"},{"location":"references/api/#bam_masterdata.utils.utils.code_to_class_name","title":"<code>code_to_class_name(code, logger=logger, entity_type='object')</code>","text":"<p>Converts an openBIS <code>code</code> to a class name by capitalizing each word and removing special characters. In the special case the entity is a property type, it retains the full name separated by points instead of only keeping the last name (e.g., \"TEM.INSTRUMENT\" -&gt; \"TemInstrument\" instead of \"Instrument\").</p> PARAMETER DESCRIPTION <code>code</code> <p>The openBIS code to convert to a class name.</p> <p> TYPE: <code>str</code> </p> <code>logger</code> <p>The logger to log messages. Default is <code>logger</code>.</p> <p> TYPE: <code>BoundLoggerLazyProxy</code> DEFAULT: <code>logger</code> </p> <code>entity_type</code> <p>The type of entity to convert. Default is \"object\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'object'</code> </p> <p>Returns:     str: The class name derived from the openBIS code.</p> Source code in <code>bam_masterdata/utils/utils.py</code> <pre><code>def code_to_class_name(\n    code: str | None,\n    logger: \"BoundLoggerLazyProxy\" = logger,\n    entity_type: str = \"object\",\n) -&gt; str:\n    \"\"\"\n    Converts an openBIS `code` to a class name by capitalizing each word and removing special characters. In\n    the special case the entity is a property type, it retains the full name separated by points instead of\n    only keeping the last name (e.g., \"TEM.INSTRUMENT\" -&gt; \"TemInstrument\" instead of \"Instrument\").\n\n    Args:\n        code (str): The openBIS code to convert to a class name.\n        logger (BoundLoggerLazyProxy): The logger to log messages. Default is `logger`.\n        entity_type (str): The type of entity to convert. Default is \"object\".\n    Returns:\n        str: The class name derived from the openBIS code.\n    \"\"\"\n    if not code:\n        logger.error(\n            \"The `code` is empty. Please, provide a proper input to the function.\"\n        )\n        return \"\"\n\n    if entity_type == \"property\":\n        code_names = chain.from_iterable(\n            [c.split(\"_\") for c in code.lstrip(\"$\").split(\".\")]\n        )\n        return \"\".join(c.capitalize() for c in code_names)\n    return \"\".join(c.capitalize() for c in code.lstrip(\"$\").rsplit(\".\")[-1].split(\"_\"))\n</code></pre>"},{"location":"references/api/#bam_masterdata.utils.utils.load_validation_rules","title":"<code>load_validation_rules(logger, file_path=os.path.join(VALIDATION_RULES_DIR, 'validation_rules.json'))</code>","text":"Source code in <code>bam_masterdata/utils/utils.py</code> <pre><code>def load_validation_rules(\n    logger: \"BoundLoggerLazyProxy\",\n    file_path: str = os.path.join(VALIDATION_RULES_DIR, \"validation_rules.json\"),\n):\n    if not os.path.exists(file_path):\n        logger.error(f\"Validation rules file not found: {file_path}\")\n        raise FileNotFoundError(f\"Validation rules file not found: {file_path}\")\n\n    try:\n        with open(file_path, encoding=\"utf-8\") as file:\n            validation_rules = json.load(file)\n\n        logger.info(\"Validation rules successfully loaded.\")\n\n        return validation_rules\n\n    except json.JSONDecodeError as e:\n        logger.error(f\"Error parsing validation rules JSON: {e}\")\n        raise ValueError(f\"Error parsing validation rules JSON: {e}\")\n</code></pre>"},{"location":"references/api/#bam_masterdata.utils.utils.duplicated_property_types","title":"<code>duplicated_property_types(module_path, logger)</code>","text":"<p>Find the duplicated property types in a module specified by <code>module_path</code> and returns a dictionary containing the duplicated property types class names as keys and the lines where they matched as values.</p> PARAMETER DESCRIPTION <code>module_path</code> <p>The path to the module containing the property types.</p> <p> TYPE: <code>str</code> </p> <code>logger</code> <p>The logger to log messages.</p> <p> TYPE: <code>BoundLoggerLazyProxy</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>A dictionary containing the duplicated property types class names as keys and the</p> <p> TYPE: <code>dict</code> </p> <code>dict</code> <p>lines where they matched as values.</p> Source code in <code>bam_masterdata/utils/utils.py</code> <pre><code>def duplicated_property_types(module_path: str, logger: \"BoundLoggerLazyProxy\") -&gt; dict:\n    \"\"\"\n    Find the duplicated property types in a module specified by `module_path` and returns a dictionary\n    containing the duplicated property types class names as keys and the lines where they matched as values.\n\n    Args:\n        module_path (str): The path to the module containing the property types.\n        logger (BoundLoggerLazyProxy): The logger to log messages.\n\n    Returns:\n        dict: A dictionary containing the duplicated property types class names as keys and the\n        lines where they matched as values.\n    \"\"\"\n    duplicated_props: dict = {}\n    module = import_module(module_path=module_path)\n    source_code = inspect.getsource(module)\n    for name, _ in inspect.getmembers(module):\n        if name.startswith(\"_\") or name == \"PropertyTypeDef\":\n            continue\n\n        pattern = rf\"^\\s*{name} *= *PropertyTypeDef\"\n\n        # Find all matching line numbers\n        matches = [\n            i + 1  # Convert to 1-based index\n            for i, line in enumerate(source_code.splitlines())\n            if re.match(pattern, line)\n        ]\n        if len(matches) &gt; 1:\n            duplicated_props[name] = matches\n    if duplicated_props:\n        logger.critical(\n            f\"Found {len(duplicated_props)} duplicated property types. These are stored in a dictionary \"\n            f\"where the keys are the names of the variables in property_types.py and the values are the lines in the module: {duplicated_props}\"\n        )\n    return duplicated_props\n</code></pre>"},{"location":"references/api/#bam_masterdata.utils.utils.format_json_id","title":"<code>format_json_id(value)</code>","text":"<p>Converts snake_case or UPPER_CASE to PascalCase while keeping special cases like '$NAME' untouched.</p> Source code in <code>bam_masterdata/utils/utils.py</code> <pre><code>def format_json_id(value):\n    \"\"\"Converts snake_case or UPPER_CASE to PascalCase while keeping special cases like '$NAME' untouched.\"\"\"\n    if value.startswith(\"$\"):\n        # Remove \"$\" and apply PascalCase transformation\n        value = value[1:]\n    return \"\".join(\n        word.capitalize() for word in re.split(r\"[\\._]\", value)\n    )  # PascalCase\n</code></pre>"},{"location":"references/api/#bam_masterdata.utils.utils.convert_enums","title":"<code>convert_enums(obj)</code>","text":"Source code in <code>bam_masterdata/utils/utils.py</code> <pre><code>def convert_enums(obj):\n    if isinstance(obj, dict):\n        return {k: convert_enums(v) for k, v in obj.items()}\n    elif isinstance(obj, list):\n        return [convert_enums(i) for i in obj]\n    elif isinstance(obj, Enum):  # Convert Enum to string\n        return obj.value\n    return obj\n</code></pre>"},{"location":"references/api/#bam_masterdata.utils.utils.is_reduced_version","title":"<code>is_reduced_version(generated_code_value, code)</code>","text":"<p>Check if generated_code_value is a reduced version of code.</p> PARAMETER DESCRIPTION <code>generated_code_value</code> <p>The potentially reduced code.</p> <p> TYPE: <code>str</code> </p> <code>code</code> <p>The original full code.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if generated_code_value is a reduced version of code, False otherwise.</p> <p> TYPE: <code>bool</code> </p> Source code in <code>bam_masterdata/utils/utils.py</code> <pre><code>def is_reduced_version(generated_code_value: str, code: str) -&gt; bool:\n    \"\"\"\n    Check if generated_code_value is a reduced version of code.\n\n    Args:\n        generated_code_value (str): The potentially reduced code.\n        code (str): The original full code.\n\n    Returns:\n        bool: True if generated_code_value is a reduced version of code, False otherwise.\n    \"\"\"\n    if generated_code_value == \"\" or code == \"\":\n        return False\n\n    if code.startswith(generated_code_value):\n        return True\n\n    # Check if both are single words (no delimiters)\n    if not any(delimiter in code for delimiter in \"._\") and not any(\n        delimiter in generated_code_value for delimiter in \"._\"\n    ):\n        return True\n\n    # Determine the delimiter in each string\n    code_delimiter = \".\" if \".\" in code else \"_\" if \"_\" in code else None\n    generated_delimiter = (\n        \".\"\n        if \".\" in generated_code_value\n        else \"_\"\n        if \"_\" in generated_code_value\n        else None\n    )\n\n    # If delimiters don't match, return False\n    if code_delimiter != generated_delimiter:\n        return False\n\n    # Split both strings using the determined delimiter\n    generated_parts = generated_code_value.split(code_delimiter)\n    original_parts = code.split(code_delimiter)\n\n    # Ensure both have the same number of parts\n    return len(generated_parts) == len(original_parts)\n</code></pre>"},{"location":"references/api/#bam_masterdata.utils.utils.store_log_message","title":"<code>store_log_message(logger, entity_ref, message, level='error')</code>","text":"<p>Logs a message and stores it inside the entity's _log_msgs list.</p> PARAMETER DESCRIPTION <code>entity_ref</code> <p>The entity dictionary where _log_msgs should be stored.</p> <p> TYPE: <code>dict</code> </p> <code>message</code> <p>The log message.</p> <p> TYPE: <code>str</code> </p> <code>level</code> <p>Log level ('error', 'warning', 'critical', 'info').</p> <p> TYPE: <code>str</code> DEFAULT: <code>'error'</code> </p> Source code in <code>bam_masterdata/utils/utils.py</code> <pre><code>def store_log_message(logger, entity_ref, message, level=\"error\"):\n    \"\"\"\n    Logs a message and stores it inside the entity's _log_msgs list.\n\n    Args:\n        entity_ref (dict): The entity dictionary where _log_msgs should be stored.\n        message (str): The log message.\n        level (str): Log level ('error', 'warning', 'critical', 'info').\n    \"\"\"\n    log_function = {\n        \"error\": logger.error,\n        \"warning\": logger.warning,\n        \"critical\": logger.critical,\n        \"info\": logger.info,\n    }.get(level, logger.error)\n\n    # Log the message\n    log_function(message)\n\n    # Ensure _log_msgs exists\n    if \"_log_msgs\" not in entity_ref:\n        entity_ref[\"_log_msgs\"] = []\n\n    # Append log message\n    entity_ref[\"_log_msgs\"].append((level, message))\n</code></pre>"},{"location":"references/api/#bam_masterdata.utils.paths","title":"<code>bam_masterdata.utils.paths</code>","text":""},{"location":"references/api/#bam_masterdata.utils.paths.DIRECTORIES","title":"<code>DIRECTORIES = {'datamodel': [Path.cwd() / 'datamodel', Path.cwd() / 'bam_masterdata' / 'datamodel', Path(__file__).parent.parent / 'datamodel'], 'validation_rules_checker': [Path.cwd() / 'bam_masterdata' / 'checker' / 'validation_rules', Path(__file__).parent.parent / 'checker' / 'validation_rules']}</code>","text":""},{"location":"references/api/#bam_masterdata.utils.paths.DATAMODEL_DIR","title":"<code>DATAMODEL_DIR = find_dir(possible_locations=(DIRECTORIES['datamodel']))</code>","text":""},{"location":"references/api/#bam_masterdata.utils.paths.VALIDATION_RULES_DIR","title":"<code>VALIDATION_RULES_DIR = find_dir(possible_locations=(DIRECTORIES['validation_rules_checker']))</code>","text":""},{"location":"references/api/#bam_masterdata.utils.paths.find_dir","title":"<code>find_dir(possible_locations)</code>","text":"<p>Search for a valid directory in a list of possible locations.</p> PARAMETER DESCRIPTION <code>possible_locations</code> <p>A list of possible locations to search for a directory.</p> <p> TYPE: <code>list[Path]</code> </p> RAISES DESCRIPTION <code>FileNotFoundError</code> <p>If no valid directory is found.</p> RETURNS DESCRIPTION <code>str</code> <p>The path of the valid directory.</p> <p> TYPE: <code>str</code> </p> Source code in <code>bam_masterdata/utils/paths.py</code> <pre><code>def find_dir(possible_locations: list[Path]) -&gt; str:\n    \"\"\"\n    Search for a valid directory in a list of possible locations.\n\n    Args:\n        possible_locations (list[Path]): A list of possible locations to search for a directory.\n\n    Raises:\n        FileNotFoundError: If no valid directory is found.\n\n    Returns:\n        str: The path of the valid directory.\n    \"\"\"\n    for path in possible_locations:\n        if path.exists():\n            return str(path.resolve())\n\n    raise FileNotFoundError(\"Could not find a valid directory.\")\n</code></pre>"},{"location":"references/api/#bam_masterdata.utils.users","title":"<code>bam_masterdata.utils.users</code>","text":""},{"location":"references/api/#bam_masterdata.utils.users.UserID","title":"<code>UserID</code>","text":"Source code in <code>bam_masterdata/utils/users.py</code> <pre><code>class UserID:\n    def __init__(self, url: str = \"\"):\n        if not url:\n            raise ValueError(\"Missing url to connect to openBIS\")\n        self.openbis = ologin(url=url)\n        self.users = self.openbis.get_users()\n\n    def _split_name(self, name: str):\n        \"\"\"\n        Split a full name into firstname and lastname using comma ',' or space ' ' as separator.\n\n        Args:\n            name (str): Full name to split, e.g., \"John Doe\" or \"Doe, John\"\n        \"\"\"\n        parts = re.split(r\",|\\s+\", name.strip())\n        parts = [p for p in parts if p]\n        if len(parts) &gt;= 2:\n            return parts[0], parts[1]\n        return parts[0], \"\"  # if only one name\n\n    def get_userid_from_names(self, firstname: str, lastname: str) -&gt; str | None:\n        \"\"\"\n        Return the userId matching the given first and last name (case-insensitive).\n\n        Args:\n            firstname (str): First name.\n            lastname (str): Last name.\n\n        Returns:\n            str | None: The userId if a match is found, otherwise None.\n        \"\"\"\n        for u in self.users:\n            if (\n                u.firstName.lower() == firstname.lower()\n                and u.lastName.lower() == lastname.lower()\n            ):\n                return u.userId\n        return None\n\n    def get_userid_from_fullname(self, name: str) -&gt; str | None:\n        \"\"\"\n        Return the userId matching the given fullname (case-insensitive). It uses the `_split_name` function.\n\n        Args:\n            name (str): Full name, e.g., \"John Doe\" or \"Doe, John\".\n\n        Returns:\n            str | None: The userId if a match is found, otherwise None.\n        \"\"\"\n        firstname, lastname = self._split_name(name)\n        for u in self.users:\n            if (u.firstName.lower(), u.lastName.lower()) == (\n                firstname.lower(),\n                lastname.lower(),\n            ) or (u.firstName.lower(), u.lastName.lower()) == (\n                lastname.lower(),\n                firstname.lower(),\n            ):\n                return u.userId\n        return None\n</code></pre>"},{"location":"references/api/#bam_masterdata.utils.users.UserID.openbis","title":"<code>openbis = ologin(url=url)</code>","text":""},{"location":"references/api/#bam_masterdata.utils.users.UserID.users","title":"<code>users = self.openbis.get_users()</code>","text":""},{"location":"references/api/#bam_masterdata.utils.users.UserID.__init__","title":"<code>__init__(url='')</code>","text":"Source code in <code>bam_masterdata/utils/users.py</code> <pre><code>def __init__(self, url: str = \"\"):\n    if not url:\n        raise ValueError(\"Missing url to connect to openBIS\")\n    self.openbis = ologin(url=url)\n    self.users = self.openbis.get_users()\n</code></pre>"},{"location":"references/api/#bam_masterdata.utils.users.UserID.get_userid_from_names","title":"<code>get_userid_from_names(firstname, lastname)</code>","text":"<p>Return the userId matching the given first and last name (case-insensitive).</p> PARAMETER DESCRIPTION <code>firstname</code> <p>First name.</p> <p> TYPE: <code>str</code> </p> <code>lastname</code> <p>Last name.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str | None</code> <p>str | None: The userId if a match is found, otherwise None.</p> Source code in <code>bam_masterdata/utils/users.py</code> <pre><code>def get_userid_from_names(self, firstname: str, lastname: str) -&gt; str | None:\n    \"\"\"\n    Return the userId matching the given first and last name (case-insensitive).\n\n    Args:\n        firstname (str): First name.\n        lastname (str): Last name.\n\n    Returns:\n        str | None: The userId if a match is found, otherwise None.\n    \"\"\"\n    for u in self.users:\n        if (\n            u.firstName.lower() == firstname.lower()\n            and u.lastName.lower() == lastname.lower()\n        ):\n            return u.userId\n    return None\n</code></pre>"},{"location":"references/api/#bam_masterdata.utils.users.UserID.get_userid_from_fullname","title":"<code>get_userid_from_fullname(name)</code>","text":"<p>Return the userId matching the given fullname (case-insensitive). It uses the <code>_split_name</code> function.</p> PARAMETER DESCRIPTION <code>name</code> <p>Full name, e.g., \"John Doe\" or \"Doe, John\".</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str | None</code> <p>str | None: The userId if a match is found, otherwise None.</p> Source code in <code>bam_masterdata/utils/users.py</code> <pre><code>def get_userid_from_fullname(self, name: str) -&gt; str | None:\n    \"\"\"\n    Return the userId matching the given fullname (case-insensitive). It uses the `_split_name` function.\n\n    Args:\n        name (str): Full name, e.g., \"John Doe\" or \"Doe, John\".\n\n    Returns:\n        str | None: The userId if a match is found, otherwise None.\n    \"\"\"\n    firstname, lastname = self._split_name(name)\n    for u in self.users:\n        if (u.firstName.lower(), u.lastName.lower()) == (\n            firstname.lower(),\n            lastname.lower(),\n        ) or (u.firstName.lower(), u.lastName.lower()) == (\n            lastname.lower(),\n            firstname.lower(),\n        ):\n            return u.userId\n    return None\n</code></pre>"},{"location":"references/api/#bam_masterdata.utils.users.get_bam_username","title":"<code>get_bam_username(firstname, lastname)</code>","text":"<p>Tries to get the BAM username from the first and last names. The BAM username guess is defined by concatenating the first letter of the first name with the, max. 7 digits of the last name.</p> PARAMETER DESCRIPTION <code>firstname</code> <p>The first name.</p> <p> TYPE: <code>str</code> </p> <code>lastname</code> <p>The last name.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The guessed BAM username.</p> <p> TYPE: <code>str</code> </p> Source code in <code>bam_masterdata/utils/users.py</code> <pre><code>def get_bam_username(firstname: str, lastname: str) -&gt; str:\n    \"\"\"\n    Tries to get the BAM username from the first and last names. The BAM username guess is defined by concatenating\n    the first letter of the first name with the, max. 7 digits of the last name.\n\n    Args:\n        firstname (str): The first name.\n        lastname (str): The last name.\n\n    Returns:\n        str: The guessed BAM username.\n    \"\"\"\n\n    def _de_replacements(name: str) -&gt; str:\n        \"\"\"\n        Perform common German name replacements to standardize the name.\n\n        Args:\n            name (str): Name to standardize.\n\n        Returns:\n            str: Standardized name.\n        \"\"\"\n        replacements = {\n            \"\u00c4\": \"Ae\",\n            \"\u00d6\": \"Oe\",\n            \"\u00dc\": \"Ue\",\n            \"\u00e4\": \"ae\",\n            \"\u00f6\": \"oe\",\n            \"\u00fc\": \"ue\",\n            \"\u00df\": \"ss\",\n        }\n        for old, new in replacements.items():\n            name = name.replace(old, new)\n        return name\n\n    # German umlaut replacements\n    firstname = _de_replacements(firstname).lower().strip()\n    lastname = _de_replacements(lastname).lower().strip()\n\n    # Defining username format\n    first_letter = firstname[0]\n    if len(lastname) &gt; 7:\n        last_part = lastname[:7]\n    else:\n        last_part = lastname\n    return f\"{first_letter}{last_part}\".upper()\n</code></pre>"},{"location":"references/extra/","title":"Extra","text":""},{"location":"references/extra/#usage-examples","title":"Usage Examples","text":""},{"location":"references/extra/#basic-entity-operations","title":"Basic Entity Operations","text":"<pre><code>from bam_masterdata.datamodel.object_types import Sample\nfrom bam_masterdata.datamodel.dataset_types import RawData\nfrom bam_masterdata.datamodel.collection_types import MeasurementsCollection\n\n# Create entities\nsample = Sample(code=\"SAMPLE_001\", name=\"Test Sample\")\ndataset = RawData(code=\"DATA_001\", name=\"Raw measurement data\")\ncollection = MeasurementsCollection(code=\"COLL_001\", name=\"Test Collection\")\n\n# Convert to different formats\nsample_dict = sample.to_dict()\nsample_json = sample.to_json()\nopenbis_sample = sample.to_openbis()\n</code></pre>"},{"location":"references/extra/#excel-integration","title":"Excel Integration","text":"<pre><code>from bam_masterdata.excel.excel_to_entities import MasterdataExcelExtractor\nimport openpyxl\n\n# Load Excel file\nworkbook = openpyxl.load_workbook(\"masterdata.xlsx\")\n\n# Extract entities\nextractor = MasterdataExcelExtractor()\nentities = extractor.excel_to_entities(workbook)\n\n# Access extracted data\nobject_types = entities.get(\"object_types\", {})\ndataset_types = entities.get(\"dataset_types\", {})\n</code></pre>"},{"location":"references/extra/#openbis-operations","title":"openBIS Operations","text":"<pre><code>from bam_masterdata.openbis.login import ologin\nfrom bam_masterdata.openbis.get_entities import OpenbisEntities\n\n# Connect to openBIS\nopenbis = ologin(url=\"https://openbis.example.com\", username=\"user\", password=\"pass\")\n\n# Retrieve entities\nopenbis_entities = OpenbisEntities(openbis)\nobject_dict = openbis_entities.get_object_dict()\ndataset_dict = openbis_entities.get_dataset_dict()\n</code></pre>"},{"location":"references/extra/#validation","title":"Validation","text":"<pre><code>from bam_masterdata.checker.masterdata_validator import MasterdataValidator\n\n# Validate entities\nvalidator = MasterdataValidator()\nentities_dict = {\n    \"object_types\": object_types,\n    \"dataset_types\": dataset_types\n}\n\nresult = validator.validate(entities_dict)\nif not result.is_valid:\n    for error in result.errors:\n        print(f\"Validation error: {error}\")\n</code></pre>"},{"location":"references/extra/#code-generation","title":"Code Generation","text":"<pre><code>from bam_masterdata.cli.fill_masterdata import MasterdataCodeGenerator\n\n# Generate Python code from entity definitions\ngenerator = MasterdataCodeGenerator(\n    objects=object_types,\n    datasets=dataset_types,\n    collections=collection_types,\n    vocabularies=vocabulary_types\n)\n\n# Generate code for different entity types\nobject_code = generator.generate_object_types()\ndataset_code = generator.generate_dataset_types()\nvocab_code = generator.generate_vocabulary_types()\n</code></pre>"},{"location":"references/extra/#type-definitions","title":"Type Definitions","text":""},{"location":"references/extra/#entity-definitions","title":"Entity Definitions","text":"<p>The package uses several base definition classes that define the structure of entities:</p> <ul> <li><code>EntityDef</code>: Base definition class for all entities</li> <li><code>ObjectTypeDef</code>: Defines object type structure</li> <li><code>DatasetTypeDef</code>: Defines dataset type structure</li> <li><code>CollectionTypeDef</code>: Defines collection type structure</li> <li><code>VocabularyTypeDef</code>: Defines vocabulary structure</li> </ul>"},{"location":"references/extra/#property-definitions","title":"Property Definitions","text":"<p>Properties are defined using:</p> <ul> <li><code>PropertyTypeDef</code>: Basic property definition</li> <li><code>PropertyTypeAssignment</code>: Property assignment to entity types</li> <li><code>VocabularyTerm</code>: Individual terms in controlled vocabularies</li> </ul>"},{"location":"references/extra/#data-types","title":"Data Types","text":"<p>The system supports several data types for properties:</p> <ul> <li><code>VARCHAR</code>: Variable-length character strings</li> <li><code>MULTILINE_VARCHAR</code>: Multi-line text</li> <li><code>INTEGER</code>: Whole numbers</li> <li><code>REAL</code>: Floating-point numbers</li> <li><code>BOOLEAN</code>: True/false values</li> <li><code>TIMESTAMP</code>: Date and time values</li> <li><code>CONTROLLEDVOCABULARY</code>: Values from controlled vocabularies</li> <li><code>XML</code>: Structured XML data <pre><code>## Configuration\n\nThe package can be configured through various mechanisms:\n\n### Environment Variables\n\n- `BAM_MASTERDATA_CONFIG_PATH`: Path to configuration file\n- `OPENBIS_URL`: Default openBIS server URL\n- `OPENBIS_USERNAME`: Default username for openBIS\n- `LOG_LEVEL`: Logging level (DEBUG, INFO, WARNING, ERROR)\n\n### Configuration File\n\n```yaml\n# config.yaml\nopenbis:\n  url: \"https://openbis.example.com\"\n  username: \"default_user\"\n  timeout: 30\n\nvalidation:\n  strict_mode: true\n  required_properties: [\"name\", \"description\"]\n\nlogging:\n  level: \"INFO\"\n  format: \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n</code></pre></li> </ul>"},{"location":"references/extra/#programmatic-configuration","title":"Programmatic Configuration","text":"<pre><code>from bam_masterdata.config import Configuration\n\n# Load configuration\nconfig = Configuration.load_from_file(\"config.yaml\")\n\n# Override specific settings\nconfig.set(\"openbis.url\", \"https://my-openbis.com\")\nconfig.set(\"validation.strict_mode\", False)\n\n# Apply configuration\nConfiguration.apply(config)\n</code></pre>"},{"location":"references/extra/#working-with-entity-types","title":"Working with Entity Types","text":""},{"location":"references/extra/#how-to-create-custom-entity-types","title":"How to Create Custom Entity Types","text":"<pre><code>from bam_masterdata.metadata.entities import ObjectType\nfrom bam_masterdata.metadata.definitions import ObjectTypeDef, PropertyTypeAssignment\n\nclass MyCustomObject(ObjectType):\n    defs = ObjectTypeDef(\n        code=\"MY_CUSTOM_OBJECT\",\n        description=\"A custom object type for specific needs\"\n    )\n\n    custom_property = PropertyTypeAssignment(\n        code=\"CUSTOM_PROP\",\n        data_type=\"VARCHAR\",\n        property_label=\"Custom Property\",\n        description=\"A custom property for this object\",\n        mandatory=True,\n        section=\"Custom Section\"\n    )\n</code></pre>"},{"location":"references/extra/#how-to-query-available-properties","title":"How to Query Available Properties","text":"<pre><code>from bam_masterdata.datamodel.object_types import Sample\n\n# Get all available properties for a Sample\nsample = Sample()\nproperties = sample.get_property_metadata()\n\nfor prop_code, metadata in properties.items():\n    print(f\"Property: {prop_code}\")\n    print(f\"  Label: {metadata.property_label}\")\n    print(f\"  Type: {metadata.data_type}\")\n    print(f\"  Mandatory: {metadata.mandatory}\")\n    print()\n</code></pre>"},{"location":"references/extra/#how-to-validate-entity-data","title":"How to Validate Entity Data","text":"<pre><code>from bam_masterdata.checker.masterdata_validator import MasterdataValidator\n\n# Create validator\nvalidator = MasterdataValidator()\n\n# Validate your entity data\nentities_dict = {\"object_types\": {\"SAMPLE_001\": sample.to_dict()}}\nvalidation_result = validator.validate(entities_dict)\n\nif validation_result.is_valid:\n    print(\"\u2713 Validation passed\")\nelse:\n    print(\"\u2717 Validation failed:\")\n    for error in validation_result.errors:\n        print(f\"  - {error}\")\n</code></pre>"},{"location":"references/extra/#data-import-and-export","title":"Data Import and Export","text":""},{"location":"references/extra/#how-to-import-from-excel-files","title":"How to Import from Excel Files","text":"<p>[Image placeholder: Excel spreadsheet template showing the correct structure for masterdata import with headers, entity types, properties, and sample data rows.]</p> <pre><code>from bam_masterdata.excel.excel_to_entities import MasterdataExcelExtractor\n\n# Initialize extractor\nextractor = MasterdataExcelExtractor()\n\n# Load workbook\nimport openpyxl\nworkbook = openpyxl.load_workbook(\"masterdata.xlsx\")\n\n# Extract entities\nentities = extractor.excel_to_entities(workbook)\n\n# Access extracted data\nobject_types = entities.get(\"object_types\", {})\ndataset_types = entities.get(\"dataset_types\", {})\nprint(f\"Loaded {len(object_types)} object types\")\nprint(f\"Loaded {len(dataset_types)} dataset types\")\n</code></pre>"},{"location":"references/extra/#how-to-export-to-excel","title":"How to Export to Excel","text":"<pre><code>from bam_masterdata.cli.entities_to_excel import entities_to_excel\nfrom bam_masterdata.metadata.entities_dict import EntitiesDict\n\n# Prepare your entities data\nentities_dict = EntitiesDict({\n    \"object_types\": {\"SAMPLE_001\": sample.to_dict()},\n    \"dataset_types\": {},\n    \"collection_types\": {},\n    \"vocabulary_types\": {}\n})\n\n# Export to Excel\nentities_to_excel(entities_dict, \"output.xlsx\")\nprint(\"\u2713 Exported to output.xlsx\")\n</code></pre>"},{"location":"references/extra/#how-to-export-to-json","title":"How to Export to JSON","text":"<pre><code>import json\n\n# Single entity to JSON\nentity_json = sample.to_json()\n\n# Multiple entities to JSON\nentities_dict = {\n    \"object_types\": {\n        \"SAMPLE_001\": sample.to_dict()\n    }\n}\n\nwith open(\"entities.json\", \"w\") as f:\n    json.dump(entities_dict, f, indent=2)\n</code></pre>"},{"location":"references/extra/#how-to-export-to-rdf","title":"How to Export to RDF","text":"<pre><code>from bam_masterdata.cli.entities_to_rdf import entities_to_rdf, rdf_graph_init\n\n# Initialize RDF graph\ngraph = rdf_graph_init()\n\n# Add entities to graph\nentities_to_rdf(entities_dict, graph)\n\n# Save to file\ngraph.serialize(destination=\"entities.ttl\", format=\"turtle\")\nprint(\"\u2713 Exported to entities.ttl\")\n</code></pre>"},{"location":"references/extra/#working-with-openbis","title":"Working with OpenBIS","text":""},{"location":"references/extra/#how-to-connect-to-openbis","title":"How to Connect to OpenBIS","text":"<pre><code>from bam_masterdata.openbis.login import ologin\n\n# Connect to OpenBIS instance\nopenbis = ologin(\n    url=\"https://your-openbis-instance.com\",\n    username=\"your_username\",\n    password=\"your_password\"\n)\n\nprint(f\"\u2713 Connected to OpenBIS: {openbis.get_server_information()}\")\n</code></pre>"},{"location":"references/extra/#how-to-retrieve-entities-from-openbis","title":"How to Retrieve Entities from OpenBIS","text":"<pre><code>from bam_masterdata.openbis.get_entities import OpenbisEntities\n\n# Initialize entities extractor\nentities_extractor = OpenbisEntities(openbis)\n\n# Get all object types\nobject_types = entities_extractor.get_object_dict()\nprint(f\"Retrieved {len(object_types)} object types from OpenBIS\")\n\n# Get specific vocabulary\nvocabularies = entities_extractor.get_vocabulary_dict()\nstorage_formats = vocabularies.get(\"STORAGE_FORMAT\", {})\nprint(f\"Storage format terms: {list(storage_formats.get('terms', {}).keys())}\")\n</code></pre>"},{"location":"references/extra/#how-to-push-data-to-openbis","title":"How to Push Data to OpenBIS","text":"<p>[Image placeholder: Screenshot of OpenBIS interface showing uploaded masterdata with entity browser and property views.]</p> <pre><code># Create entities in OpenBIS\nfor obj_code, obj_data in object_types.items():\n    obj_type = ObjectType.from_dict(obj_data)\n    openbis_obj = obj_type.to_openbis()\n\n    # Register with OpenBIS\n    result = openbis.create_object_type(openbis_obj)\n    print(f\"\u2713 Created object type: {obj_code}\")\n</code></pre>"},{"location":"references/extra/#command-line-operations","title":"Command Line Operations","text":""},{"location":"references/extra/#how-to-use-the-cli-for-bulk-operations","title":"How to Use the CLI for Bulk Operations","text":"<pre><code># Export all masterdata to Excel\nbam_masterdata export_to_excel masterdata_export.xlsx\n\n# Export specific entity types to JSON\nbam_masterdata export_to_json --entity-types object_types dataset_types output.json\n\n# Run consistency checker\nbam_masterdata checker --verbose\n\n# Fill masterdata from OpenBIS\n# environment variables OPENBIS_USERNAME and OPENBIS_PASSWORD are required for authentication\nbam_masterdata fill_masterdata --url https://openbis.example.com\n</code></pre>"},{"location":"references/extra/#how-to-generate-code-from-masterdata","title":"How to Generate Code from Masterdata","text":"<pre><code>from bam_masterdata.cli.fill_masterdata import MasterdataCodeGenerator\n\n# Initialize code generator with entities data\ngenerator = MasterdataCodeGenerator(\n    objects=object_types,\n    datasets=dataset_types,\n    collections=collection_types,\n    vocabularies=vocabulary_types\n)\n\n# Generate Python code for dataset types\ndataset_code = generator.generate_dataset_types()\n\n# Save to file\nwith open(\"generated_dataset_types.py\", \"w\") as f:\n    f.write(dataset_code)\n\nprint(\"\u2713 Generated dataset types code\")\n</code></pre>"},{"location":"references/extra/#advanced-usage","title":"Advanced Usage","text":""},{"location":"references/extra/#how-to-create-custom-validators","title":"How to Create Custom Validators","text":"<pre><code>from bam_masterdata.checker.masterdata_validator import MasterdataValidator\n\nclass CustomValidator(MasterdataValidator):\n    def validate_custom_rules(self, entities_dict):\n        \"\"\"Add custom validation rules.\"\"\"\n        errors = []\n\n        # Example: Check that all samples have required properties\n        for obj_code, obj_data in entities_dict.get(\"object_types\", {}).items():\n            if obj_data.get(\"code\", \"\").startswith(\"SAMPLE_\"):\n                required_props = [\"material_type\", \"dimensions\"]\n                for prop in required_props:\n                    if prop not in obj_data.get(\"properties\", {}):\n                        errors.append(f\"Sample {obj_code} missing required property: {prop}\")\n\n        return errors\n\n# Use custom validator\nvalidator = CustomValidator()\ncustom_errors = validator.validate_custom_rules(entities_dict)\n</code></pre>"},{"location":"references/extra/#how-to-handle-large-datasets","title":"How to Handle Large Datasets","text":"<pre><code>import multiprocessing\nfrom concurrent.futures import ProcessPoolExecutor\n\ndef process_entity_batch(entities_batch):\n    \"\"\"Process a batch of entities.\"\"\"\n    results = []\n    for entity_data in entities_batch:\n        # Process individual entity\n        entity = ObjectType.from_dict(entity_data)\n        results.append(entity.to_dict())\n    return results\n\n# Process entities in parallel\nentities_list = list(object_types.values())\nbatch_size = 100\nbatches = [entities_list[i:i+batch_size] for i in range(0, len(entities_list), batch_size)]\n\nwith ProcessPoolExecutor() as executor:\n    results = list(executor.map(process_entity_batch, batches))\n\nprint(f\"\u2713 Processed {len(entities_list)} entities in {len(batches)} batches\")\n</code></pre>"},{"location":"references/extra/#how-to-extend-entity-relationships","title":"How to Extend Entity Relationships","text":"<pre><code># Add relationships between entities\nsample = Sample(code=\"SAMPLE_001\")\ninstrument = Instrument(code=\"INSTR_001\")\n\n# Create relationship\nsample.add_relationship(\"measured_with\", instrument)\n\n# Access relationships\nrelationships = sample.get_relationships()\nprint(f\"Sample relationships: {relationships}\")\n</code></pre>"},{"location":"references/extra/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":""},{"location":"references/extra/#memory-issues-with-large-files","title":"Memory Issues with Large Files","text":"<pre><code># Use streaming for large Excel files\nimport pandas as pd\n\n# Read Excel in chunks\nchunk_size = 1000\nfor chunk in pd.read_excel(\"large_file.xlsx\", chunksize=chunk_size):\n    # Process chunk\n    entities_batch = extractor.process_chunk(chunk)\n    # Save incrementally\n</code></pre>"},{"location":"references/extra/#performance-optimization","title":"Performance Optimization","text":"<pre><code># Use caching for repeated operations\nfrom functools import lru_cache\n\n@lru_cache(maxsize=128)\ndef get_cached_entity(entity_code):\n    return entity_registry.get(entity_code)\n\n# Batch operations instead of individual calls\nentities_to_create = []\nfor entity_data in batch_data:\n    entities_to_create.append(ObjectType.from_dict(entity_data))\n\n# Create all at once\nresult = openbis.create_objects(entities_to_create)\n</code></pre>"},{"location":"references/glossary/","title":"Glossary","text":"<p>The following terms are used throughout <code>bam-masterdata</code> and are defined here for clarity.</p>"},{"location":"references/glossary/#masterdata","title":"Masterdata","text":"<p>The term Masterdata describes all information structures and plugins that are used to define metadata in openBIS (i.e., masterdata = \"meta-metadata\"). It includes</p> <ul> <li>Entity types (Collection, Object, Dataset types)</li> <li>Property types</li> <li>Controlled vocabularies</li> <li>Related scripts (e.g., dynamic property plugins, entity validation scripts)</li> </ul>"},{"location":"tutorials/getting_started/","title":"Getting Started with <code>bam-masterdata</code>","text":"<p>This tutorial will guide you through your first interaction with the <code>bam-masterdata</code> package, helping you understand its core concepts and basic functionality.</p>"},{"location":"tutorials/getting_started/#what-is-bam-masterdata","title":"What is <code>bam-masterdata</code>?","text":"<p>The <code>bam-masterdata</code> is a Python package designed to help administrators and users to manage the Masterdata/schema definitions. It provides with a set of Python classes and utilities for working with different types of entities in the openBIS Research Data Management (RDM) system. It also contains the Masterdata definitions used at the Bundesanstalt f\u00fcr Materialforschung und -pr\u00fcfung (BAM) in the context of Materials Science and Engineering research.</p> <p>[Image placeholder: Architecture overview diagram showing the relationship between BAM Masterdata, openBIS, and the BAM Data Store. The diagram should illustrate data flow and the role of masterdata schemas in the system.]</p> <p>The <code>bam-masterdata</code> provides you with tools to:</p> <ul> <li>Export the Masterdata from your openBIS instance.</li> <li>Update the Masterdata in your openBIS instance.</li> <li>Export/Import from different formats: Excel, Python, RDF/XML, JSON.</li> <li>Check consistency of the Masterdata with respect to a ground truth.</li> <li>Automatically parse metainformation in your openBIS instance.</li> </ul> <p>Prerequisites</p> <ul> <li>Basic Python and openBIS knowledge.</li> <li>A system with Python 3.10 or higher.</li> <li>Knowledge of virtual environments, CLI usage, IDEs such as VSCode, and GitHub.</li> </ul> <p>Warning</p> <p>Note all steps in this documentation are done in Ubuntu 22.04. All the commands in the terminal need to be modified if you work from Windows.</p>"},{"location":"tutorials/getting_started/#installation-and-setup","title":"Installation and Setup","text":""},{"location":"tutorials/getting_started/#create-an-empty-test-directory","title":"Create an empty test directory","text":"<p>We will test the basic functionalities of <code>bam-masterdata</code> in an empty directory. Open your terminal and type: <pre><code>mkdir test_bm\ncd test_bm/\n</code></pre></p>"},{"location":"tutorials/getting_started/#create-a-virtual-environment","title":"Create a Virtual Environment","text":"<p>We strongly recommend using a virtual environment to avoid conflicts with other packages.</p> <p>Using venv: <pre><code>python3 -m venv .venv\nsource .venv/bin/activate\n</code></pre></p> <p>Using conda: <pre><code>conda create --name .venv python=3.10  # or any version 3.10 &lt;= python &lt;= 3.12\nconda activate .venv\n</code></pre></p>"},{"location":"tutorials/getting_started/#install-the-package","title":"Install the Package","text":"<p><code>bam-masterdata</code> is part of the PyPI registry and can be installed via pip: <pre><code>pip install --upgrade pip\npip install bam-masterdata\n</code></pre></p> <p>Faster Installation</p> <p>For faster installation, you can use <code>uv</code>: <pre><code>pip install uv\nuv pip install bam-masterdata\n</code></pre></p>"},{"location":"tutorials/getting_started/#verify-installation","title":"Verify Installation","text":"<p>You can verify that the installation was successful. Open a Python script and write: <pre><code>from importlib.metadata import version\n\n\nprint(f\"BAM Masterdata version: {version(\"bam_masterdata\")}\")\n</code></pre></p> <p>And running in your terminal: <pre><code>python &lt;path-to-Python-script&gt;\n</code></pre></p> <p>This should return the version of the installed package.</p>"},{"location":"tutorials/getting_started/#your-first-bam-masterdata-experience","title":"Your First <code>bam-masterdata</code> Experience","text":""},{"location":"tutorials/getting_started/#understanding-entity-types","title":"Understanding Entity Types","text":"<p>The BAM Masterdata system organizes information into different entity types:</p> <ul> <li>Object Types: Physical or conceptual objects (samples, instruments, people)</li> <li>Collection Types: Groups of related objects</li> <li>Dataset Types: Data files and their metadata</li> <li>Vocabulary Types: Controlled vocabularies for standardized values</li> </ul> <p>[Image placeholder: Entity relationship diagram showing the four main entity types and their relationships. Should include sample instances of each type.]</p> Deprecating Collection Types and Dataset Types <p>As of September 2025, the development of new Collection and Dataset types is stalled. We will use the abstract concepts only, i.e., a Collection Type is a class used to add objects to it and their relationships, and a Dataset Type is a class to attach raw data files to it.</p>"},{"location":"tutorials/getting_started/#overview-of-the-object-types","title":"Overview of the Object Types","text":"<p>The central ingredients for defining data models associated with a research activity are the Object Types. These are classes inheriting from an abstract class called <code>ObjectType</code> and with two types of attributes:</p> <ul> <li><code>defs</code>: The definitions of the Object Type. These attributes do not change when filling with data the object.</li> <li><code>properties</code>: The list of properties assigned to an object. These attributes are filled when assigning data to the object.</li> </ul> <p>All accessible object types are defined as Python classes in <code>bam_masterdata/datamodel/object_types.py</code>. Each object type has a set of assigned properties (metadata fields), some of which are mandatory and some are optional. For example: <pre><code>class Chemical(ObjectType):\n    defs = ObjectTypeDef(\n        code=\"CHEMICAL\",\n        description=\"\"\"Chemical Substance//Chemische Substanz\"\"\",\n        generated_code_prefix=\"CHEM\",\n    )\n\n    name = PropertyTypeAssignment(\n        code=\"$NAME\",\n        data_type=\"VARCHAR\",\n        property_label=\"Name\",\n        description=\"\"\"Name\"\"\",\n        mandatory=True,\n        show_in_edit_views=False,\n        section=\"General Information\",\n    )\n\n    alias = PropertyTypeAssignment(\n        code=\"ALIAS\",\n        data_type=\"VARCHAR\",\n        property_label=\"Alternative Name\",\n        description=\"\"\"e.g. abbreviation or nickname//z.B. Abk\u00fcrzung oder Spitzname\"\"\",\n        mandatory=False,\n        show_in_edit_views=False,\n        section=\"General Information\",\n    )\n\n    # ... more PropertyTypeAssignment\n</code></pre></p> <p>You can read more in Schema Definitions to learn about the definitions of Object Types and how to assign properties.</p>"},{"location":"tutorials/getting_started/#creating-your-first-entity","title":"Creating Your First Entity","text":"<p>Let's create/instantiate a simple experimental step object:</p> <pre><code>from bam_masterdata.datamodel.object_types import ExperimentalStep\n\n\n# Create a new experimental step instance\nstep = ExperimentalStep(\n    name=\"SEM measurement\",\n    finished_flag=True,\n)\nprint(step) # prints the object type and its assigned properties\n</code></pre> <p>This will print: <pre><code>SEM measurement:ExperimentalStep(name=\"SEM measurement\", finished_flag=True)\n</code></pre></p> <p>You can assign values to other properties after instantiation as well:</p> <p><pre><code>step.show_in_project_overview = True\nprint(step)\n</code></pre> This will print: <pre><code>SEM measurement:ExperimentalStep(name=\"SEM measurement\", show_in_project_overview=True, finished_flag=True)\n</code></pre></p> <p>If the type of the property does not match the expected type, an error will be shown. For example, <code>ExperimentalStep.show_in_project_overview</code> is a boolean, hence: <pre><code>step.show_in_project_overview = 2\n</code></pre> will return: <pre><code>TypeError: Invalid type for 'show_in_project_overview': Expected bool, got int\n</code></pre></p>"},{"location":"tutorials/getting_started/#available-properties-for-an-object-type","title":"Available properties for an Object Type","text":"<p>To explore which attributes are available for a given type, check its <code>_property_metadata</code>.</p> <p><pre><code>from bam_masterdata.datamodel.object_types import ExperimentalStep\n\nstep = ExperimentalStep()\nprint(list(step._property_metadata.keys()))\n</code></pre> will return: <pre><code>['name', 'show_in_project_overview', 'finished_flag', 'start_date', ...]\n</code></pre></p> <p>If you want a detailed list of the <code>PropertyTypeAssignment</code> assigned to an Object Type, you can print <code>properties</code> instead.</p>"},{"location":"tutorials/getting_started/#data-types","title":"Data types","text":"<p>The data types for each assigned property are defined according to openBIS. These have their direct counterpart in Python types. The following table shows the equivalency of each type:</p> DataType Python type Example assignment <code>BOOLEAN</code> <code>bool</code> <code>myobj.flag = True</code> <code>CONTROLLEDVOCABULARY</code> <code>str</code> (enum term code) <code>myobj.status = \"ACTIVE\"</code> (must match allowed vocabulary term) <code>DATE</code> <code>datetime.date</code> <code>myobj.start_date = datetime.date(2025, 9, 29)</code> <code>HYPERLINK</code> <code>str</code> <code>myobj.url = \"https://example.com\"</code> <code>INTEGER</code> <code>int</code> <code>myobj.count = 42</code> <code>MULTILINE_VARCHAR</code> <code>str</code> <code>myobj.notes = \"Line 1\\nLine 2\\nLine 3\"</code> <code>OBJECT</code> <code>ObjectType</code> or <code>str</code> (path) <code>myobj.parent = person_obj</code> or <code>myobj.parent = \"/SPACE/PROJECT/PERSON_001\"</code> <code>REAL</code> <code>float</code> <code>myobj.temperature = 21.7</code> <code>TIMESTAMP</code> <code>datetime.datetime</code> <code>myobj.created_at = datetime.datetime.now()</code> <code>VARCHAR</code> <code>str</code> <code>myobj.name = \"Test sample\"</code> <code>XML</code> <code>str</code> (XML string) <code>myobj.config = \"&lt;root&gt;&lt;tag&gt;value&lt;/tag&gt;&lt;/root&gt;\"</code>"},{"location":"tutorials/getting_started/#working-with-controlled-vocabularies","title":"Working with controlled vocabularies","text":"<p>Many object types have fields that only accept certain values (controlled vocabularies). Use the value codes found in bam_masterdata/datamodel/vocabulary_types.py or check the class directly: <pre><code>from bam_masterdata.datamodel.vocabulary_types import StorageValidationLevel\n\n\nprint([term.code for term in StorageValidationLevel().terms])\n</code></pre> will return: <pre><code>['BOX', 'BOX_POSITION', 'RACK']\n</code></pre></p> <p>Thus we can assign only: <pre><code>from bam_masterdata.datamodel.object_types import Storage\n\n\nstore = Storage()\nstore.storage_storage_validation_level = \"BOX\"  # CONTROLLEDVOCABULARY\n</code></pre></p> <p>Tip</p> <p>When assigning values to properties assigned to Object Types, we recommend carefully handling potential errors. This will allow your scripts to work without interruption and with a total control of conflictive lines.</p>"},{"location":"tutorials/getting_started/#working-with-object-references","title":"Working with object references","text":"<p>Some object types have properties that reference other objects in openBIS (when data type is <code>OBJECT</code>). For example, an <code>Instrument</code> might have a <code>responsible_person</code> property that references a <code>Person</code> object.</p> <p>There are two ways to assign object references:</p>"},{"location":"tutorials/getting_started/#1-using-an-object-instance","title":"1. Using an Object Instance","text":"<p>If you're creating objects in the same batch, you can directly reference the object instance.</p> <pre><code>from bam_masterdata.datamodel.object_types import Instrument\n\n# Create a person object\nperson = Person(name=\"Dr. Jane Smith\")\nperson.code = \"PERSON_001\"  # Must have a code to be used as reference\n\n# Create an instrument and reference the person\ninstrument = Instrument(name=\"Microscope A\")\ninstrument.responsible_person = person  # Direct object reference\n</code></pre> <p>Object must have a code</p> <p>When using object instances as references, they must have a <code>code</code> attribute set. Otherwise, a <code>ValueError</code> will be raised.</p>"},{"location":"tutorials/getting_started/#2-using-a-path-string","title":"2. Using a Path String","text":"<p>If the object already exists in openBIS, you can reference it using its identifier path:</p> <pre><code># Reference an existing object using its path\ninstrument = Instrument(name=\"Microscope B\")\n\n# Path format: /{space}/{project}/{collection}/{object}\ninstrument.responsible_person = \"/LAB_SPACE/PEOPLE_LIST_PROJECT/STAFF_COLLECTION/PERSON_001\"\n\n# Or without collection: /{space}/{project}/{object}\ninstrument.responsible_person = \"/LAB_SPACE/PEOPLE_LIST_PROJECT/PERSON_001\"\n</code></pre> <p>Path format validation</p> <p>The path must:</p> <ul> <li>Start with a forward slash <code>/</code></li> <li>Have either 3 parts (space/project/object) or 4 parts (space/project/collection/object)</li> <li>Match an existing object identifier in openBIS</li> </ul>"},{"location":"tutorials/getting_started/#when-to-use-each-approach","title":"When to use each approach","text":"<ul> <li>Use object instances when creating multiple related objects in the same parsing operation. This option is the alternative to defining parent-child relationships if in an Object Type definition you have assigned a property of a certain data type OBJECT.</li> <li>Use path strings when referencing existing objects in openBIS that were created separately.</li> </ul> <p>Example combining both approaches:</p> <pre><code>from bam_masterdata.metadata.entities import CollectionType\n\ncollection = CollectionType()\n\n# Create and add a person\nperson = Person(name=\"Dr. Smith\")\nperson.code = \"PERSON_001\"\nperson_id = collection.add(person)\n\n# Create instrument referencing the new person (by instance)\ninstrument1 = Instrument(name=\"Instrument 1\")\ninstrument1.responsible_person = person\ncollection.add(instrument1)\n\n# Create another instrument referencing an existing person in openBIS (by path)\ninstrument2 = Instrument(name=\"Instrument 2\")\ninstrument2.responsible_person = \"/MY_SPACE/MY_PROJECT/EXISTING_PERSON_002\"\ncollection.add(instrument2)\n</code></pre>"},{"location":"tutorials/getting_started/#saving-your-object-types-instances-in-a-collection","title":"Saving your Object Types instances in a collection","text":"<p>Most usecases end with saving the Object Types and their field values in a colletion for further use. This can be done by adding those Object Types in a <code>CollectionType</code> like: <pre><code>from bam_masterdata.metadata.entities import CollectionType\nfrom bam_masterdata.datamodel.object_types import ExperimentalStep\n\n\nstep_1 = ExperimentalStep(name=\"Step 1\")\n\ncollection = CollectionType()\nstep_1_id = collection.add(step_1)\nprint(collection)\n</code></pre></p> <p>This will return the <code>CollectionType</code> with the attached objects: <pre><code>CollectionType(attached_objects={'EXP8f78245b': ExperimentalStep(name='Step 1')}, relationships={})\n</code></pre></p> <p>You can also add relationships between objects by using their ids when attached to the <code>CollectionType</code>: <pre><code>from bam_masterdata.metadata.entities import CollectionType\nfrom bam_masterdata.datamodel.object_types import ExperimentalStep\n\n\nstep_1 = ExperimentalStep(name=\"Step 1\")\nstep_2 = ExperimentalStep(name=\"Step 2\")\n\ncollection = CollectionType()\nstep_1_id = collection.add(step_1)\nstep_2_id = collection.add(step_2)\n_ = collection.add_relationship(parent_id=step_1_id, child_id=step_2_id)\nprint(collection)\n</code></pre> will return: <pre><code>CollectionType(attached_objects={'EXP3e6f674e': ExperimentalStep(name='Step 1'), 'EXP87b64b62': ExperimentalStep(name='Step 2')}, relationships={'EXP3e6f674e&gt;&gt;EXP87b64b62': ('EXP3e6f674e', 'EXP87b64b62')})\n</code></pre></p>"},{"location":"tutorials/getting_started/#converting-object-types","title":"Converting Object Types","text":"<p>The package supports various export formats for working with Object Types. These divide in two main purposes:</p> <ul> <li>Exporting the schema definitions: this is done using the methods <code>model_to_&lt;format&gt;()</code>.</li> <li>Exporting the data model: this is done using the methods <code>to_&lt;format()&gt;</code>.</li> </ul> <p>For example: <pre><code># Convert data model to dictionary\nstep_dict = step.to_dict()\n# Convert schema to dictionary\nstep_schema_dict = step.model_to_dict()\nprint(step_dict)  # print: {'name': 'SEM measurement', 'finished_flag': True}\nprint(step_schema_dict)  # print: {'properties': [{...}, {...}, {...}, {...}, {...}, {...}, {...}, {...}, {...}, {...}, {...}, {...}, {...}, {...}], 'defs': {'code': 'EXPERIMENTAL_STEP', 'description': 'Experimental Step (generic)//Experimenteller Schritt (allgemein)', 'iri': None, 'id': 'ExperimentalStep', 'row_location': None, 'validation_script': None, 'generated_code_prefix': 'EXP', 'auto_generate_codes': True}}\n\n# Convert to JSON\nstep_json = step.to_json()\nstep_schema_json = step.model_to_json()\n</code></pre></p> <p>The possible export formats can be found in the API Reference documentation page.</p>"},{"location":"tutorials/getting_started/#working-with-real-raw-data","title":"Working with real raw data","text":"<p>In order to work with raw data, you will need to create a script (also called parser) to read the i/o files metainformation, and map such information to the corresponding Object Types classes and properties. You can find more information in How-to: Create new parsers</p>"},{"location":"tutorials/getting_started/#using-the-command-line-interface","title":"Using the Command Line Interface","text":"<p>The package provides a CLI for common operations:</p> <pre><code># Export current masterdata to Excel\nbam_masterdata export_to_excel --export-dir=example_export_excel\n\n# Fill masterdata from OpenBIS\nbam_masterdata fill_masterdata\n</code></pre> <p>A comprehensive explanation of all options can be found in the terminal when adding the <code>--help</code> flag at the end of the command. For example: <pre><code>bam_masterdata export_to_json --help\n</code></pre> will produce: <pre><code>Usage: bam_masterdata export_to_json [OPTIONS]\n\n  Export entities to JSON files to the `./artifacts/` folder.\n\nOptions:\n  --force-delete BOOLEAN  (Optional) If set to `True`, it will delete the\n                          current `./artifacts/` folder and create a new one.\n                          Default is `False`.\n  --python-path TEXT      (Optional) The path to the individual Python module\n                          or the directory containing the Python modules to\n                          process the datamodel. Default is the `/datamodel/`\n                          directory.\n  --export-dir TEXT       The directory where the JSON files will be exported.\n                          Default is `./artifacts`.\n  --single-json BOOLEAN   Whether the export to JSON is done to a single JSON\n                          file. Default is False.\n  --help                  Show this message and exit.\n</code></pre></p>"},{"location":"tutorials/getting_started/#next-steps","title":"Next Steps","text":"<p>Now that you've completed this tutorial, you can:</p> <ol> <li>Explore the How-to Guides: Learn specific tasks and workflows when using <code>bam-masterdata</code>.</li> <li>Read the Explanations: Understand the concepts behind the system.</li> <li>Browse the API Reference: Dive deep into specific classes and methods.</li> </ol>"},{"location":"tutorials/getting_started/#development-setup","title":"Development Setup","text":"<p>If you want to contribute or modify the package:</p> <pre><code>git clone https://github.com/BAMresearch/bam-masterdata.git\ncd bam-masterdata\npython3 -m venv .venv\nsource .venv/bin/activate\n./scripts/install_python_dependencies.sh\n</code></pre> <p>Read the <code>README.md</code> for more details.</p>"},{"location":"tutorials/parsing/","title":"Automating Metadata Injection with the Parser API","text":"<p>This tutorial teaches you how to automate metadata injection from multiple files into openBIS using the <code>run_parser()</code> function and custom parsers. This is useful when you have many files (e.g., experimental data, instrument outputs, logs) and want to systematically extract metadata and push it to your openBIS instance.</p> <p>By the end of this tutorial, you will be able to:</p> <ul> <li>Understand the purpose and workflow of <code>run_parser()</code>.</li> <li>Create a custom parser that extracts metadata from files.</li> <li>Use the parser to inject metadata into openBIS automatically.</li> <li>Handle relationships between objects.</li> <li>Update existing objects in openBIS.</li> </ul> <p>Prerequisites</p> <ul> <li>Python \u2265 3.10 installed</li> <li>bam-masterdata package installed (<code>pip install bam-masterdata</code>)</li> <li>Basic understanding of Object Types, as explained in Tutorial - Getting started: Your first <code>bam-masterdata</code> experience</li> <li>An openBIS instance with login credentials (for local testing)</li> </ul> Working with openBIS <p>This tutorial includes examples that connect to openBIS. If you don't have access to an openBIS instance, you can still follow along and understand the concepts by focusing on the parser creation and testing sections.</p> <p>We recommend contacting your openBIS admin team to get access credentials.</p>"},{"location":"tutorials/parsing/#understanding-the-etl-workflow","title":"Understanding the ETL Workflow","text":"<p>When working with research data, you often need to:</p> <ol> <li>Extract metadata from raw files (CSV, JSON, XML, etc.).</li> <li>Transform the metadata into structured objects and define their relations.</li> <li>Load the structured objects into a database system like openBIS.</li> </ol> <p>The <code>run_parser()</code> function automates this ETL (Extract-Transform-Load) pipeline. The process can be summarized in the following diagram:</p> <pre><code>graph LR\n    A[openBIS login] --&gt; B[Specify Space/Project/Collection]\n    B --&gt; C[Raw Files upload]\n    C --&gt; D[Custom Parser]\n    D --&gt; E[run_parser]</code></pre> <p>The <code>run_parser()</code> then maps the <code>bam-masterdata</code> objects and their relationships into openBIS native objects, relationships, and datasets:</p> <pre><code>graph LR\n    A[run_parser] --&gt; B[openBIS Objects]\n    A --&gt; C[openBIS Datasets]\n    A --&gt; D[openBIS Relationships]</code></pre> <p>The whole process then depends on the inputs and output <code>run_parser()</code>:</p> <ul> <li>Inputs<ul> <li><code>space_name</code>: string, this coincides with the space name you have associated in your openBIS instance</li> <li><code>project_name</code>: string, the name of the project where you want to store the parsed objects</li> <li><code>collection_name</code>: string, optional, the name of the collection you want to store the parsed objects</li> <li><code>collection_type</code>: string, optional, the type of collection to create in openBIS. Options are <code>\"COLLECTION\"</code> or <code>\"DEFAULT_EXPERIMENT\"</code>. Defaults to <code>\"COLLECTION\"</code>.</li> <li><code>files_parser</code>: dict, a dictionary where the keys are the parser class instances (see below) and the values are the files you want to pass to the specific parser class</li> </ul> </li> <li>Output: the metadata objects and relationships mapped to your space/project/collection in openBIS</li> </ul>"},{"location":"tutorials/parsing/#creating-your-first-parser","title":"Creating Your First Parser","text":"<p>Let's start by creating a simple parser that reads metadata from JSON files. You can read more in How-to - Create new parsers.</p> <p>Imagine you have experimental setup JSON files that look like this:</p> <pre><code>{\n  \"experiment_name\": \"Tensile Test 001\",\n  \"date\": \"2024-10-15\",\n  \"temperature\": 23.5,\n  \"operator\": \"Dr. Smith\"\n}\n</code></pre>"},{"location":"tutorials/parsing/#step-1-create-the-parser-class","title":"Step 1: Create the Parser Class","text":"<p>All parsers must inherit from <code>AbstractParser</code> and implement the <code>parse()</code> method:</p> <pre><code>import json\nimport datetime\nfrom bam_masterdata.parsing import AbstractParser\nfrom bam_masterdata.datamodel.object_types import ExperimentalStep\n\n\nclass ExperimentSetupParser(AbstractParser):\n    \"\"\"Parser for experimental setup JSON files.\"\"\"\n\n    def parse(self, files, collection, logger):\n        \"\"\"\n        Parse experimental setup files and populate the collection.\n\n        Args:\n            files (list[str]): List of file paths to parse\n            collection (CollectionType): Collection to populate with objects\n            logger: Logger for status messages\n        \"\"\"\n        for file_path in files:\n            logger.info(f\"Parsing file: {file_path}\")\n\n            # Read the JSON file\n            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                data = json.load(f)\n\n            # Create an ExperimentalStep object\n            experiment = ExperimentalStep(\n                name=data.get(\"experiment_name\"),\n                start_date=datetime.datetime.strptime(\n                    data.get(\"date\"), \"%Y-%m-%d\"\n                ).date(),\n            )\n\n            # Add to collection\n            exp_id = collection.add(experiment)\n            logger.info(f\"Added experiment with ID: {exp_id}\")\n</code></pre> Mapping metadata to openBIS <p>Not all metadata fields you have defined in your files are necessarily defined in your openBIS instance. If you feel a field is important but missing in openBIS, please, contact the admins of your instance to include such field.</p> <p>For <code>bam-masterdata</code>, you can open an issue with your prefered changes. The whole list of Object Types and their metadata is defined here.</p>"},{"location":"tutorials/parsing/#step-2-test-the-parser-locally","title":"Step 2: Test the Parser Locally","text":"<p>Before using <code>run_parser()</code>, test your parser to ensure it works correctly:</p> <pre><code>from bam_masterdata.metadata.entities import CollectionType\nfrom bam_masterdata.logger import logger\n\n# Create a test collection\ntest_collection = CollectionType()\n\n# Create an instance of your parser\nparser = ExperimentSetupParser()\n\n# Parse some test files\ntest_files = [\"experiment_001.json\", \"experiment_002.json\"]\nparser.parse(test_files, test_collection, logger)\n\n# Check what was added\nprint(f\"Objects added: {len(test_collection.attached_objects)}\")\nfor obj_id, obj in test_collection.attached_objects.items():\n    print(f\"{obj_id}: {obj}\")\n</code></pre> <p>This will output something like: <pre><code>Objects added: 2\nEXP12345678: ExperimentalStep(name='Tensile Test 001', start_date=datetime.date(2024, 10, 15))\nEXP87654321: ExperimentalStep(name='Tensile Test 002', start_date=datetime.date(2024, 10, 16))\n</code></pre></p>"},{"location":"tutorials/parsing/#using-run_parser-to-upload-to-openbis","title":"Using run_parser() to Upload to openBIS","text":"<p>Once your parser is tested, use <code>run_parser()</code> to inject the metadata into openBIS.</p>"},{"location":"tutorials/parsing/#step-1-connect-to-openbis","title":"Step 1: Connect to openBIS","text":"<p>First, establish a connection to your openBIS instance:</p> <pre><code>from pybis import Openbis\n\n# Connect to openBIS\nopenbis = Openbis(url=\"https://your-openbis-instance.com\")\nopenbis.login(username=\"your_username\", password=\"your_password\", save_token=True)\n</code></pre> <p>Token-based Authentication</p> <p>Using <code>save_token=True</code> saves your session token so you don't need to re-enter credentials for subsequent connections.</p>"},{"location":"tutorials/parsing/#step-2-run-the-parser","title":"Step 2: Run the Parser","text":"<p>Now, use <code>run_parser()</code> to automate the entire workflow:</p> <pre><code>from bam_masterdata.cli.run_parser import run_parser\n\n# Define which parser to use and which files to parse\nfiles_parser = {\n    ExperimentSetupParser(): [\n        \"experiment_001.json\",\n        \"experiment_002.json\",\n        \"experiment_003.json\"\n    ]\n}\n\n# Run the parser\nrun_parser(\n    openbis=openbis,\n    space_name=\"MY_RESEARCH_SPACE\",\n    project_name=\"MY_PROJECT\",\n    collection_name=\"MY_EXPERIMENT_COLLECTION\",\n    files_parser=files_parser,\n    collection_type=\"COLLECTION\"  # Optional: defaults to \"COLLECTION\"\n)\n</code></pre> <p>Collection Types</p> <p>You can choose between two collection types: - <code>\"COLLECTION\"</code> (default): A general-purpose collection type - <code>\"DEFAULT_EXPERIMENT\"</code>: A collection type designed for experiments with additional metadata fields like start/end dates, experimental goals, etc.</p> <p>If you don't specify <code>collection_type</code>, it defaults to <code>\"COLLECTION\"</code>. We recommend keeping the default <code>collection_type</code>, as future openBIS releases will get rid of the collection concept and we will, as a result, deprecate this feature.</p>"},{"location":"tutorials/parsing/#what-happens-next","title":"What Happens Next?","text":"<p>When you call <code>run_parser()</code>, the following steps occur automatically:</p> <ol> <li>Space and Project Setup: The function checks if the specified space and project exist. If not, it uses your user space and creates a new project.</li> <li>Collection Creation: A collection is created (or retrieved if it exists) to group the objects. If not specified, the objects will be directly created under the project.</li> <li>Parsing: Each parser processes its files and populates a <code>CollectionType</code>.</li> <li>Object Creation: Objects from the collection are created in openBIS with their metadata.</li> <li>File Upload: The raw files are uploaded as datasets to the collection.</li> <li>Relationship Mapping: Any parent-child relationships are established.</li> </ol>"},{"location":"tutorials/parsing/#adding-multiple-parsers","title":"Adding Multiple Parsers","text":"<p>You can use multiple parsers in a single <code>run_parser()</code> call to process different types of files:</p> <pre><code>from bam_masterdata.cli.run_parser import run_parser\n\n# Use multiple parsers for different file types\nfiles_parser = {\n    ExperimentSetupParser(): [\n        \"experiment_001.json\",\n        \"experiment_002.json\"\n    ],\n    MaterialSampleParser(): [\n        \"sample_data_A.csv\",\n        \"sample_data_B.csv\"\n    ],\n    TestResultParser(): [\n        \"results_20241015.xml\"\n    ]\n}\n\nrun_parser(\n    openbis=openbis,\n    space_name=\"MY_RESEARCH_SPACE\",\n    project_name=\"MY_PROJECT\",\n    collection_name=\"MULTI_DATA_COLLECTION\",\n    files_parser=files_parser,\n)\n</code></pre> <p>Each parser handles its specific file format, and all results are combined into a single collection.</p>"},{"location":"tutorials/parsing/#creating-relationships-between-objects","title":"Creating Relationships Between Objects","text":"<p>Often, objects in your data model are related. For example, a measurement might be related to a sample, or an experimental step might be a child of another step.</p>"},{"location":"tutorials/parsing/#example-linking-samples-to-experiments","title":"Example: Linking Samples to Experiments","text":"<pre><code>import json\nfrom bam_masterdata.parsing import AbstractParser\nfrom bam_masterdata.datamodel.object_types import ExperimentalStep, Sample\n\n\nclass ExperimentWithSamplesParser(AbstractParser):\n    \"\"\"Parser that creates experiments and links samples to them.\"\"\"\n\n    def parse(self, files, collection, logger):\n        for file_path in files:\n            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                data = json.load(f)\n\n            # Create the main experiment\n            experiment = ExperimentalStep(name=data.get(\"experiment_name\"))\n            exp_id = collection.add(experiment)\n            logger.info(f\"Created experiment: {exp_id}\")\n\n            # Create samples and link them to the experiment\n            for sample_data in data.get(\"samples\", []):\n                sample = Sample(name=sample_data.get(\"sample_name\"))\n                sample_id = collection.add(sample)\n\n                # Create parent-child relationship\n                # The experiment is the parent, samples are children\n                collection.add_relationship(parent_id=exp_id, child_id=sample_id)\n                logger.info(f\"Linked sample {sample_id} to experiment {exp_id}\")\n</code></pre> <p>Example JSON file structure: <pre><code>{\n  \"experiment_name\": \"Batch Processing 2024-10\",\n  \"samples\": [\n    {\"sample_name\": \"Sample A\"},\n    {\"sample_name\": \"Sample B\"},\n    {\"sample_name\": \"Sample C\"}\n  ]\n}\n</code></pre></p> <p>When <code>run_parser()</code> processes this, it will: 1. Create the experiment object in openBIS 2. Create all sample objects 3. Establish parent-child links between the experiment and each sample</p> <p>OBJECT properties vs. parent-child relationships</p> <p>The <code>add_relationship()</code> method creates parent-child relationships in openBIS. However, some object types have properties with <code>data_type=\"OBJECT\"</code> that reference other objects directly (e.g., an Instrument's <code>responsible_person</code> property). These are semantically different ways of linking objects:</p> <ul> <li>Parent-child relationships: Use <code>collection.add_relationship(parent_id, child_id)</code> for hierarchical structures. The link has no semantic meaning and instead links inputs with outputs.</li> <li>OBJECT property references: Assign directly to the property (e.g., <code>instrument.responsible_person = person_obj</code>). The link has semantic meaning according to the assigned property.</li> </ul> <p>See Working with object references for more details on object referencing.</p>"},{"location":"tutorials/parsing/#updating-existing-objects","title":"Updating Existing Objects","text":"<p>Sometimes you want to use parsers to update objects that already exist in openBIS rather than creating new ones. To do this, set the <code>code</code> attribute on your object before adding it to the collection.</p>"},{"location":"tutorials/parsing/#example-updating-an-existing-sample","title":"Example: Updating an Existing Sample","text":"<pre><code>import json\nfrom bam_masterdata.parsing import AbstractParser\nfrom bam_masterdata.datamodel.object_types import Sample\n\n\nclass SampleUpdateParser(AbstractParser):\n    \"\"\"Parser that updates existing samples with new metadata.\"\"\"\n\n    def parse(self, files, collection, logger):\n        for file_path in files:\n            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                data = json.load(f)\n\n            # Create a sample object\n            sample = Sample(\n                name=data.get(\"sample_name\"),\n                # Add any other metadata fields\n            )\n\n            # Reference an existing object by its code\n            if data.get(\"existing_code\"):\n                sample.code = data.get(\"existing_code\")\n                logger.info(f\"Updating existing sample: {sample.code}\")\n            else:\n                logger.info(\"Creating new sample\")\n\n            # Add to collection\n            sample_id = collection.add(sample)\n</code></pre> <p>Example JSON for updating: <pre><code>{\n  \"sample_name\": \"Updated Sample Name\",\n  \"existing_code\": \"SAMPLE_001\"\n}\n</code></pre></p> <p>Code Format</p> <p>The <code>code</code> must exactly match the object's code in openBIS, including case and format (usually uppercase with underscores).</p> <p>Identifier Construction</p> <p>The full identifier for updating is:</p> <ul> <li>With collection: <code>/{space_name}/{project_name}/{collection_name}/{code}</code></li> <li>Without collection: <code>/{space_name}/{project_name}/{code}</code></li> </ul>"},{"location":"tutorials/parsing/#working-without-a-collection","title":"Working Without a Collection","text":"<p>You can also run parsers without specifying a collection. In this case, objects are attached directly to the project:</p> <pre><code>run_parser(\n    openbis=openbis,\n    space_name=\"MY_RESEARCH_SPACE\",\n    project_name=\"MY_PROJECT\",\n    collection_name=\"\",  # Empty string for no collection\n    files_parser=files_parser\n)\n</code></pre> <p>Dataset Limitation</p> <p>When working without a collection, dataset uploads may fail because pyBIS requires datasets to be attached to collections. The objects will still be created, but raw files may not be uploaded.</p>"},{"location":"tutorials/parsing/#complete-example-multi-file-automation-script","title":"Complete Example: Multi-File Automation Script","text":"<p>Here's a complete script that demonstrates automating metadata injection for multiple files:</p> <pre><code>import json\nimport datetime\nimport os\n\nfrom pybis import Openbis\n\nfrom bam_masterdata.cli.run_parser import run_parser\nfrom bam_masterdata.parsing import AbstractParser\nfrom bam_masterdata.datamodel.object_types import ExperimentalStep, SoftwareCode\n\n\nclass ExperimentParser(AbstractParser):\n    \"\"\"Parser for experiment metadata files.\"\"\"\n\n    def parse(self, files, collection, logger):\n        for file_path in files:\n            logger.info(f\"Processing: {file_path}\")\n\n            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                data = json.load(f)\n\n            # Create experiment object\n            experiment = ExperimentalStep(\n                name=data.get(\"name\"),\n                start_date=datetime.datetime.strptime(\n                    data.get(\"date\"), \"%Y-%m-%d\"\n                ).date() if data.get(\"date\") else None,\n                finished_flag=data.get(\"completed\", False),\n            )\n\n            exp_id = collection.add(experiment)\n            logger.info(f\"Added experiment: {exp_id}\")\n\n            # If software information is provided, create and link it\n            if data.get(\"software\"):\n                software = SoftwareCode(\n                    name=data[\"software\"].get(\"name\"),\n                    version=data[\"software\"].get(\"version\"),\n                )\n                soft_id = collection.add(software)\n                collection.add_relationship(parent_id=soft_id, child_id=exp_id)\n                logger.info(f\"Linked software {soft_id} to experiment {exp_id}\")\n\n\ndef main():\n    # Configuration\n    OPENBIS_URL = \"https://your-openbis-instance.com\"\n    USERNAME = os.getenv(\"OPENBIS_USERNAME\")\n    PASSWORD = os.getenv(\"OPENBIS_PASSWORD\")\n\n    # Connect to openBIS\n    print(\"Connecting to openBIS...\")\n    openbis = Openbis(url=OPENBIS_URL)\n    openbis.login(username=USERNAME, password=PASSWORD, save_token=True)\n    print(f\"Connected as: {openbis.username}\")\n\n    # Find all experiment files in a directory\n    data_dir = \"./experiment_data\"\n    experiment_files = [\n        os.path.join(data_dir, f)\n        for f in os.listdir(data_dir)\n        if f.endswith(\".json\")\n    ]\n\n    print(f\"Found {len(experiment_files)} files to process\")\n\n    # Setup parser\n    files_parser = {\n        ExperimentParser(): experiment_files\n    }\n\n    # Run the parser\n    print(\"Running parser...\")\n    run_parser(\n        openbis=openbis,\n        space_name=\"RESEARCH_SPACE\",\n        project_name=\"AUTOMATION_TEST\",\n        collection_name=\"BATCH_EXPERIMENTS\",\n        files_parser=files_parser,\n    )\n\n    print(\"Metadata injection complete!\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"tutorials/parsing/#best-practices","title":"Best Practices","text":""},{"location":"tutorials/parsing/#error-handling","title":"Error Handling","text":"<p>Add error handling to make your parsers robust:</p> <pre><code>class RobustParser(AbstractParser):\n    def parse(self, files, collection, logger):\n        for file_path in files:\n            try:\n                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                    data = json.load(f)\n\n                # Your parsing logic here\n                experiment = ExperimentalStep(name=data.get(\"name\"))\n                collection.add(experiment)\n\n            except FileNotFoundError:\n                logger.error(f\"File not found: {file_path}\")\n            except json.JSONDecodeError:\n                logger.error(f\"Invalid JSON in file: {file_path}\")\n            except Exception as e:\n                logger.error(f\"Error parsing {file_path}: {str(e)}\")\n</code></pre>"},{"location":"tutorials/parsing/#validation","title":"Validation","text":"<p>Validate data before creating objects:</p> <pre><code>def parse(self, files, collection, logger):\n    for file_path in files:\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n\n        # Validate required fields\n        if not data.get(\"name\"):\n            logger.warning(f\"Skipping file {file_path}: missing required 'name' field\")\n            continue\n\n        # Validate data types\n        if data.get(\"temperature\") and not isinstance(data[\"temperature\"], (int, float)):\n            logger.warning(f\"Invalid temperature value in {file_path}\")\n            continue\n\n        # Create object only if validation passes\n        experiment = ExperimentalStep(name=data[\"name\"])\n        collection.add(experiment)\n</code></pre>"},{"location":"tutorials/parsing/#logging","title":"Logging","text":"<p>Use different log levels appropriately:</p> <pre><code>logger.info(\"Starting to parse files\")      # General progress\nlogger.warning(\"Missing optional field\")    # Non-critical issues\nlogger.error(\"Failed to parse file\")        # Errors that need attention\nlogger.critical(\"Database connection lost\") # Critical failures\n</code></pre>"},{"location":"tutorials/parsing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/parsing/#common-issues","title":"Common Issues","text":"<p>Issue: \"No files or parsers to parse\" error - Solution: Ensure <code>files_parser</code> is not empty and contains valid parser instances with file lists.</p> <p>Issue: Objects not appearing in openBIS - Solution: Check that the space and project names are correct and that you have write permissions.</p> <p>Issue: \"Space does not exist in openBIS\" - Solution: The function will automatically use your user space. Make sure it exists or specify a valid space name.</p> <p>Issue: Relationships not created - Solution: Verify that both parent and child objects are added to the collection before calling <code>add_relationship()</code>.</p> <p>Issue: Files not uploaded as datasets - Solution: Ensure you're using a collection (not attaching directly to a project), as pyBIS requires collections for dataset uploads.</p>"}]}